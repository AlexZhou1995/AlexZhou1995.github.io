---
title: 深度召回算法
date: 2020-04-04 21:32:12
tags: ['ML']
categories: ['ML','论文阅读']
---

## 一、简介
推荐系统的基本架构一般由索引、召回、粗排、精排、重排等几个部分构成，而其中的召回阶段（或者称之为Matching阶段）则主要负责根据用户和内容特征，从众多的内容库中找到用户可能感兴趣的内容。传统的召回算法，如ItemCF等，大多基于统计的方法来计算item之间的相似度，根据用户近期买过的商品进行召回。传统召回算法实现成本低，模型简单响应速度快，但是基于数据统计得到的相似关系缺乏个性化能力，召回商品的发现性，多样性较低。
近年来，随着深度学习的兴起，一些深度召回算法被提出。它们利用深度模型表达能力高，特征提取能力强的特点，提高了个性化召回算法的效果。最初的深度召回算法主要通过深度神经网络挖掘用户和商品在低维表示空间上的相似性，为用户召回低维表示相似的商品，这类方法我们往往称之为向量化召回算法。相比于基于数据统计的相似性计算，向量化召回的个性化程度更高，召回商品的发现性和多样性也较高。还有一些深度召回算法尝试建立商品之间的关系图，通过基于图的深度网络学习，从拓扑关系的角度捕捉商品之间的相关性，这类方法我们称之为基于图的深度召回算法。最后，还有一些方法致力于攻克深度模型带来的性能开销，使得深度模型能够在大规模的召回场景中可用，比如通过树结构和哈希的方法对召回的过程进行提速等等，这类方法我们称之为大规模深度召回算法。本文就从这三类方法展开，介绍一些现有的深度召回算法。

<!--more-->

## 二、分类
- DeepMatch
youtube在2016年提出了用深度模型进行多分类的监督训练，得到item和user的embedding最后在线上通过内积进行召回检索，算是深度模型时代向量化召回的开端。本文中将通过分类任务监督训练得到embedding最后通过内积进行检索的方法都归到这一类

- Graph-based
这类方法大多同样是产出item和user的embedding，最后通过向量内积进行召回检索。但是这类方法引入图的结构来描述item和user之间的关系，他们声称相比于直接进行分类训练，引入了额外的拓扑信息。

- Large-scale
这类方法可能属于上面两类方法，但是他们着重处理Large-scale的问题，致力于缩短召回检索的开销。

## 三、DeepMatch
### 1. Deep neural networks for youtube recommendations. (RecSys 2016)
**结构和特征**
本文提出了一个较为基础的个性化向量召回算法，结构如下
![](15855389430461.jpg)


主要特征：
(a) 历史搜索query：把历史搜索的query分词后的token的embedding向量进行加权平均
(b) 人口统计学信息：性别、年龄、地域等
(c) 其他上下文信息
(d) Age信息：视频上传时间，保证时效性。

**训练和生效**
离线训练时把问题建模成一个多分类问题，对用户U和上下文C，预测视频V是否点击，数学表示如下：
![](15855396893807.jpg)
其中u和v为用户和商品的embedding，内积后进行softmax多分类。最后线上生效直接使用内积选取Top—N的视频。


### 2. Sequential Deep Matching Model for Online Large-scale Recommender System. (CIKM 2019) 
本文相比于上面Youtube的工作，引入了用户行为序列的信息进行序列化建模。示意图如下
![](15855399525004.jpg)
对于user，这边用user prediction network（上图虚线框）来提取用户向量，user prediction network中，使用Attention来提取用户长周期的特征(防止遗忘)，使用LSTM提取短周期的特征(注重实效)，最后通过一个门结构将长短周期和user embedding进行混合得到最终的用户向量表示。

## 四、Graph-based 
### 1. Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba. (KDD 2018) 
本文提出了一种基于图网络的低维表示提取方法，并且提出了两种改进方案。
提取的低维表示用于做个性化召回检索

#### BGE
首先最基础的图网络提取低维表示的方法，BGE（Base Graph Embedding）如下：
(1) 构建图：商品为点，用户行为为边，构建带权图，表示商品之间的关联。
(2) 采样：在图上进行Random walk的采样，得到一些商品序列，相关的商品在序列中同时出现的概率较高
(3) 训练：将采样得到的商品序列当做句子，其中每一个商品当做一个词、用NLP中的Skip-gram方法进行训练，得到每一个词的词向量，也就是每一个商品的向量。
![](15855442964204.jpg)

#### GES & EGES
上述的BGE方法难以处理冷启动的问题，新商品没有任何点击，无法产生有效的低维表示。
为了解决这个问题，本文提出了改进方案GES（Graph Embedding with Side information）
相比于只对商品学习低维表示，本方法还一并学习商品的其他属性信息。如下图SI0为item_id信息，SI1可能是store_id信息，etc. 
![](15855447612156.jpg)
最终一个商品的低维表示由所有的Side Information向量求均值得到：
![](15855448211571.jpg)

而EGES（Enhanced Graph Embedding with Side information）是GES的改进，认为不同的Information有着不同的重要性，最终商品的低维表示是各个Side Information的向量加权求和得到，权值也在训练中学习。
![](15855448929951.jpg)

### 2. Graph Convolutional Neural Networks for Web-Scal Recommender Systems. (KDD 2018)
本文提出用图卷积网络来提取商品的低维表示。
图的构建方式还是同上，商品为点，用户行为作为边，构建一个图。但是本文用图卷积来生成一个商品的向量，即一个商品的向量通过他的近邻混合得到。下图展示了一个2层的图卷积，对于商品A，他的近邻是BCD，而BCD的近邻分别又是AC，ABEF，A。则商品A的最终向量由两层级的卷积操作得到。
![](15855475119931.jpg)
一次卷积操作的过程如下，实际上就是对所有近邻的向量过一层全连接，然后通过pooling得到$n_u$，然后将目标向量$z_u$和$n_u$ concat之后再过一个全连接，得到新的目标向量$z_n^{new}$
![](15855478737833.jpg)

训练使用了max-margin ranking loss，数据组织为$<i,j,l>$，其中$<i,j>$为一个商品对，$l$为label，表示这一对商品是否相关。

## 五、Large—Scale
### 1. Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation. (CIKM 2019)
本文侧重关注召回检索的效率，结合哈希检索和向量化召回。通过对输出向量施加逐渐逼近符号函数的约束，使得生成的向量是二值的，以便使用哈希检索结构Multi-Index Hashing进行检索。这种检索方法的耗时与总商品数据量是一种次线性的关系。

本文其他部分与大多数DeepMatch类的算法相似，但是最后内积计算分类loss之前，对商品和用户向量施加一个符号函数$sgn(x)$来让输出的向量成为二值的。但是，符号函数不可导，这里就用带参数的tanh函数来进行逼近，随着训练epoch逐渐增加参数$\beta$大小，使其逼近符号函数$sgn$。损失函数和训练过程如下图。
![](15855485459116.jpg)
![](15855485133191.jpg)


### 2. Learning tree-based deep model for recommender systems. (KDD 2018) 
向量化召回由于使用向量内积作为最后的结果，表达能力有限，因此难以取得较好的个性化效果。如果使用深度模型，则超大的候选集合使得线上的延迟难以接受。本文结合了最大堆树和深度模型，提出了TDM算法。将庞大数量的商品分配到树的各个叶子节点，每个中间节点相当于其子节点商品的一个抽象表示，通过BeamSearch的方法对树结构进行高效检索。选取Top-K的商品，只需要深度模型做$Klog(N)$次预测。
![w400](15855493488403.jpg)


在训练过程中，每一个中间节点也被当做一个普通的item进行处理。
具体的训练方式是 树的结构 和 深度模型 交替进行更新。
当树的结构固定时，对于一条样本，如果用户点击了一个item，则这个item的搜索父节点也被视作点击。如此便可构造数据集对深度模型进行训练。
而树的结构，则是通过对当前的embedding进行聚类得到。
![](15855518986729.jpg)


### 3. Joint Optimization of Tree-based Index and Deep Model for Recommender Systems. (NIPS 2019)
本文是对上面TDM算法的改进。上面TDM算法中，树的构建和深度模型的训练是分隔开的，目标并不相同，可能两者的优化相互牵制导致总体效果次优。
本文主要有两个改进：
1.树结构、深度模型联合优化
通过最大似然构建联合优化的loss function。其中树结构的优化难以求解，转化为带权二部图的最大匹配问题，通过贪心算法求解。
2.用户序列特征分层建模
深度模型中用到了用户的行为序列，TDM中训练数的中间层时在序列特征方面也是用的item_id粒度的序列特征。在本文中改进为使用当前层的embedding作为序列的特征。这样可以减少每层训练的噪声，并且可以从粗到细的精准建模。

## 六、总结
#### 1. Deep neural networks for youtube recommendations
方法名：无
动机：使用深度模型进行个性化的召回，同时保证效率
方法：通过深度模型多分类任务训练，得到用户和商品的向量，用向量内积做召回检索。

#### 2. Sequential Deep Matching Model for Online Large-scale Recommender System
方法名：SDM
动机：建模用户行为序列
方法：引入用户长短期行为序列，分别用Attention和LSTM结构进行建模，最终进行向量召回。

#### 3. Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba. 
方法名：BGE，GES，EGES
动机：使用图网络表示商品之间的关系，再学习商品的低维表示
方法：使用用户行为建图，通过采样得到多条商品轨迹。将轨迹当成句子，商品当做词，通过NLP中求解词向量的方法求解商品向量。并且使用商品其他属性的向量来缓解冷启动问题。

#### 4. Graph Convolutional Neural Networks for Web-Scal Recommender Systems.
方法名：PinSage
动机：使用图表示商品之间的关系，再使用图卷积网络学习商品的低维表示
方法：每一个商品的embedding由其近邻的embedding通过图卷积操作得到。通过max-margine ranking loss进行训练。

#### 5. Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation.
方法名：CICAR
动机：通过高效哈希检索结构来提升召回的效率
方法：使用次线性复杂度的哈希检索方法Multi-index hashing，需要embedding是二值的。因此在现有双塔内积网络的基础上，使用一个近似符号函数的映射函数$tanh_\beta$将输出的向量映射到二值。

#### 6. Learning tree-based deep model for recommender systems.
方法名：TDM
动机：通过内积进行召回表达能力有限，使用深度模型打分则商品太多开销过大。
方法：在全部N个商品集合上构建树的结构，在树上通过BeamSearch进行检索TopK商品，深度模型只需要预测$Klog(N)$次。树的中间节点是商品的抽象，当做商品一次训练，其label来自于子节点。深度模型和树结构交替训练。

#### 7. Joint Optimization of Tree-based Index and Deep Model for Recommender Systems.
方法名：JTM
动机：TDM中深度模型和树结构训练是完全分隔开的，目标不同可能导致相互牵制无法训练到最优。
方法：构建统一loss function，深度模型和树结构联合优化。树结构的优化是个较难的组合优化问题，转化为带权二部图匹配问题后用贪心算法求解。

## 七、结语
本文介绍了近几年内的一些深度召回算法，主要把它们划分成“Deep Matching类算法”、“基于图的算法”、“面向大规模场景算法”3类算法，并对这3类算法进行归纳和总结，分析他们的动机、做法和优缺点。深度学习在推荐系统中的应用是近年来持续的热门研究课题，每年都会有大量新技术和新模型出现。我们希望这篇文章能帮助读者对这个领域有一个大致的了解，并为未来的研究提供一些思路和参考。
<!DOCTYPE html>


<html lang="cn">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>llama3_from_scratch |  HomePage</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
      <canvas width="1777" height="841"
        style="position: fixed; left: 0px; top: 0px; z-index: 99999; pointer-events: none;"></canvas>
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-llama3-from-scratch"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  llama3_from_scratch
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2024/10/24/llama3-from-scratch/" class="article-date">
  <time datetime="2024-10-24T03:44:34.000Z" itemprop="datePublished">2024-10-24</time>
</a>   
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">8.5k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">36 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <blockquote>
<p>文章来源： https://mp.weixin.qq.com/s/q1BGBbpSWZVqgbEzQ-5Ufg</p>
</blockquote>
<p>大家好，本文将详细指导如何从零开始构建完整的Llama 3模型架构，并在自定义数据集上执行训练和推理。</p>
<p><img src="/2024/10/24/llama3-from-scratch/1.webp"> 图1：Llama 3架构展示训练和推理流程。因为官方Llama 3论文中未提供相关图表。所以此图为大概架构图，阅读本文后你应能绘制出更为精确的架构图。</p>
<h1 id="本文目标">本文目标</h1>
<p>通过本文。你可以了解到:</p>
<ol type="1">
<li><p>深入理解Llama 3模型各组件的底层工作原理。</p></li>
<li><p>编写代码构建Llama 3的每个组件，并将它们组装成一个功能完整的Llama 3模型。</p></li>
<li><p>编写代码使用新的自定义数据集训练模型。</p></li>
<li><p>编写代码执行推理，使Llama 3模型能够根据输入提示生成新文本。</p></li>
</ol>
<h1 id="输入模块">1、输入模块</h1>
<p>如图1所示，输入模块包含三个组件：文本/提示、分词器和嵌入。</p>
<p>输入模块内部工作流程</p>
<p>让我们通过下图了解输入模块内的工作流程。</p>
<p><img src="/2024/10/24/llama3-from-scratch/2.webp"> 图2：输入模块流程图，展示提示、分词器和嵌入流程。</p>
<p>首先，单个或批量文本/提示被输入模型。例如：图中的"Hello World"。</p>
<p>输入模型的必须是数字格式，因为模型无法直接处理文本。分词器将这些文本/提示转换为标记ID（词汇表中标记的索引号表示）。我们将使用Tiny Shakespeare数据集构建词汇表并训练模型。Llama 3模型使用TikToken作为分词器，这是一种子词分词器。但是我们这个实现将使用字符级分词器。这样做的主要原因是让我们能够自行构建词汇表和分词器，包括编码和解码函数，这样可以深入理解底层工作原理并完全掌控代码。</p>
<p>每个标记ID将被转换为128维的嵌入向量（原始Llama 3 8B中为4096维）。然后这些嵌入将被传递到下一个解码器模块。</p>
<p>输入模块代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必要的库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Optional, Tuple, List</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">### 步骤1: 输入模块 ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Tiny Shakespeare数据集实现字符级分词器。部分字符级分词器代码参考自Andrej Karpathy的GitHub仓库</span></span><br><span class="line"><span class="comment"># (https://github.com/karpathy/nanoGPT/blob/master/data/shakespeare_char/prepare.py)</span></span><br><span class="line"><span class="comment"># 加载tiny_shakespeare数据文件 (https://github.com/tamangmilan/llama3/blob/main/tiny_shakespeare.txt)</span></span><br><span class="line"></span><br><span class="line">device: str = <span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>  <span class="comment"># 根据可用性分配设备为cuda或cpu</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载tiny_shakespeare数据文件</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'tiny_shakespeare.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = f.read()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过提取tiny_shakespeare数据中的所有唯一字符准备词汇表</span></span><br><span class="line">vocab = sorted(list(set(data)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练Llama 3模型需要额外的标记，如&lt;|begin_of_text|&gt;、&lt;|end_of_text|&gt;和&lt;|pad_id|&gt;，将它们添加到词汇表中</span></span><br><span class="line">vocab.extend([<span class="string">'&lt;|begin_of_text|&gt;'</span>, <span class="string">'&lt;|end_of_text|&gt;'</span>, <span class="string">'&lt;|pad_id|&gt;'</span>])</span><br><span class="line">vocab_size = len(vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建字符与词汇表中对应整数索引之间的映射。</span></span><br><span class="line"><span class="comment"># 这对于构建分词器的编码和解码函数至关重要。</span></span><br><span class="line">itos = &#123;i: ch <span class="keyword">for</span> i, ch <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line">stoi = &#123;ch: i <span class="keyword">for</span> i, ch <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分词器编码函数：输入字符串，输出整数列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [stoi[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> s]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分词器解码函数：输入整数列表，输出字符串</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(l)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> l)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义稍后在模型训练中使用的张量标记变量</span></span><br><span class="line">token_bos = torch.tensor([stoi[<span class="string">'&lt;|begin_of_text|&gt;'</span>]], dtype=torch.int, device=device)</span><br><span class="line">token_eos = torch.tensor([stoi[<span class="string">'&lt;|end_of_text|&gt;'</span>]], dtype=torch.int, device=device)</span><br><span class="line">token_pad = torch.tensor([stoi[<span class="string">'&lt;|pad_id|&gt;'</span>]], dtype=torch.int, device=device)</span><br><span class="line"></span><br><span class="line">prompts = <span class="string">"Hello World"</span></span><br><span class="line">encoded_tokens = encode(prompts)</span><br><span class="line">decoded_text = decode(encoded_tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 输入模块代码测试 ###</span></span><br><span class="line"><span class="comment"># 取消下面的三重引号来执行测试</span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">print(f"Shakespeare文本字符长度: &#123;len(data)&#125;")  </span></span><br><span class="line"><span class="string">print(f"词汇表内容: &#123;''.join(vocab)&#125;\n")  </span></span><br><span class="line"><span class="string">print(f"词汇表大小: &#123;vocab_size&#125;")  </span></span><br><span class="line"><span class="string">print(f"编码后的标记: &#123;encoded_tokens&#125;")  </span></span><br><span class="line"><span class="string">print(f"解码后的文本: &#123;decoded_text&#125;")  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment">### 测试结果: ###</span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">Shakespeare文本字符长度: 1115394  </span></span><br><span class="line"><span class="string">词汇表内容:  </span></span><br><span class="line"><span class="string"> !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz&lt;|begin_of_text|&gt;&lt;|end_of_text|&gt;&lt;|pad_id|&gt;  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">词汇表大小: 68  </span></span><br><span class="line"><span class="string">编码后的标记: [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]  </span></span><br><span class="line"><span class="string">解码后的文本: Hello World  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h1 id="解码器模块">2、解码器模块</h1>
<p>参照图1的架构图，解码器模块包含以下子组件：</p>
<ul>
<li><p>RMS归一化</p></li>
<li><p>旋转位置编码</p></li>
<li><p>KV缓存</p></li>
<li><p>分组查询注意力</p></li>
<li><p>前馈网络</p></li>
<li><p>解码器块</p></li>
</ul>
<h2 id="rms归一化root-mean-square-normalization">RMS归一化（Root Mean Square Normalization）</h2>
<h3 id="rmsnorm的必要性">RMSNorm的必要性</h3>
<p>从图1可以看出，输入模块的输出（嵌入向量）经过RMSNorm模块。这是因为嵌入向量具有多个维度（Llama3-8b中为4096维），可能出现不同范围的值。这会导致模型梯度爆炸或消失，从而导致收敛缓慢甚至发散。而RMSNorm将这些值归一化到一定范围，有助于稳定和加速训练过程。这使得梯度具有更一致的幅度，从而加快模型收敛。</p>
<h3 id="rmsnorm的工作原理">RMSNorm的工作原理</h3>
<p><img src="/2024/10/24/llama3-from-scratch/3.webp"></p>
<p>图3：对形状为[3,3]的输入嵌入应用RMSNorm</p>
<p>类似于层归一化，RMSNorm沿嵌入特征或维度应用。上图中的嵌入形状为[3,3]，意味着每个标记有3个维度。</p>
<p>示例：对第一个标记X1的嵌入应用RMSNorm：</p>
<p>X1标记在每个维度上的值（x11、x12和x13）分别除以所有这些值的均方根。公式如图3所示。</p>
<p>为避免除以零并保证数值稳定性，在均方根中加入一个小常数E（Epsilon）。乘以一个缩放参数Gamma (Y)。每个特征都有一个独特的Gamma参数（如图中d1维度的Y1、d2维度的Y2和d3维度的Y3），这是一个学习参数，可以向上或向下缩放以进一步稳定归一化。gamma参数初始化为1（如上面的计算所示）。</p>
<p>如示例所示，嵌入值原本较大且分布范围宽。应用RMSNorm后，值变小且范围缩小。计算使用实际的RMSNorm函数完成。</p>
<p>RMSNorm相比层归一化的优势</p>
<p>如上例所示没有计算任何均值或方差，而这在层归一化中是必需的。所以RMSNorm通过避免计算均值和方差减少了计算开销。根据作者的研究，RMSNorm在不影响准确性的同时提供了性能优势。</p>
<p>RMSNorm代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 步骤2: 解码器模块  </span></span><br><span class="line"><span class="comment"># 注：由于Llama 3模型由Meta开发，为了与他们的代码库保持一致并考虑未来兼容性，</span></span><br><span class="line"><span class="comment"># 我将使用Meta GitHub上的大部分代码，并进行必要的修改以实现我们的目标。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义参数数据类：我们将在模型构建、训练和推理过程中使用这些参数。</span></span><br><span class="line"><span class="comment"># 注：为了更快地看到训练和推理结果，而不是专注于高准确性，我们对大多数参数采用较低的值，</span></span><br><span class="line"><span class="comment"># 这些值在Llama 3模型中设置得更高。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelArgs</span>:</span></span><br><span class="line">    dim: int = <span class="number">512</span>  <span class="comment"># 嵌入维度  </span></span><br><span class="line">    n_layers: int = <span class="number">8</span>  <span class="comment"># 模型解码器块的数量  </span></span><br><span class="line">    n_heads: int = <span class="number">8</span>  <span class="comment"># 查询嵌入的头数  </span></span><br><span class="line">    n_kv_heads: int = <span class="number">4</span>  <span class="comment"># 键和值嵌入的头数  </span></span><br><span class="line">    vocab_size: int = len(vocab)  <span class="comment"># 词汇表长度  </span></span><br><span class="line">    multiple_of: int = <span class="number">256</span>  <span class="comment"># 用于计算前馈网络维度  </span></span><br><span class="line">    ffn_dim_multiplier: Optional[float] = <span class="literal">None</span>  <span class="comment"># 用于计算前馈网络维度  </span></span><br><span class="line">    norm_eps: float = <span class="number">1e-5</span>  <span class="comment"># RMSNorm计算的默认Epsilon值  </span></span><br><span class="line">    rope_theta: float = <span class="number">10000.0</span>  <span class="comment"># RePE计算的默认theta值  </span></span><br><span class="line"></span><br><span class="line">    max_batch_size: int = <span class="number">10</span>  <span class="comment"># 最大批量大小  </span></span><br><span class="line">    max_seq_len: int = <span class="number">256</span>  <span class="comment"># 最大序列长度  </span></span><br><span class="line"></span><br><span class="line">    epochs: int = <span class="number">2500</span>  <span class="comment"># 总训练迭代次数  </span></span><br><span class="line">    log_interval: int = <span class="number">10</span>  <span class="comment"># 打印日志和损失值的间隔数    </span></span><br><span class="line">    device: str = <span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>  <span class="comment"># 根据可用性分配设备为cuda或cpu</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 步骤2a: RMSNorm  </span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RMSNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim: int, eps: float = <span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        device = ModelArgs.device</span><br><span class="line">        self.eps = eps</span><br><span class="line">        <span class="comment"># 缩放参数gamma，初始化为1，参数数量等于dim的大小  </span></span><br><span class="line">        self.weight = nn.Parameter(torch.ones(dim).to(device))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_norm</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x * torch.rsqrt(x.pow(<span class="number">2</span>).mean(dim=<span class="number">-1</span>, keepdim=<span class="literal">True</span>) + self.eps).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 形状: x[bs,seq,dim]  </span></span><br><span class="line">        output = self._norm(x.float()).type_as(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 形状: x[bs,seq,dim] -&gt; x_norm[bs,seq,dim]  </span></span><br><span class="line">        <span class="keyword">return</span> output * self.weight</span><br><span class="line"></span><br><span class="line">    <span class="comment">### RMSNorm代码测试 ###  </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取消下面的三重引号来执行测试  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">x = torch.randn((ModelArgs.max_batch_size, ModelArgs.max_seq_len, ModelArgs.dim), device=device)  </span></span><br><span class="line"><span class="string">rms_norm = RMSNorm(dim=ModelArgs.dim)  </span></span><br><span class="line"><span class="string">x_norm = rms_norm(x)  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">print(f"x的形状: &#123;x.shape&#125;")  </span></span><br><span class="line"><span class="string">print(f"x_norm的形状: &#123;x_norm.shape&#125;")  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment">### 测试结果: ###  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">x的形状: torch.Size([10, 256, 512])  </span></span><br><span class="line"><span class="string">x_norm的形状: torch.Size([10, 256, 512])  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="旋转位置编码rotary-positional-encoding-rope">旋转位置编码（Rotary Positional Encoding, RoPE）</h2>
<p>回顾之前的步骤，我们已将输入文本转换为嵌入，并对嵌入应用了RMSNorm。然而，这里存在一个问题：假设输入文本是"I love apple"或"apple love I"，模型会将两个句子视为相同并以相同方式学习。这是因为嵌入中没有为模型定义顺序信息。因此对于任何语言模型来说，保持标记的顺序至关重要。在Llama 3模型架构中，引入了旋转位置编码（RoPE）来定义句子中每个标记的位置，这不仅维护了顺序，还保留了句子中标记的相对位置信息。</p>
<h3 id="旋转位置编码的工作原理">旋转位置编码的工作原理</h3>
<p>RoPE是一种位置编码方法，它通过添加绝对位置信息以及包含标记之间的相对位置信息来编码嵌入，从而维护句子中标记的顺序。它通过使用一个特殊的旋转矩阵来旋转给定的嵌入来执行编码操作。这种利用旋转矩阵的简洁而强大的数学推导是RoPE的核心。</p>
<p><img src="/2024/10/24/llama3-from-scratch/4.webp"> 图4：应用于2维向量的旋转矩阵</p>
<p>上图展示了旋转矩阵应用于2维向量的情况。Llama 3模型中的维度数是4096，远高于此。我们详细介绍如何对更高维度的嵌入应用旋转。</p>
<p><img src="/2024/10/24/llama3-from-scratch/5.webp"> 图5：RoPE应用于嵌入的示例</p>
<p>嵌入的旋转涉及每个嵌入位置(m)值和theta (θ)对每对嵌入维度的乘法。这就是RoPE如何通过实现旋转矩阵来捕获绝对位置和相对位置信息的方式。</p>
<p>注意：在执行旋转之前，需要将旋转矩阵转换为极坐标形式，并将嵌入向量转换为复数。旋转完成后，旋转后的嵌入需要转换回实数以进行注意力操作。另外RoPE仅应用于查询和键嵌入，不适用于值嵌入。</p>
<p>RoPE的代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 步骤2b: RoPE实现  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">precompute_freqs_cis</span><span class="params">(dim: int, seq_len: int, theta: float = <span class="number">10000.0</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 计算每对维度的Theta值，即dim/2  </span></span><br><span class="line">    device = ModelArgs.device</span><br><span class="line">    freqs = <span class="number">1.0</span> / (theta ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>, device=device)[:(dim // <span class="number">2</span>)].float() / dim))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算序列中位置(m)的范围  </span></span><br><span class="line">    t = torch.arange(seq_len, dtype=torch.float32, device=device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># freqs给出序列中所有标记位置的Theta值范围  </span></span><br><span class="line">    freqs = torch.outer(t, freqs).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这是需要转换为极坐标形式的旋转矩阵，以便对嵌入执行旋转  </span></span><br><span class="line">    freqs_cis = torch.polar(torch.ones_like(freqs).to(device), freqs).to(device)</span><br><span class="line">    <span class="keyword">return</span> freqs_cis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reshape_for_broadcast</span><span class="params">(freqs_cis, x)</span>:</span></span><br><span class="line">    ndim = x.ndim</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= <span class="number">1</span> &lt; ndim</span><br><span class="line">    <span class="keyword">assert</span> freqs_cis.shape == (x.shape[<span class="number">1</span>], x.shape[<span class="number">-1</span>]), <span class="string">"freqs_cis的最后两个维度必须与x匹配"</span></span><br><span class="line">    shape = [d <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">or</span> i == ndim - <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> i, d <span class="keyword">in</span> enumerate(x.shape)]</span><br><span class="line">    <span class="keyword">return</span> freqs_cis.view(*shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_rotary_emb</span><span class="params">(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor)</span> -&gt; Tuple[torch.Tensor, torch.Tensor]:</span></span><br><span class="line">    device = ModelArgs.device</span><br><span class="line">    <span class="comment"># 同时对查询和键嵌入应用旋转位置编码  </span></span><br><span class="line">    <span class="comment"># 首先：xq和xk嵌入的最后一个维度需要重塑为一对。因为旋转矩阵应用于每对维度。  </span></span><br><span class="line">    <span class="comment"># 其次：将xq和xk转换为复数，因为旋转矩阵只适用于复数  </span></span><br><span class="line">    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:<span class="number">-1</span>], <span class="number">-1</span>, <span class="number">2</span>)).to(device)  <span class="comment"># xq_:[bsz, seq_len, n_heads, head_dim/2]  </span></span><br><span class="line">    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:<span class="number">-1</span>], <span class="number">-1</span>, <span class="number">2</span>)).to(device)  <span class="comment"># xk_:[bsz, seq_len, n_heads, head_dim/2]  </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 旋转矩阵(freqs_cis)在seq_len(dim=1)和head_dim(dim=3)维度上应与嵌入匹配  </span></span><br><span class="line">    <span class="comment"># 此外，freqs_cis的形状应与xq和xk相同，因此将freqs_cis的形状从[seq_len,head_dim]改变为[1,seq_len,1,head_dim]  </span></span><br><span class="line">    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后，通过与freqs_cis相乘执行旋转操作。  </span></span><br><span class="line">    <span class="comment"># 旋转完成后，将xq_out和xk_out转换回实数并返回  </span></span><br><span class="line">    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(<span class="number">3</span>).to(device)  <span class="comment"># xq_out:[bsz, seq_len, n_heads, head_dim]  </span></span><br><span class="line">    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(<span class="number">3</span>).to(device)  <span class="comment"># xk_out:[bsz, seq_len, n_heads, head_dim]  </span></span><br><span class="line">    <span class="keyword">return</span> xq_out.type_as(xq), xk_out.type_as(xk)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### RoPE代码测试 ###  </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注：x_norm在RMSNorm测试中计算，这里用于测试。  </span></span><br><span class="line"><span class="comment"># 取消下面的三重引号来执行测试  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">head_dim = ModelArgs.dim//ModelArgs.n_heads  </span></span><br><span class="line"><span class="string">wq = nn.Linear(ModelArgs.dim, ModelArgs.n_heads * head_dim, bias=False, device=device)  </span></span><br><span class="line"><span class="string">wk = nn.Linear(ModelArgs.dim, ModelArgs.n_kv_heads * head_dim, bias=False, device=device)  </span></span><br><span class="line"><span class="string">xq = wq(x_norm)  </span></span><br><span class="line"><span class="string">xk = wk(x_norm)  </span></span><br><span class="line"><span class="string">print(f"xq.shape: &#123;xq.shape&#125;")  </span></span><br><span class="line"><span class="string">print(f"xk.shape: &#123;xk.shape&#125;")  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">xq = xq.view(xq.shape[0],xq.shape[1],ModelArgs.n_heads, head_dim)  </span></span><br><span class="line"><span class="string">xk = xk.view(xk.shape[0],xk.shape[1],ModelArgs.n_kv_heads, head_dim)  </span></span><br><span class="line"><span class="string">print(f"xq.re-shape: &#123;xq.shape&#125;")  </span></span><br><span class="line"><span class="string">print(f"xk.re-shape: &#123;xk.shape&#125;")  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">freqs_cis = precompute_freqs_cis(dim=head_dim, seq_len=ModelArgs.max_seq_len)  </span></span><br><span class="line"><span class="string">print(f"freqs_cis.shape: &#123;freqs_cis.shape&#125;")  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">xq_rotate, xk_rotate = apply_rotary_emb(xq, xk, freqs_cis)  </span></span><br><span class="line"><span class="string">print(f"xq_rotate.shape: &#123;xq_rotate.shape&#125;")  </span></span><br><span class="line"><span class="string">print(f"xk_rotate.shape: &#123;xk_rotate.shape&#125;")  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment">### 测试结果: ###  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">xq.shape: torch.Size([10, 256, 512])  </span></span><br><span class="line"><span class="string">xk.shape: torch.Size([10, 256, 256])  </span></span><br><span class="line"><span class="string">xq.re-shape: torch.Size([10, 256, 8, 64])  </span></span><br><span class="line"><span class="string">xk.re-shape: torch.Size([10, 256, 4, 64])  </span></span><br><span class="line"><span class="string">freqs_cis.shape: torch.Size([256, 32])  </span></span><br><span class="line"><span class="string">xq_rotate.shape: torch.Size([10, 256, 8, 64])  </span></span><br><span class="line"><span class="string">xk_rotate.shape: torch.Size([10, 256, 4, 64])  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="kv缓存">KV缓存</h2>
<p>在Llama 3架构中，推理阶段引入了KV缓存的概念，用于以键和值缓存的形式存储先前生成的标记。这些缓存用于计算自注意力以生成下一个标记。只缓存键和值标记，而不缓存查询标记，因此称为KV缓存。</p>
<h3 id="kv缓存的必要性">KV缓存的必要性</h3>
<p>让我们通过下图来理解KV缓存的重要性。</p>
<p><img src="/2024/10/24/llama3-from-scratch/6.webp"> 图6：KV缓存实现</p>
<p>图中的A块：在生成output3标记时，仍在计算先前的输出标记（output1, output2），这是不必要的。这在注意力计算期间导致了额外的矩阵乘法，显著增加了计算资源的使用。</p>
<p>图中的B块：输出标记替换了查询嵌入中的输入标记。KV缓存存储了先前生成的标记。在注意力分数计算期间，我们只需要使用查询中的1个标记，并使用键和值缓存中的先前标记。这将矩阵乘法从A块的3x3减少到B块的1x3，减少了约66%。在实际应用中，对于巨大的序列长度和批量大小，这将显著减少计算资源的使用。</p>
<h2 id="分组查询注意力">分组查询注意力</h2>
<p>分组查询注意力与之前模型（如Llama 1）中使用的多头注意力相似，唯一的区别在于为查询和键/值使用单独的头。分配给查询的头数是键和值头数的n倍。让我们通过图表来进一步理解。</p>
<p><img src="/2024/10/24/llama3-from-scratch/7.webp"> 图7：分组查询注意力和多头注意力对比</p>
<p>在给定的图中，多头注意力在所有查询、键和值中都有相等数量的头，即n_heads = 8。</p>
<p>分组查询注意力块有8个查询头（n_heads）和4个键和值头（n_kv_heads），这是查询头数量的一半。</p>
<p>分组查询注意力的优势</p>
<p>尽管多头注意力已经表现出色，引入分组查询注意力是有其特定原因。我们先回顾KV缓存，KV缓存确实大大减少了计算资源的使用。但是随着KV缓存存储越来越多的先前标记，内存使用会显著增加。这对模型性能和计算成本都不利。所以引入了分组查询注意力。减少K和V的头数会减少需要存储的参数数量，从而减少内存使用。多项测试结果表明，使用这种方法模型的准确性仍保持在相近的范围内。</p>
<p>注意力模块的代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 注意力模块 [步骤2c: KV缓存; 步骤2d: 分组查询注意力]  </span></span><br><span class="line"><span class="comment">## 如前所述，命名约定遵循原始Meta LLama3 GitHub  </span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args: ModelArgs)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.args = args</span><br><span class="line">        <span class="comment"># 嵌入维度  </span></span><br><span class="line">        self.dim = args.dim</span><br><span class="line">        <span class="comment"># 分配给查询的头数  </span></span><br><span class="line">        self.n_heads = args.n_heads</span><br><span class="line">        <span class="comment"># 分配给键和值的头数。如果为"None"，则数量与查询相同。  </span></span><br><span class="line">        self.n_kv_heads = args.n_heads <span class="keyword">if</span> args.n_kv_heads <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> args.n_kv_heads</span><br><span class="line">        <span class="comment"># 每个头相对于模型维度的维度  </span></span><br><span class="line">        self.head_dim = args.dim // args.n_heads</span><br><span class="line">        <span class="comment"># 重复次数，以使键、值头数与查询头数匹配  </span></span><br><span class="line">        self.n_rep = args.n_heads // args.n_kv_heads</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化键、查询、值和输出的权重。注意q和kv的权重out_feature值基于其头数  </span></span><br><span class="line">        self.wq = nn.Linear(self.dim, self.n_heads * self.head_dim, bias=<span class="literal">False</span>, device=device)</span><br><span class="line">        self.wk = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=<span class="literal">False</span>, device=device)</span><br><span class="line">        self.wv = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=<span class="literal">False</span>, device=device)</span><br><span class="line">        self.wo = nn.Linear(self.n_heads * self.head_dim, self.dim, bias=<span class="literal">False</span>, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化缓存以在开始时存储键、值 (KV缓存实现)  </span></span><br><span class="line">        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim), device=args.device)</span><br><span class="line">        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim), device=args.device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x: torch.Tensor, start_pos, inference)</span>:</span></span><br><span class="line">        <span class="comment"># 输入嵌入的形状: [bsz,seq_len,dim]  </span></span><br><span class="line">        bsz, seq_len, _ = x.shape</span><br><span class="line">        <span class="comment"># 掩码将在"训练"期间使用，由于使用KV缓存，"推理"不需要掩码。</span></span><br><span class="line">        mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        xq = self.wq(x)  <span class="comment"># x[bsz,seq_len,dim]*wq[dim,n_heads * head_dim] -&gt; q[bsz,seq_len,n_heads * head_dim]  </span></span><br><span class="line">        xk = self.wk(x)  <span class="comment"># x[bsz,seq_len,dim]*wq[dim,n_kv_heads * head_dim] -&gt; k[bsz,seq_len,n_kv_heads * head_dim]  </span></span><br><span class="line">        xv = self.wv(x)  <span class="comment"># x[bsz,seq_len,dim]*wq[dim,n_kv_heads * head_dim] -&gt; v[bsz,seq_len,n_kv_heads * head_dim]  </span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据头数重塑查询、键和值 (分组查询注意力实现)  </span></span><br><span class="line">        xq = xq.view(bsz, seq_len, self.n_heads, self.head_dim)  <span class="comment"># xq[bsz,seq_len,n_heads, head_dim]  </span></span><br><span class="line">        xk = xk.view(bsz, seq_len, self.n_kv_heads, self.head_dim)  <span class="comment"># xk[bsz,seq_len,n_kv_heads, head_dim]  </span></span><br><span class="line">        xv = xv.view(bsz, seq_len, self.n_kv_heads, self.head_dim)  <span class="comment"># xv[bsz,seq_len,n_kv_heads, head_dim]  </span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型 - 推理模式: kv-cache仅在推理模式下启用  </span></span><br><span class="line">        <span class="keyword">if</span> inference:</span><br><span class="line">            <span class="comment"># 计算序列中每个位置的旋转矩阵  </span></span><br><span class="line">            freqs_cis = precompute_freqs_cis(dim=self.head_dim, seq_len=self.args.max_seq_len * <span class="number">2</span>)</span><br><span class="line">            <span class="comment"># 在推理过程中,我们应该只取从当前标记位置开始的旋转矩阵范围  </span></span><br><span class="line">            freqs_cis = freqs_cis[start_pos: start_pos + seq_len]</span><br><span class="line">            <span class="comment"># 将RoPE应用于查询和键嵌入  </span></span><br><span class="line">            xq, xk = apply_rotary_emb(xq, xk, freqs_cis)</span><br><span class="line"></span><br><span class="line">            self.cache_k = self.cache_k.to(xq)</span><br><span class="line">            self.cache_v = self.cache_v.to(xq)</span><br><span class="line">            <span class="comment"># 将键和值标记嵌入存储到它们各自的缓存中 [KV缓存实现]  </span></span><br><span class="line">            self.cache_k[:bsz, start_pos:start_pos + seq_len] = xk</span><br><span class="line">            self.cache_v[:bsz, start_pos:start_pos + seq_len] = xv</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 为注意力计算分配所有直到当前标记位置的先前标记嵌入给键和值变量  </span></span><br><span class="line">            keys = self.cache_k[:bsz, :start_pos + seq_len]</span><br><span class="line">            values = self.cache_v[:bsz, :start_pos + seq_len]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 此时,键和值的形状与查询嵌入不同,但为了计算注意力分数,它们必须相同  </span></span><br><span class="line">            <span class="comment"># 使用repeat_kv函数使键、值的形状与查询形状相同  </span></span><br><span class="line">            keys = repeat_kv(keys, self.n_rep)  <span class="comment"># keys[bsz,seq_len,n_heads,head_dim]  </span></span><br><span class="line">            values = repeat_kv(values, self.n_rep)  <span class="comment"># values[bsz,seq_len,n_heads,head_dim]  </span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模式 - 训练模式: 未实现KV-Cache  </span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 计算旋转矩阵并将RoPE应用于训练的查询和键  </span></span><br><span class="line">            freqs_cis = precompute_freqs_cis(dim=self.head_dim, seq_len=self.args.max_seq_len)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># xq[bsz,seq_len,n_heads, head_dim], xk[bsz,seq_len,n_heads, head_dim]  </span></span><br><span class="line">            xq, xk = apply_rotary_emb(xq, xk, freqs_cis)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使用repeat_kv函数使键、值的形状与查询形状相同  </span></span><br><span class="line">            <span class="comment"># keys[bsz,seq_len,n_heads,head_dim], #values[bsz,seq_len,n_heads,head_dim]  </span></span><br><span class="line">            keys = repeat_kv(xk, self.n_rep)</span><br><span class="line">            values = repeat_kv(xv, self.n_rep)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 对于训练模式,我们将计算掩码并稍后应用于注意力分数  </span></span><br><span class="line">            mask = torch.full((seq_len, seq_len), float(<span class="string">"-inf"</span>), device=self.args.device)</span><br><span class="line">            mask = torch.triu(mask, diagonal=<span class="number">1</span>).to(self.args.device)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 为了计算注意力,我们需要执行转置操作来重塑所有查询、键和值,将头部放在维度1,序列放在维度2  </span></span><br><span class="line">        xq = xq.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># xq[bsz,n_heads,seq_len,head_dim]  </span></span><br><span class="line">        keys = keys.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># keys[bsz,n_heads,seq_len,head_dim]  </span></span><br><span class="line">        values = values.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># values[bsz,n_heads,seq_len,head_dim]  </span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力分数  </span></span><br><span class="line">        scores = torch.matmul(xq, keys.transpose(<span class="number">2</span>, <span class="number">3</span>)).to(self.args.device) / math.sqrt(self.head_dim)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = scores + mask</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 对注意力分数应用softmax  </span></span><br><span class="line">        scores = F.softmax(scores.float(), dim=<span class="number">-1</span>).type_as(xq)</span><br><span class="line">        <span class="comment"># 注意力分数与值的矩阵乘法  </span></span><br><span class="line">        output = torch.matmul(scores, values).to(self.args.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 我们得到了每个头部的上下文嵌入  </span></span><br><span class="line">        <span class="comment"># 所有头部需要重塑回来并组合,以给出单个上下文注意力输出  </span></span><br><span class="line">        <span class="comment"># 形状变化: output[bsz,n_heads,seq_len,head_dim] -&gt; output[bsz,seq_len, n_heads,head_dim] -&gt; output[bsz,seq_len, n_heads * head_dim]  </span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(bsz, seq_len, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 形状: output [bsz,seq_len,dim]  </span></span><br><span class="line">        <span class="keyword">return</span> self.wo(output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果键/值头的数量少于查询头,此函数使用所需的重复次数扩展键/值嵌入  </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repeat_kv</span><span class="params">(x: torch.Tensor, n_rep: int)</span> -&gt; torch.Tensor:</span></span><br><span class="line">    bsz, seq_len, n_kv_heads, head_dim = x.shape</span><br><span class="line">    <span class="keyword">if</span> n_rep == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        x[:, :, :, <span class="literal">None</span>, :]</span><br><span class="line">        .expand(bsz, seq_len, n_kv_heads, n_rep, head_dim)</span><br><span class="line">        .reshape(bsz, seq_len, n_kv_heads * n_rep, head_dim)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 测试: Repeat_kv函数 ###  </span></span><br><span class="line"><span class="comment"># 注: xk, x_norm已在RoPE, RMSNorm测试中计算,这里用于测试  </span></span><br><span class="line"><span class="comment"># 取消下面的三重引号来执行测试  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">n_rep = ModelArgs.n_heads // ModelArgs.n_kv_heads  </span></span><br><span class="line"><span class="string">keys = repeat_kv(xk, n_rep)  </span></span><br><span class="line"><span class="string">print(f"xk.shape: &#123;xk.shape&#125;")  </span></span><br><span class="line"><span class="string">print(f"keys.shape: &#123;keys.shape&#125;")  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## 测试: Attention函数  </span></span><br><span class="line"><span class="string"># 取消下面的三重引号来执行测试  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">attention = Attention(ModelArgs)  </span></span><br><span class="line"><span class="string">x_out = attention(x_norm,start_pos=0, inference=False)  </span></span><br><span class="line"><span class="string">print(f"x_out.shape: &#123;x_out.shape&#125;")  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment">### 测试结果: ###  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">xk.shape: torch.Size([10, 256, 4, 64])  </span></span><br><span class="line"><span class="string">keys.shape: torch.Size([10, 256, 8, 64])  </span></span><br><span class="line"><span class="string">x_out.shape: torch.Size([10, 256, 512])  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="前馈网络-使用swiglu激活函数">前馈网络 (使用SwiGLU激活函数)</h2>
<p>如图1所示,注意力输出首先经过RMSNorm,然后输入前馈网络。在前馈网络中,注意力输出嵌入会在其隐藏层中扩展到更高维度,学习标记的更复杂特征。</p>
<p><img src="/2024/10/24/llama3-from-scratch/8.webp"> 图8：带有SwiGLU函数的前馈网络</p>
<p>如图所示,SwiGLU函数在正轴上的行为与ReLU相似。然而,在负轴上,SwiGLU输出一些负值,这在学习较小值时可能有用,而不是像ReLU那样在负轴上为平坦的0。根据作者的研究,使用SwiGLU的性能优于ReLU,因此被选用。</p>
<p>前馈网络的代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 步骤2e: 前馈网络 (SwiGLU激活)  </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim: int, hidden_dim: int, multiple_of: int, ffn_dim_multiplier: Optional[float])</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment"># 模型嵌入维度  </span></span><br><span class="line">        self.dim = dim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 我们必须使用Meta提供的隐藏维度计算方法,这是该模型的理想设置  </span></span><br><span class="line">        <span class="comment"># 隐藏维度的计算方式使其是256的倍数  </span></span><br><span class="line">        hidden_dim = int(<span class="number">2</span> * hidden_dim / <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">if</span> ffn_dim_multiplier <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            hidden_dim = int(ffn_dim_multiplier * hidden_dim)</span><br><span class="line">        hidden_dim = multiple_of * ((hidden_dim + multiple_of - <span class="number">1</span>) // multiple_of)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义隐藏层权重  </span></span><br><span class="line">        self.w1 = nn.Linear(self.dim, hidden_dim, bias=<span class="literal">False</span>, device=device)</span><br><span class="line">        self.w2 = nn.Linear(hidden_dim, self.dim, bias=<span class="literal">False</span>, device=device)</span><br><span class="line">        self.w3 = nn.Linear(self.dim, hidden_dim, bias=<span class="literal">False</span>, device=device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 形状: [bsz,seq_len,dim]  </span></span><br><span class="line">        <span class="keyword">return</span> self.w2(F.silu(self.w1(x)) * self.w3(x))</span><br><span class="line"></span><br><span class="line">    <span class="comment">### 测试: 前馈模块 ###  </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注: x_out已在Attention测试中计算,这里用于测试  </span></span><br><span class="line"><span class="comment"># 取消下面的三重引号来执行测试  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">feed_forward = FeedForward(ModelArgs.dim, 4 * ModelArgs.dim, ModelArgs.multiple_of, ModelArgs.ffn_dim_multiplier)  </span></span><br><span class="line"><span class="string">x_out = rms_norm(x_out)  </span></span><br><span class="line"><span class="string">x_out = feed_forward(x_out)  </span></span><br><span class="line"><span class="string">print(f"前馈输出: x_out.shape: &#123;x_out.shape&#125;")  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 测试结果: ###  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">前馈输出: x_out.shape: torch.Size([10, 256, 512])  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h2 id="解码器块">解码器块</h2>
<p>如图1所示,解码器块由多个子组件组成,我们在前面的部分中已经实现了这些组件。以下是解码器块内进行的逐步操作:</p>
<ol type="1">
<li><p>来自输入模块的嵌入首先经过注意力-RMSNorm,然后输入分组查询注意力模块。</p></li>
<li><p>同时,来自输入模块的原始嵌入与注意力输出相加。</p></li>
<li><p>然后,这个结果经过前馈-RMSNorm,输入前馈网络模块。</p></li>
<li><p>前馈网络的输出再次与步骤2的结果相加。</p></li>
<li><p>最终输出被称为解码器输出。这个解码器输出然后作为输入传递给下一个解码器块。这个过程在接下来的31个解码器块中重复。第32个解码器块的最终输出然后传递到输出模块。</p></li>
</ol>
<p>解码器块的代码实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 步骤2f: 解码器块。类名为TransformerBlock,以匹配Meta Llama 3代码库  </span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args: ModelArgs)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.args = args</span><br><span class="line">        <span class="comment"># 初始化注意力的RMSNorm  </span></span><br><span class="line">        self.attention_norm = RMSNorm(dim=args.dim, eps=args.norm_eps)</span><br><span class="line">        <span class="comment"># 初始化注意力类  </span></span><br><span class="line">        self.attention = Attention(args)</span><br><span class="line">        <span class="comment"># 初始化前馈网络的RMSNorm  </span></span><br><span class="line">        self.ff_norm = RMSNorm(dim=args.dim, eps=args.norm_eps)</span><br><span class="line">        <span class="comment"># 初始化前馈网络类  </span></span><br><span class="line">        self.feedforward = FeedForward(args.dim, <span class="number">4</span> * args.dim, args.multiple_of, args.ffn_dim_multiplier)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, start_pos, inference)</span>:</span></span><br><span class="line">        <span class="comment"># start_pos: 推理模式下的标记位置, inference: True表示推理模式,False表示训练模式  </span></span><br><span class="line">        <span class="comment"># 1) 将输入嵌入传递给attention_norm,然后传递给注意力模块  </span></span><br><span class="line">        <span class="comment"># 2) 注意力的输出与原始输入(归一化前)相加  </span></span><br><span class="line">        h = x + self.attention(self.attention_norm(x), start_pos, inference)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1) 将注意力输出传递给ff_norm，然后传递给前馈网络  </span></span><br><span class="line">        <span class="comment"># 2) 前馈网络的输出与注意力输出(ff_norm前)相加  </span></span><br><span class="line">        out = h + self.feedforward(self.ff_norm(h))</span><br><span class="line">        <span class="comment"># 形状: [bsz,seq_len,dim]  </span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="comment">### 测试: TransformerBlock ###  </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取消下面的三重引号来执行测试  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">x = torch.randn((ModelArgs.max_batch_size, ModelArgs.max_seq_len, ModelArgs.dim), device=device)  </span></span><br><span class="line"><span class="string">transformer_block = TransformerBlock(ModelArgs)  </span></span><br><span class="line"><span class="string">transformer_block_out = transformer_block(x,start_pos=0, inference=False)  </span></span><br><span class="line"><span class="string">print(f"transformer_block_out.shape: &#123;transformer_block_out.shape&#125;")  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 测试结果: ###  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">transformer_block_out.shape: torch.Size([10, 64, 128])  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<h1 id="输出模块">3、输出模块</h1>
<p>最后一个解码器块的输出将传入输出模块。它首先经过RMSNorm处理，然后传入线性层生成logits。接下来根据模式的不同，会执行以下两种操作之一：</p>
<p>如果是推理模式，计算top_p概率并生成下一个标记。如果达到最大生成长度或生成的下一个标记为句子结束标记，则停止生成。</p>
<p>如果是训练模式，使用目标标签计算损失，并重复训练直到达到最大epoch数。</p>
<p>下图展示了输出模块的流程：</p>
<p><img src="/2024/10/24/llama3-from-scratch/9.webp"> 图9：Llama 3在训练和推理模式下的输出流程图</p>
<p>最终的Llama 3模型实现</p>
<p>我们将组合三个模块（输入模块、解码器模块和输出模块）的所有组件。这就构成了我们的完整Llama 3模型。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 步骤3: 输出模块  </span></span><br><span class="line"><span class="comment"># 这是Llama 3模型。类名保持为Transformer以匹配Meta Llama 3模型  </span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params: ModelArgs)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment"># 设置params变量中的所有ModelArgs  </span></span><br><span class="line">        self.params = params</span><br><span class="line">        <span class="comment"># 从输入模块初始化嵌入类  </span></span><br><span class="line">        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化解码器块并将其存储在ModuleList中  </span></span><br><span class="line">        <span class="comment"># 这是因为我们的Llama 3模型中有4个解码器块 (官方Llama 3有32个块)  </span></span><br><span class="line">        self.layers = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> layer_id <span class="keyword">in</span> range(params.n_layers):</span><br><span class="line">            self.layers.append(TransformerBlock(args=params))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 为输出模块初始化RMSNorm  </span></span><br><span class="line">        self.norm = RMSNorm(params.dim, eps=params.norm_eps)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在输出模块初始化线性层  </span></span><br><span class="line">        self.output = nn.Linear(params.dim, params.vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, start_pos=<span class="number">0</span>, targets=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># start_pos: 推理模式的标记位置, inference: True表示推理模式, False表示训练模式  </span></span><br><span class="line">        <span class="comment"># x是使用分词器从文本或提示生成的标记ID批次  </span></span><br><span class="line">        <span class="comment"># x[bsz, seq_len] -&gt; h[bsz, seq_len, dim]  </span></span><br><span class="line">        h = self.tok_embeddings(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果目标为None，则激活推理模式并设置为"True"，否则为训练模式"False"  </span></span><br><span class="line">        inference = targets <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 嵌入(h)然后将通过所有解码器块  </span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            h = layer(h, start_pos, inference)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 最后解码器块的输出将馈入RMSNorm  </span></span><br><span class="line">        h = self.norm(h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 归一化后，嵌入h将馈入线性层  </span></span><br><span class="line">        <span class="comment"># 线性层的主要任务是生成将嵌入映射到词汇表大小的logits  </span></span><br><span class="line">        <span class="comment"># h[bsz, seq_len, dim] -&gt; logits[bsz, seq_len, vocab_size]  </span></span><br><span class="line">        logits = self.output(h).float()</span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果目标不可用，则为推理模式  </span></span><br><span class="line">        <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            loss = <span class="literal">None</span></span><br><span class="line">            <span class="comment"># 如果目标可用，则为训练模式。计算损失以进行进一步的模型训练  </span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss = F.cross_entropy(logits.view(<span class="number">-1</span>, self.params.vocab_size), targets.view(<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits, loss</span><br><span class="line"></span><br><span class="line">    <span class="comment">### 测试: Transformer (Llama模型) ###  </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取消下面的三重引号来执行测试  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">model = Transformer(ModelArgs).to(ModelArgs.device)  </span></span><br><span class="line"><span class="string">print(model)  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p><img src="/2024/10/24/llama3-from-scratch/10.webp"> 图10: Llama 3分层架构</p>
<p>我们刚刚构建的Llama 3模型结构看起来很完整。现在我们可以开始训练过程了。</p>
<h1 id="训练llama-3模型">4、训练Llama 3模型</h1>
<p>训练流程在输出模块流程图（图9）中已经展示。在开始训练之前，让我们先实现训练代码。以下代码块中包含了必要的解释。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 步骤4: 训练Llama 3模型:  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用我们在输入模块部分构建的分词器的encode函数，通过对整个tiny_shakespeare数据进行编码来创建数据集  </span></span><br><span class="line">dataset = torch.tensor(encode(data), dtype=torch.int).to(ModelArgs.device)</span><br><span class="line">print(<span class="string">f"dataset-shape: <span class="subst">&#123;dataset.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数从给定数据集生成批次  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset_batch</span><span class="params">(data, split, args: ModelArgs)</span>:</span></span><br><span class="line">    seq_len = args.max_seq_len</span><br><span class="line">    batch_size = args.max_batch_size</span><br><span class="line">    device = args.device</span><br><span class="line"></span><br><span class="line">    train = data[:int(<span class="number">0.8</span> * len(data))]</span><br><span class="line">    val = data[int(<span class="number">0.8</span> * len(data)): int(<span class="number">0.9</span> * len(data))]</span><br><span class="line">    test = data[int(<span class="number">0.9</span> * len(data)):]</span><br><span class="line"></span><br><span class="line">    batch_data = train</span><br><span class="line">    <span class="keyword">if</span> split == <span class="string">"val"</span>:</span><br><span class="line">        batch_data = val</span><br><span class="line">    <span class="keyword">elif</span> split == <span class="string">"test"</span>:</span><br><span class="line">        batch_data = test</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从数据集中选择随机起点，为训练、验证和测试提供随机样本  </span></span><br><span class="line">    ix = torch.randint(<span class="number">0</span>, len(batch_data) - seq_len - <span class="number">3</span>, (batch_size,)).to(device)</span><br><span class="line">    x = torch.stack([torch.cat([token_bos, batch_data[i:i + seq_len - <span class="number">1</span>]]) <span class="keyword">for</span> i <span class="keyword">in</span> ix]).long().to(device)</span><br><span class="line">    y = torch.stack([torch.cat([batch_data[i + <span class="number">1</span>:i + seq_len], token_eos]) <span class="keyword">for</span> i <span class="keyword">in</span> ix]).long().to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 测试: get_dataset函数 ###  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">xs, ys = get_dataset_batch(dataset, split="train", args=ModelArgs)  </span></span><br><span class="line"><span class="string">print([(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))])  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义evaluate_loss函数来计算和存储训练和验证损失，用于日志记录和绘图  </span></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_loss</span><span class="params">(model, args: ModelArgs)</span>:</span></span><br><span class="line">    out = &#123;&#125;</span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> split <span class="keyword">in</span> [<span class="string">"train"</span>, <span class="string">"val"</span>]:</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            xb, yb = get_dataset_batch(dataset, split, args)</span><br><span class="line">            _, loss = model(x=xb, targets=yb)</span><br><span class="line">            losses.append(loss.item())</span><br><span class="line">        out[split] = np.mean(losses)</span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练函数来执行模型训练  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, optimizer, args: ModelArgs)</span>:</span></span><br><span class="line">    epochs = args.epochs</span><br><span class="line">    log_interval = args.log_interval</span><br><span class="line">    device = args.device</span><br><span class="line">    losses = []</span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        xs, ys = get_dataset_batch(dataset, <span class="string">'train'</span>, args)</span><br><span class="line">        xs = xs.to(device)</span><br><span class="line">        ys = ys.to(device)</span><br><span class="line">        logits, loss = model(x=xs, targets=ys)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> epoch % log_interval == <span class="number">0</span>:</span><br><span class="line">            batch_time = time.time() - start_time</span><br><span class="line">            x = evaluate_loss(model, args)</span><br><span class="line">            losses.append(x)</span><br><span class="line">            print(<span class="string">f"Epoch <span class="subst">&#123;epoch&#125;</span> | val loss <span class="subst">&#123;x[<span class="string">'val'</span>]:<span class="number">.3</span>f&#125;</span> | Time <span class="subst">&#123;batch_time:<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line">            start_time = time.time()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 打印最终验证损失  </span></span><br><span class="line">    print(<span class="string">"验证损失: "</span>, losses[<span class="number">-1</span>][<span class="string">'val'</span>])</span><br><span class="line">    <span class="comment"># 在图表中显示间隔损失  </span></span><br><span class="line">    pd.DataFrame(losses).plot()</span><br><span class="line">    plt.savefig(<span class="string">"losses.png"</span>)</span><br><span class="line">    torch.save(model.state_dict(), <span class="string">f"model-<span class="subst">&#123;epoch&#125;</span>.pth"</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>定义完训练函数。就可以开始训练过程，并在训练完成后观察结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 开始训练我们的Llama 3模型  </span></span><br><span class="line">model = Transformer(ModelArgs).to(ModelArgs.device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line">train(model, optimizer, ModelArgs)</span><br></pre></td></tr></table></figure>
<p><img src="/2024/10/24/llama3-from-scratch/11.webp"> 图11. 训练与验证损失图</p>
<p>上图显示了训练和验证损失的变化。训练进行了2500个epoch。使用Google Colab的默认GPU和RAM设置，整个训练过程大约花费了10分钟，这是相当快速的。最后一个epoch的验证损失为2.19，考虑到我们使用的训练数据量和epoch数量，这个结果是可以接受的。要显著降低损失，我们还需要增加训练数据的规模、提高epoch数量，并使用更强大的GPU或处理能力。</p>
<h1 id="llama-3模型推理">5、Llama 3模型推理</h1>
<p>推理流程在输出模块流程图（图9）中已经展示。让我们实现推理代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 步骤5: Llama 3模型推理  </span></span><br><span class="line"><span class="comment"># 这个函数使用我们构建和训练的Llama 3模型，基于提供的提示生成文本序列  </span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(model, prompts: str, params: ModelArgs, max_gen_len: int = <span class="number">500</span>, temperature: float = <span class="number">0.6</span>, top_p: float = <span class="number">0.9</span>)</span>:</span></span><br><span class="line">    <span class="comment"># prompt_tokens: 用户输入文本或提示列表  </span></span><br><span class="line">    <span class="comment"># max_gen_len: 生成文本序列的最大长度  </span></span><br><span class="line">    <span class="comment"># temperature: 用于控制采样随机性的温度值。默认为0.6  </span></span><br><span class="line">    <span class="comment"># top_p: 从logits采样prob输出的top-p概率阈值。默认为0.9  </span></span><br><span class="line">    bsz = <span class="number">1</span>  <span class="comment"># 对于推理，通常用户只输入一个提示，我们将其作为1个批次  </span></span><br><span class="line">    prompt_tokens = token_bos.tolist() + encode(prompts)</span><br><span class="line">    <span class="keyword">assert</span> len(prompt_tokens) &lt;= params.max_seq_len, <span class="string">"提示标记长度应小于max_seq_len"</span></span><br><span class="line">    total_len = min(len(prompt_tokens) + max_gen_len, params.max_seq_len)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这个tokens矩阵用于存储输入提示和模型生成的所有输出  </span></span><br><span class="line">    <span class="comment"># 稍后我们将使用分词器的decode函数来解码这个token，以文本格式查看结果  </span></span><br><span class="line">    tokens = torch.full((bsz, total_len), fill_value=token_pad.item(), dtype=torch.long, device=params.device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将提示tokens填入token矩阵  </span></span><br><span class="line">    tokens[:, :len(prompt_tokens)] = torch.tensor(prompt_tokens, dtype=torch.long, device=params.device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建一个prompt_mask_token，用于稍后识别token是提示token还是填充token  </span></span><br><span class="line">    <span class="comment"># 如果是提示token则为True，如果是填充token则为False  </span></span><br><span class="line">    input_text_mask = tokens != token_pad.item()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 现在我们可以从第一个位置开始，一次使用一个token从prompt_tokens列表开始推理  </span></span><br><span class="line">    prev_pos = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> cur_pos <span class="keyword">in</span> range(<span class="number">1</span>, total_len):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            logits, _ = model(x=tokens[:, prev_pos:cur_pos], start_pos=prev_pos)</span><br><span class="line">        <span class="keyword">if</span> temperature &gt; <span class="number">0</span>:</span><br><span class="line">            probs = torch.softmax(logits[:, <span class="number">-1</span>] / temperature, dim=<span class="number">-1</span>)</span><br><span class="line">            next_token = sample_top_p(probs, top_p)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            next_token = torch.argmax(logits[:, <span class="number">-1</span>], dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        next_token = next_token.reshape(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 只有在是填充token时才替换token  </span></span><br><span class="line">        next_token = torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)</span><br><span class="line">        tokens[:, cur_pos] = next_token</span><br><span class="line"></span><br><span class="line">        prev_pos = cur_pos</span><br><span class="line">        <span class="keyword">if</span> tokens[:, cur_pos] == token_pad.item() <span class="keyword">and</span> next_token == token_eos.item():</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    output_tokens, output_texts = [], []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, toks <span class="keyword">in</span> enumerate(tokens.tolist()):</span><br><span class="line">        <span class="keyword">if</span> token_eos.item() <span class="keyword">in</span> toks:</span><br><span class="line">            eos_idx = toks.index(token_eos.item())</span><br><span class="line">            toks = toks[:eos_idx]</span><br><span class="line"></span><br><span class="line">        output_tokens.append(toks)</span><br><span class="line">        output_texts.append(decode(toks))</span><br><span class="line">    <span class="keyword">return</span> output_tokens, output_texts</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对概率分布执行top-p (nucleus) 采样  </span></span><br><span class="line"><span class="comment"># probs (torch.Tensor): 由logits导出的概率分布张量  </span></span><br><span class="line"><span class="comment"># p: top-p采样的概率阈值  </span></span><br><span class="line"><span class="comment"># 根据相关研究，Top-p采样选择累积概率质量超过阈值p的最小标记集  </span></span><br><span class="line"><span class="comment"># 基于选定的标记重新归一化分布  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_top_p</span><span class="params">(probs, p)</span>:</span></span><br><span class="line">    probs_sort, prob_idx = torch.sort(probs, dim=<span class="number">-1</span>, descending=<span class="literal">True</span>)</span><br><span class="line">    probs_sum = torch.cumsum(probs_sort, dim=<span class="number">-1</span>)</span><br><span class="line">    mask = probs_sum - probs_sort &gt; p</span><br><span class="line">    probs_sort[mask] = <span class="number">0.0</span></span><br><span class="line">    probs_sort.div_(probs_sort.sum(dim=<span class="number">-1</span>, keepdim=<span class="literal">True</span>))</span><br><span class="line">    next_token = torch.multinomial(probs_sort, num_samples=<span class="number">1</span>)</span><br><span class="line">    next_token = torch.gather(prob_idx, <span class="number">-1</span>, next_token)</span><br><span class="line">    <span class="comment"># 返回从词汇表中采样的标记索引  </span></span><br><span class="line">    <span class="keyword">return</span> next_token</span><br></pre></td></tr></table></figure>
<p>对新的提示执行推理，并检查生成的输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 对用户输入的提示执行推理  </span></span><br><span class="line">prompts = <span class="string">"Consider you what services he has done"</span></span><br><span class="line">output_tokens, output_texts = generate(model, prompts, ModelArgs)</span><br><span class="line">output_texts = output_texts[<span class="number">0</span>].replace(<span class="string">"&lt;|begin_of_text|&gt;"</span>, <span class="string">""</span>)</span><br><span class="line">print(output_texts)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出 ##  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">Consider you what services he has done o eretrane  </span></span><br><span class="line"><span class="string">adetranytnn i eey i ade hs rcuh i eey,ad hsatsTns rpae,T  </span></span><br><span class="line"><span class="string">eon o i hseflns o i eee ee hs ote i ocal ersl,Bnnlnface  </span></span><br><span class="line"><span class="string">o i hmr a il nwye ademto nt i a ere  </span></span><br><span class="line"><span class="string">h i ees.  </span></span><br><span class="line"><span class="string">Frm oe o etrane o oregae,alh,t orede i oeral  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>从结果可以看出，我们的Llama 3模型能够对新的提示执行推理并生成文本。虽然考虑到我们使用的训练数据量和训练轮数，输出质量并不是很高，但这证明了模型的基本功能是正常的。通过使用更大规模的训练数据和更多的训练轮数，我们将能够获得更高质量的输出。</p>
<h1 id="总结">总结</h1>
<p>我们已经成功地从零开始构建了自己的Llama 3模型。我们不仅实现了模型的架构，还成功地进行了训练，并能够执行推理以生成新的文本。值得注意的是，我们在相对有限的计算资源（Google Colab Notebook提供的免费GPU和RAM）下，在较短的时间内完成了这个过程。</p>
<p>本文中的代码和方法主要用于教育和研究目的。在实际应用中，可能需要进行更多的优化和调整，以达到生产级别的性能和效果。</p>
 
      <!-- reward -->
      
      <div id="reword-out">
        <div id="reward-btn">
          Donate
        </div>
      </div>
      
    </div>
    

    <!-- copyright -->
    
    <div class="declare">
      <ul class="post-copyright">
        <li>
          <i class="ri-copyright-line"></i>
          <strong>Copyright： </strong>
          
          Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source.
          
        </li>
      </ul>
    </div>
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=http://yoursite.com/2024/10/24/llama3-from-scratch/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
    </footer>
  </div>

   
  <nav class="article-nav">
    
    
      <a href="/2023/05/06/LLM-01-chatGLM%E7%9A%84finetune/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">LLM-01-chatGLM的finetune</div>
      </a>
    
  </nav>

   
<!-- valine评论 -->
<div id="vcomments-box">
  <div id="vcomments"></div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js"></script>
<script>
  new Valine({
    el: "#vcomments",
    app_id: "i0uKYOaVBylrIaKuIP6VoUug-gzGzoHsz",
    app_key: "OSxgPizi7Bk0QMgdHblOg1qF",
    path: window.location.pathname,
    avatar: "mp",
    placeholder: "给我的文章加点评论吧~",
    recordIP: true,
  });
  const infoEle = document.querySelector("#vcomments .info");
  if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
    infoEle.childNodes.forEach(function (item) {
      item.parentNode.removeChild(item);
    });
  }
</script>
<style>
  #vcomments-box {
    padding: 5px 30px;
  }

  @media screen and (max-width: 800px) {
    #vcomments-box {
      padding: 5px 0px;
    }
  }

  #vcomments-box #vcomments {
    background-color: #fff;
  }

  .v .vlist .vcard .vh {
    padding-right: 20px;
  }

  .v .vlist .vcard {
    padding-left: 10px;
  }
</style>

 
   
     
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020-2024
        <i class="ri-heart-fill heart_icon"></i> AlexZhou
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="HomePage"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/ML/">ML</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E9%9A%8F%E7%AC%94/">随笔</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E5%85%B6%E4%BB%96/">其他</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/photos">相册</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/friends">友链</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->
 
<script src="/js/clickBoom2.js"></script>
 
<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.5,"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":300,"height":300,"superSample":2,"hOffset":20,"vOffset":0},"mobile":{"show":false,"scale":0.5},"react":{"opacity":0.8},"log":false});</script></body>

</html>
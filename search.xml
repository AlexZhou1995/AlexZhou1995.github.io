<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>llama3_from_scratch</title>
      <link href="/2024/10/24/llama3-from-scratch/"/>
      <url>/2024/10/24/llama3-from-scratch/</url>
      
        <content type="html"><![CDATA[<blockquote><p>文章来源： https://mp.weixin.qq.com/s/q1BGBbpSWZVqgbEzQ-5Ufg</p></blockquote><p>大家好，本文将详细指导如何从零开始构建完整的Llama 3模型架构，并在自定义数据集上执行训练和推理。</p><p><img src="/2024/10/24/llama3-from-scratch/1.webp"> 图1：Llama 3架构展示训练和推理流程。因为官方Llama 3论文中未提供相关图表。所以此图为大概架构图，阅读本文后你应能绘制出更为精确的架构图。</p><h1 id="本文目标">本文目标</h1><p>通过本文。你可以了解到:</p><ol type="1"><li><p>深入理解Llama 3模型各组件的底层工作原理。</p></li><li><p>编写代码构建Llama 3的每个组件，并将它们组装成一个功能完整的Llama 3模型。</p></li><li><p>编写代码使用新的自定义数据集训练模型。</p></li><li><p>编写代码执行推理，使Llama 3模型能够根据输入提示生成新文本。</p></li></ol><h1 id="输入模块">1、输入模块</h1><p>如图1所示，输入模块包含三个组件：文本/提示、分词器和嵌入。</p><p>输入模块内部工作流程</p><p>让我们通过下图了解输入模块内的工作流程。</p><p><img src="/2024/10/24/llama3-from-scratch/2.webp"> 图2：输入模块流程图，展示提示、分词器和嵌入流程。</p><p>首先，单个或批量文本/提示被输入模型。例如：图中的"Hello World"。</p><p>输入模型的必须是数字格式，因为模型无法直接处理文本。分词器将这些文本/提示转换为标记ID（词汇表中标记的索引号表示）。我们将使用Tiny Shakespeare数据集构建词汇表并训练模型。Llama 3模型使用TikToken作为分词器，这是一种子词分词器。但是我们这个实现将使用字符级分词器。这样做的主要原因是让我们能够自行构建词汇表和分词器，包括编码和解码函数，这样可以深入理解底层工作原理并完全掌控代码。</p><p>每个标记ID将被转换为128维的嵌入向量（原始Llama 3 8B中为4096维）。然后这些嵌入将被传递到下一个解码器模块。</p><p>输入模块代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入必要的库</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Optional, Tuple, List</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment">### 步骤1: 输入模块 ###</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用Tiny Shakespeare数据集实现字符级分词器。部分字符级分词器代码参考自Andrej Karpathy的GitHub仓库</span></span><br><span class="line"><span class="comment"># (https://github.com/karpathy/nanoGPT/blob/master/data/shakespeare_char/prepare.py)</span></span><br><span class="line"><span class="comment"># 加载tiny_shakespeare数据文件 (https://github.com/tamangmilan/llama3/blob/main/tiny_shakespeare.txt)</span></span><br><span class="line"></span><br><span class="line">device: str = <span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>  <span class="comment"># 根据可用性分配设备为cuda或cpu</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载tiny_shakespeare数据文件</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'tiny_shakespeare.txt'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = f.read()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过提取tiny_shakespeare数据中的所有唯一字符准备词汇表</span></span><br><span class="line">vocab = sorted(list(set(data)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练Llama 3模型需要额外的标记，如&lt;|begin_of_text|&gt;、&lt;|end_of_text|&gt;和&lt;|pad_id|&gt;，将它们添加到词汇表中</span></span><br><span class="line">vocab.extend([<span class="string">'&lt;|begin_of_text|&gt;'</span>, <span class="string">'&lt;|end_of_text|&gt;'</span>, <span class="string">'&lt;|pad_id|&gt;'</span>])</span><br><span class="line">vocab_size = len(vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建字符与词汇表中对应整数索引之间的映射。</span></span><br><span class="line"><span class="comment"># 这对于构建分词器的编码和解码函数至关重要。</span></span><br><span class="line">itos = &#123;i: ch <span class="keyword">for</span> i, ch <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line">stoi = &#123;ch: i <span class="keyword">for</span> i, ch <span class="keyword">in</span> enumerate(vocab)&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分词器编码函数：输入字符串，输出整数列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(s)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> [stoi[ch] <span class="keyword">for</span> ch <span class="keyword">in</span> s]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分词器解码函数：输入整数列表，输出字符串</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(l)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">''</span>.join(itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> l)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义稍后在模型训练中使用的张量标记变量</span></span><br><span class="line">token_bos = torch.tensor([stoi[<span class="string">'&lt;|begin_of_text|&gt;'</span>]], dtype=torch.int, device=device)</span><br><span class="line">token_eos = torch.tensor([stoi[<span class="string">'&lt;|end_of_text|&gt;'</span>]], dtype=torch.int, device=device)</span><br><span class="line">token_pad = torch.tensor([stoi[<span class="string">'&lt;|pad_id|&gt;'</span>]], dtype=torch.int, device=device)</span><br><span class="line"></span><br><span class="line">prompts = <span class="string">"Hello World"</span></span><br><span class="line">encoded_tokens = encode(prompts)</span><br><span class="line">decoded_text = decode(encoded_tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment">### 输入模块代码测试 ###</span></span><br><span class="line"><span class="comment"># 取消下面的三重引号来执行测试</span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">print(f"Shakespeare文本字符长度: &#123;len(data)&#125;")  </span></span><br><span class="line"><span class="string">print(f"词汇表内容: &#123;''.join(vocab)&#125;\n")  </span></span><br><span class="line"><span class="string">print(f"词汇表大小: &#123;vocab_size&#125;")  </span></span><br><span class="line"><span class="string">print(f"编码后的标记: &#123;encoded_tokens&#125;")  </span></span><br><span class="line"><span class="string">print(f"解码后的文本: &#123;decoded_text&#125;")  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment">### 测试结果: ###</span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">Shakespeare文本字符长度: 1115394  </span></span><br><span class="line"><span class="string">词汇表内容:  </span></span><br><span class="line"><span class="string"> !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz&lt;|begin_of_text|&gt;&lt;|end_of_text|&gt;&lt;|pad_id|&gt;  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">词汇表大小: 68  </span></span><br><span class="line"><span class="string">编码后的标记: [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42]  </span></span><br><span class="line"><span class="string">解码后的文本: Hello World  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h1 id="解码器模块">2、解码器模块</h1><p>参照图1的架构图，解码器模块包含以下子组件：</p><ul><li><p>RMS归一化</p></li><li><p>旋转位置编码</p></li><li><p>KV缓存</p></li><li><p>分组查询注意力</p></li><li><p>前馈网络</p></li><li><p>解码器块</p></li></ul><h2 id="rms归一化root-mean-square-normalization">RMS归一化（Root Mean Square Normalization）</h2><h3 id="rmsnorm的必要性">RMSNorm的必要性</h3><p>从图1可以看出，输入模块的输出（嵌入向量）经过RMSNorm模块。这是因为嵌入向量具有多个维度（Llama3-8b中为4096维），可能出现不同范围的值。这会导致模型梯度爆炸或消失，从而导致收敛缓慢甚至发散。而RMSNorm将这些值归一化到一定范围，有助于稳定和加速训练过程。这使得梯度具有更一致的幅度，从而加快模型收敛。</p><h3 id="rmsnorm的工作原理">RMSNorm的工作原理</h3><p><img src="/2024/10/24/llama3-from-scratch/3.webp"></p><p>图3：对形状为[3,3]的输入嵌入应用RMSNorm</p><p>类似于层归一化，RMSNorm沿嵌入特征或维度应用。上图中的嵌入形状为[3,3]，意味着每个标记有3个维度。</p><p>示例：对第一个标记X1的嵌入应用RMSNorm：</p><p>X1标记在每个维度上的值（x11、x12和x13）分别除以所有这些值的均方根。公式如图3所示。</p><p>为避免除以零并保证数值稳定性，在均方根中加入一个小常数E（Epsilon）。乘以一个缩放参数Gamma (Y)。每个特征都有一个独特的Gamma参数（如图中d1维度的Y1、d2维度的Y2和d3维度的Y3），这是一个学习参数，可以向上或向下缩放以进一步稳定归一化。gamma参数初始化为1（如上面的计算所示）。</p><p>如示例所示，嵌入值原本较大且分布范围宽。应用RMSNorm后，值变小且范围缩小。计算使用实际的RMSNorm函数完成。</p><p>RMSNorm相比层归一化的优势</p><p>如上例所示没有计算任何均值或方差，而这在层归一化中是必需的。所以RMSNorm通过避免计算均值和方差减少了计算开销。根据作者的研究，RMSNorm在不影响准确性的同时提供了性能优势。</p><p>RMSNorm代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 步骤2: 解码器模块  </span></span><br><span class="line"><span class="comment"># 注：由于Llama 3模型由Meta开发，为了与他们的代码库保持一致并考虑未来兼容性，</span></span><br><span class="line"><span class="comment"># 我将使用Meta GitHub上的大部分代码，并进行必要的修改以实现我们的目标。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义参数数据类：我们将在模型构建、训练和推理过程中使用这些参数。</span></span><br><span class="line"><span class="comment"># 注：为了更快地看到训练和推理结果，而不是专注于高准确性，我们对大多数参数采用较低的值，</span></span><br><span class="line"><span class="comment"># 这些值在Llama 3模型中设置得更高。</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ModelArgs</span>:</span></span><br><span class="line">    dim: int = <span class="number">512</span>  <span class="comment"># 嵌入维度  </span></span><br><span class="line">    n_layers: int = <span class="number">8</span>  <span class="comment"># 模型解码器块的数量  </span></span><br><span class="line">    n_heads: int = <span class="number">8</span>  <span class="comment"># 查询嵌入的头数  </span></span><br><span class="line">    n_kv_heads: int = <span class="number">4</span>  <span class="comment"># 键和值嵌入的头数  </span></span><br><span class="line">    vocab_size: int = len(vocab)  <span class="comment"># 词汇表长度  </span></span><br><span class="line">    multiple_of: int = <span class="number">256</span>  <span class="comment"># 用于计算前馈网络维度  </span></span><br><span class="line">    ffn_dim_multiplier: Optional[float] = <span class="literal">None</span>  <span class="comment"># 用于计算前馈网络维度  </span></span><br><span class="line">    norm_eps: float = <span class="number">1e-5</span>  <span class="comment"># RMSNorm计算的默认Epsilon值  </span></span><br><span class="line">    rope_theta: float = <span class="number">10000.0</span>  <span class="comment"># RePE计算的默认theta值  </span></span><br><span class="line"></span><br><span class="line">    max_batch_size: int = <span class="number">10</span>  <span class="comment"># 最大批量大小  </span></span><br><span class="line">    max_seq_len: int = <span class="number">256</span>  <span class="comment"># 最大序列长度  </span></span><br><span class="line"></span><br><span class="line">    epochs: int = <span class="number">2500</span>  <span class="comment"># 总训练迭代次数  </span></span><br><span class="line">    log_interval: int = <span class="number">10</span>  <span class="comment"># 打印日志和损失值的间隔数    </span></span><br><span class="line">    device: str = <span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>  <span class="comment"># 根据可用性分配设备为cuda或cpu</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 步骤2a: RMSNorm  </span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RMSNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim: int, eps: float = <span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        device = ModelArgs.device</span><br><span class="line">        self.eps = eps</span><br><span class="line">        <span class="comment"># 缩放参数gamma，初始化为1，参数数量等于dim的大小  </span></span><br><span class="line">        self.weight = nn.Parameter(torch.ones(dim).to(device))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_norm</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> x * torch.rsqrt(x.pow(<span class="number">2</span>).mean(dim=<span class="number">-1</span>, keepdim=<span class="literal">True</span>) + self.eps).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 形状: x[bs,seq,dim]  </span></span><br><span class="line">        output = self._norm(x.float()).type_as(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 形状: x[bs,seq,dim] -&gt; x_norm[bs,seq,dim]  </span></span><br><span class="line">        <span class="keyword">return</span> output * self.weight</span><br><span class="line"></span><br><span class="line">    <span class="comment">### RMSNorm代码测试 ###  </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取消下面的三重引号来执行测试  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">x = torch.randn((ModelArgs.max_batch_size, ModelArgs.max_seq_len, ModelArgs.dim), device=device)  </span></span><br><span class="line"><span class="string">rms_norm = RMSNorm(dim=ModelArgs.dim)  </span></span><br><span class="line"><span class="string">x_norm = rms_norm(x)  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">print(f"x的形状: &#123;x.shape&#125;")  </span></span><br><span class="line"><span class="string">print(f"x_norm的形状: &#123;x_norm.shape&#125;")  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment">### 测试结果: ###  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">x的形状: torch.Size([10, 256, 512])  </span></span><br><span class="line"><span class="string">x_norm的形状: torch.Size([10, 256, 512])  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h2 id="旋转位置编码rotary-positional-encoding-rope">旋转位置编码（Rotary Positional Encoding, RoPE）</h2><p>回顾之前的步骤，我们已将输入文本转换为嵌入，并对嵌入应用了RMSNorm。然而，这里存在一个问题：假设输入文本是"I love apple"或"apple love I"，模型会将两个句子视为相同并以相同方式学习。这是因为嵌入中没有为模型定义顺序信息。因此对于任何语言模型来说，保持标记的顺序至关重要。在Llama 3模型架构中，引入了旋转位置编码（RoPE）来定义句子中每个标记的位置，这不仅维护了顺序，还保留了句子中标记的相对位置信息。</p><h3 id="旋转位置编码的工作原理">旋转位置编码的工作原理</h3><p>RoPE是一种位置编码方法，它通过添加绝对位置信息以及包含标记之间的相对位置信息来编码嵌入，从而维护句子中标记的顺序。它通过使用一个特殊的旋转矩阵来旋转给定的嵌入来执行编码操作。这种利用旋转矩阵的简洁而强大的数学推导是RoPE的核心。</p><p><img src="/2024/10/24/llama3-from-scratch/4.webp"> 图4：应用于2维向量的旋转矩阵</p><p>上图展示了旋转矩阵应用于2维向量的情况。Llama 3模型中的维度数是4096，远高于此。我们详细介绍如何对更高维度的嵌入应用旋转。</p><p><img src="/2024/10/24/llama3-from-scratch/5.webp"> 图5：RoPE应用于嵌入的示例</p><p>嵌入的旋转涉及每个嵌入位置(m)值和theta (θ)对每对嵌入维度的乘法。这就是RoPE如何通过实现旋转矩阵来捕获绝对位置和相对位置信息的方式。</p><p>注意：在执行旋转之前，需要将旋转矩阵转换为极坐标形式，并将嵌入向量转换为复数。旋转完成后，旋转后的嵌入需要转换回实数以进行注意力操作。另外RoPE仅应用于查询和键嵌入，不适用于值嵌入。</p><p>RoPE的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 步骤2b: RoPE实现  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">precompute_freqs_cis</span><span class="params">(dim: int, seq_len: int, theta: float = <span class="number">10000.0</span>)</span>:</span></span><br><span class="line">    <span class="comment"># 计算每对维度的Theta值，即dim/2  </span></span><br><span class="line">    device = ModelArgs.device</span><br><span class="line">    freqs = <span class="number">1.0</span> / (theta ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>, device=device)[:(dim // <span class="number">2</span>)].float() / dim))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算序列中位置(m)的范围  </span></span><br><span class="line">    t = torch.arange(seq_len, dtype=torch.float32, device=device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># freqs给出序列中所有标记位置的Theta值范围  </span></span><br><span class="line">    freqs = torch.outer(t, freqs).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这是需要转换为极坐标形式的旋转矩阵，以便对嵌入执行旋转  </span></span><br><span class="line">    freqs_cis = torch.polar(torch.ones_like(freqs).to(device), freqs).to(device)</span><br><span class="line">    <span class="keyword">return</span> freqs_cis</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reshape_for_broadcast</span><span class="params">(freqs_cis, x)</span>:</span></span><br><span class="line">    ndim = x.ndim</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= <span class="number">1</span> &lt; ndim</span><br><span class="line">    <span class="keyword">assert</span> freqs_cis.shape == (x.shape[<span class="number">1</span>], x.shape[<span class="number">-1</span>]), <span class="string">"freqs_cis的最后两个维度必须与x匹配"</span></span><br><span class="line">    shape = [d <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">or</span> i == ndim - <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> i, d <span class="keyword">in</span> enumerate(x.shape)]</span><br><span class="line">    <span class="keyword">return</span> freqs_cis.view(*shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply_rotary_emb</span><span class="params">(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor)</span> -&gt; Tuple[torch.Tensor, torch.Tensor]:</span></span><br><span class="line">    device = ModelArgs.device</span><br><span class="line">    <span class="comment"># 同时对查询和键嵌入应用旋转位置编码  </span></span><br><span class="line">    <span class="comment"># 首先：xq和xk嵌入的最后一个维度需要重塑为一对。因为旋转矩阵应用于每对维度。  </span></span><br><span class="line">    <span class="comment"># 其次：将xq和xk转换为复数，因为旋转矩阵只适用于复数  </span></span><br><span class="line">    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:<span class="number">-1</span>], <span class="number">-1</span>, <span class="number">2</span>)).to(device)  <span class="comment"># xq_:[bsz, seq_len, n_heads, head_dim/2]  </span></span><br><span class="line">    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:<span class="number">-1</span>], <span class="number">-1</span>, <span class="number">2</span>)).to(device)  <span class="comment"># xk_:[bsz, seq_len, n_heads, head_dim/2]  </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 旋转矩阵(freqs_cis)在seq_len(dim=1)和head_dim(dim=3)维度上应与嵌入匹配  </span></span><br><span class="line">    <span class="comment"># 此外，freqs_cis的形状应与xq和xk相同，因此将freqs_cis的形状从[seq_len,head_dim]改变为[1,seq_len,1,head_dim]  </span></span><br><span class="line">    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后，通过与freqs_cis相乘执行旋转操作。  </span></span><br><span class="line">    <span class="comment"># 旋转完成后，将xq_out和xk_out转换回实数并返回  </span></span><br><span class="line">    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(<span class="number">3</span>).to(device)  <span class="comment"># xq_out:[bsz, seq_len, n_heads, head_dim]  </span></span><br><span class="line">    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(<span class="number">3</span>).to(device)  <span class="comment"># xk_out:[bsz, seq_len, n_heads, head_dim]  </span></span><br><span class="line">    <span class="keyword">return</span> xq_out.type_as(xq), xk_out.type_as(xk)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### RoPE代码测试 ###  </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注：x_norm在RMSNorm测试中计算，这里用于测试。  </span></span><br><span class="line"><span class="comment"># 取消下面的三重引号来执行测试  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">head_dim = ModelArgs.dim//ModelArgs.n_heads  </span></span><br><span class="line"><span class="string">wq = nn.Linear(ModelArgs.dim, ModelArgs.n_heads * head_dim, bias=False, device=device)  </span></span><br><span class="line"><span class="string">wk = nn.Linear(ModelArgs.dim, ModelArgs.n_kv_heads * head_dim, bias=False, device=device)  </span></span><br><span class="line"><span class="string">xq = wq(x_norm)  </span></span><br><span class="line"><span class="string">xk = wk(x_norm)  </span></span><br><span class="line"><span class="string">print(f"xq.shape: &#123;xq.shape&#125;")  </span></span><br><span class="line"><span class="string">print(f"xk.shape: &#123;xk.shape&#125;")  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">xq = xq.view(xq.shape[0],xq.shape[1],ModelArgs.n_heads, head_dim)  </span></span><br><span class="line"><span class="string">xk = xk.view(xk.shape[0],xk.shape[1],ModelArgs.n_kv_heads, head_dim)  </span></span><br><span class="line"><span class="string">print(f"xq.re-shape: &#123;xq.shape&#125;")  </span></span><br><span class="line"><span class="string">print(f"xk.re-shape: &#123;xk.shape&#125;")  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">freqs_cis = precompute_freqs_cis(dim=head_dim, seq_len=ModelArgs.max_seq_len)  </span></span><br><span class="line"><span class="string">print(f"freqs_cis.shape: &#123;freqs_cis.shape&#125;")  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">xq_rotate, xk_rotate = apply_rotary_emb(xq, xk, freqs_cis)  </span></span><br><span class="line"><span class="string">print(f"xq_rotate.shape: &#123;xq_rotate.shape&#125;")  </span></span><br><span class="line"><span class="string">print(f"xk_rotate.shape: &#123;xk_rotate.shape&#125;")  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment">### 测试结果: ###  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">xq.shape: torch.Size([10, 256, 512])  </span></span><br><span class="line"><span class="string">xk.shape: torch.Size([10, 256, 256])  </span></span><br><span class="line"><span class="string">xq.re-shape: torch.Size([10, 256, 8, 64])  </span></span><br><span class="line"><span class="string">xk.re-shape: torch.Size([10, 256, 4, 64])  </span></span><br><span class="line"><span class="string">freqs_cis.shape: torch.Size([256, 32])  </span></span><br><span class="line"><span class="string">xq_rotate.shape: torch.Size([10, 256, 8, 64])  </span></span><br><span class="line"><span class="string">xk_rotate.shape: torch.Size([10, 256, 4, 64])  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h2 id="kv缓存">KV缓存</h2><p>在Llama 3架构中，推理阶段引入了KV缓存的概念，用于以键和值缓存的形式存储先前生成的标记。这些缓存用于计算自注意力以生成下一个标记。只缓存键和值标记，而不缓存查询标记，因此称为KV缓存。</p><h3 id="kv缓存的必要性">KV缓存的必要性</h3><p>让我们通过下图来理解KV缓存的重要性。</p><p><img src="/2024/10/24/llama3-from-scratch/6.webp"> 图6：KV缓存实现</p><p>图中的A块：在生成output3标记时，仍在计算先前的输出标记（output1, output2），这是不必要的。这在注意力计算期间导致了额外的矩阵乘法，显著增加了计算资源的使用。</p><p>图中的B块：输出标记替换了查询嵌入中的输入标记。KV缓存存储了先前生成的标记。在注意力分数计算期间，我们只需要使用查询中的1个标记，并使用键和值缓存中的先前标记。这将矩阵乘法从A块的3x3减少到B块的1x3，减少了约66%。在实际应用中，对于巨大的序列长度和批量大小，这将显著减少计算资源的使用。</p><h2 id="分组查询注意力">分组查询注意力</h2><p>分组查询注意力与之前模型（如Llama 1）中使用的多头注意力相似，唯一的区别在于为查询和键/值使用单独的头。分配给查询的头数是键和值头数的n倍。让我们通过图表来进一步理解。</p><p><img src="/2024/10/24/llama3-from-scratch/7.webp"> 图7：分组查询注意力和多头注意力对比</p><p>在给定的图中，多头注意力在所有查询、键和值中都有相等数量的头，即n_heads = 8。</p><p>分组查询注意力块有8个查询头（n_heads）和4个键和值头（n_kv_heads），这是查询头数量的一半。</p><p>分组查询注意力的优势</p><p>尽管多头注意力已经表现出色，引入分组查询注意力是有其特定原因。我们先回顾KV缓存，KV缓存确实大大减少了计算资源的使用。但是随着KV缓存存储越来越多的先前标记，内存使用会显著增加。这对模型性能和计算成本都不利。所以引入了分组查询注意力。减少K和V的头数会减少需要存储的参数数量，从而减少内存使用。多项测试结果表明，使用这种方法模型的准确性仍保持在相近的范围内。</p><p>注意力模块的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 注意力模块 [步骤2c: KV缓存; 步骤2d: 分组查询注意力]  </span></span><br><span class="line"><span class="comment">## 如前所述，命名约定遵循原始Meta LLama3 GitHub  </span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args: ModelArgs)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.args = args</span><br><span class="line">        <span class="comment"># 嵌入维度  </span></span><br><span class="line">        self.dim = args.dim</span><br><span class="line">        <span class="comment"># 分配给查询的头数  </span></span><br><span class="line">        self.n_heads = args.n_heads</span><br><span class="line">        <span class="comment"># 分配给键和值的头数。如果为"None"，则数量与查询相同。  </span></span><br><span class="line">        self.n_kv_heads = args.n_heads <span class="keyword">if</span> args.n_kv_heads <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> args.n_kv_heads</span><br><span class="line">        <span class="comment"># 每个头相对于模型维度的维度  </span></span><br><span class="line">        self.head_dim = args.dim // args.n_heads</span><br><span class="line">        <span class="comment"># 重复次数，以使键、值头数与查询头数匹配  </span></span><br><span class="line">        self.n_rep = args.n_heads // args.n_kv_heads</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化键、查询、值和输出的权重。注意q和kv的权重out_feature值基于其头数  </span></span><br><span class="line">        self.wq = nn.Linear(self.dim, self.n_heads * self.head_dim, bias=<span class="literal">False</span>, device=device)</span><br><span class="line">        self.wk = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=<span class="literal">False</span>, device=device)</span><br><span class="line">        self.wv = nn.Linear(self.dim, self.n_kv_heads * self.head_dim, bias=<span class="literal">False</span>, device=device)</span><br><span class="line">        self.wo = nn.Linear(self.n_heads * self.head_dim, self.dim, bias=<span class="literal">False</span>, device=device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化缓存以在开始时存储键、值 (KV缓存实现)  </span></span><br><span class="line">        self.cache_k = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim), device=args.device)</span><br><span class="line">        self.cache_v = torch.zeros((args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim), device=args.device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x: torch.Tensor, start_pos, inference)</span>:</span></span><br><span class="line">        <span class="comment"># 输入嵌入的形状: [bsz,seq_len,dim]  </span></span><br><span class="line">        bsz, seq_len, _ = x.shape</span><br><span class="line">        <span class="comment"># 掩码将在"训练"期间使用，由于使用KV缓存，"推理"不需要掩码。</span></span><br><span class="line">        mask = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        xq = self.wq(x)  <span class="comment"># x[bsz,seq_len,dim]*wq[dim,n_heads * head_dim] -&gt; q[bsz,seq_len,n_heads * head_dim]  </span></span><br><span class="line">        xk = self.wk(x)  <span class="comment"># x[bsz,seq_len,dim]*wq[dim,n_kv_heads * head_dim] -&gt; k[bsz,seq_len,n_kv_heads * head_dim]  </span></span><br><span class="line">        xv = self.wv(x)  <span class="comment"># x[bsz,seq_len,dim]*wq[dim,n_kv_heads * head_dim] -&gt; v[bsz,seq_len,n_kv_heads * head_dim]  </span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据头数重塑查询、键和值 (分组查询注意力实现)  </span></span><br><span class="line">        xq = xq.view(bsz, seq_len, self.n_heads, self.head_dim)  <span class="comment"># xq[bsz,seq_len,n_heads, head_dim]  </span></span><br><span class="line">        xk = xk.view(bsz, seq_len, self.n_kv_heads, self.head_dim)  <span class="comment"># xk[bsz,seq_len,n_kv_heads, head_dim]  </span></span><br><span class="line">        xv = xv.view(bsz, seq_len, self.n_kv_heads, self.head_dim)  <span class="comment"># xv[bsz,seq_len,n_kv_heads, head_dim]  </span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型 - 推理模式: kv-cache仅在推理模式下启用  </span></span><br><span class="line">        <span class="keyword">if</span> inference:</span><br><span class="line">            <span class="comment"># 计算序列中每个位置的旋转矩阵  </span></span><br><span class="line">            freqs_cis = precompute_freqs_cis(dim=self.head_dim, seq_len=self.args.max_seq_len * <span class="number">2</span>)</span><br><span class="line">            <span class="comment"># 在推理过程中,我们应该只取从当前标记位置开始的旋转矩阵范围  </span></span><br><span class="line">            freqs_cis = freqs_cis[start_pos: start_pos + seq_len]</span><br><span class="line">            <span class="comment"># 将RoPE应用于查询和键嵌入  </span></span><br><span class="line">            xq, xk = apply_rotary_emb(xq, xk, freqs_cis)</span><br><span class="line"></span><br><span class="line">            self.cache_k = self.cache_k.to(xq)</span><br><span class="line">            self.cache_v = self.cache_v.to(xq)</span><br><span class="line">            <span class="comment"># 将键和值标记嵌入存储到它们各自的缓存中 [KV缓存实现]  </span></span><br><span class="line">            self.cache_k[:bsz, start_pos:start_pos + seq_len] = xk</span><br><span class="line">            self.cache_v[:bsz, start_pos:start_pos + seq_len] = xv</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 为注意力计算分配所有直到当前标记位置的先前标记嵌入给键和值变量  </span></span><br><span class="line">            keys = self.cache_k[:bsz, :start_pos + seq_len]</span><br><span class="line">            values = self.cache_v[:bsz, :start_pos + seq_len]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 此时,键和值的形状与查询嵌入不同,但为了计算注意力分数,它们必须相同  </span></span><br><span class="line">            <span class="comment"># 使用repeat_kv函数使键、值的形状与查询形状相同  </span></span><br><span class="line">            keys = repeat_kv(keys, self.n_rep)  <span class="comment"># keys[bsz,seq_len,n_heads,head_dim]  </span></span><br><span class="line">            values = repeat_kv(values, self.n_rep)  <span class="comment"># values[bsz,seq_len,n_heads,head_dim]  </span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模式 - 训练模式: 未实现KV-Cache  </span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 计算旋转矩阵并将RoPE应用于训练的查询和键  </span></span><br><span class="line">            freqs_cis = precompute_freqs_cis(dim=self.head_dim, seq_len=self.args.max_seq_len)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># xq[bsz,seq_len,n_heads, head_dim], xk[bsz,seq_len,n_heads, head_dim]  </span></span><br><span class="line">            xq, xk = apply_rotary_emb(xq, xk, freqs_cis)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 使用repeat_kv函数使键、值的形状与查询形状相同  </span></span><br><span class="line">            <span class="comment"># keys[bsz,seq_len,n_heads,head_dim], #values[bsz,seq_len,n_heads,head_dim]  </span></span><br><span class="line">            keys = repeat_kv(xk, self.n_rep)</span><br><span class="line">            values = repeat_kv(xv, self.n_rep)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 对于训练模式,我们将计算掩码并稍后应用于注意力分数  </span></span><br><span class="line">            mask = torch.full((seq_len, seq_len), float(<span class="string">"-inf"</span>), device=self.args.device)</span><br><span class="line">            mask = torch.triu(mask, diagonal=<span class="number">1</span>).to(self.args.device)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 为了计算注意力,我们需要执行转置操作来重塑所有查询、键和值,将头部放在维度1,序列放在维度2  </span></span><br><span class="line">        xq = xq.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># xq[bsz,n_heads,seq_len,head_dim]  </span></span><br><span class="line">        keys = keys.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># keys[bsz,n_heads,seq_len,head_dim]  </span></span><br><span class="line">        values = values.transpose(<span class="number">1</span>, <span class="number">2</span>)  <span class="comment"># values[bsz,n_heads,seq_len,head_dim]  </span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算注意力分数  </span></span><br><span class="line">        scores = torch.matmul(xq, keys.transpose(<span class="number">2</span>, <span class="number">3</span>)).to(self.args.device) / math.sqrt(self.head_dim)</span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = scores + mask</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 对注意力分数应用softmax  </span></span><br><span class="line">        scores = F.softmax(scores.float(), dim=<span class="number">-1</span>).type_as(xq)</span><br><span class="line">        <span class="comment"># 注意力分数与值的矩阵乘法  </span></span><br><span class="line">        output = torch.matmul(scores, values).to(self.args.device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 我们得到了每个头部的上下文嵌入  </span></span><br><span class="line">        <span class="comment"># 所有头部需要重塑回来并组合,以给出单个上下文注意力输出  </span></span><br><span class="line">        <span class="comment"># 形状变化: output[bsz,n_heads,seq_len,head_dim] -&gt; output[bsz,seq_len, n_heads,head_dim] -&gt; output[bsz,seq_len, n_heads * head_dim]  </span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(bsz, seq_len, <span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 形状: output [bsz,seq_len,dim]  </span></span><br><span class="line">        <span class="keyword">return</span> self.wo(output)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果键/值头的数量少于查询头,此函数使用所需的重复次数扩展键/值嵌入  </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repeat_kv</span><span class="params">(x: torch.Tensor, n_rep: int)</span> -&gt; torch.Tensor:</span></span><br><span class="line">    bsz, seq_len, n_kv_heads, head_dim = x.shape</span><br><span class="line">    <span class="keyword">if</span> n_rep == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        x[:, :, :, <span class="literal">None</span>, :]</span><br><span class="line">        .expand(bsz, seq_len, n_kv_heads, n_rep, head_dim)</span><br><span class="line">        .reshape(bsz, seq_len, n_kv_heads * n_rep, head_dim)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 测试: Repeat_kv函数 ###  </span></span><br><span class="line"><span class="comment"># 注: xk, x_norm已在RoPE, RMSNorm测试中计算,这里用于测试  </span></span><br><span class="line"><span class="comment"># 取消下面的三重引号来执行测试  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">n_rep = ModelArgs.n_heads // ModelArgs.n_kv_heads  </span></span><br><span class="line"><span class="string">keys = repeat_kv(xk, n_rep)  </span></span><br><span class="line"><span class="string">print(f"xk.shape: &#123;xk.shape&#125;")  </span></span><br><span class="line"><span class="string">print(f"keys.shape: &#123;keys.shape&#125;")  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## 测试: Attention函数  </span></span><br><span class="line"><span class="string"># 取消下面的三重引号来执行测试  </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">attention = Attention(ModelArgs)  </span></span><br><span class="line"><span class="string">x_out = attention(x_norm,start_pos=0, inference=False)  </span></span><br><span class="line"><span class="string">print(f"x_out.shape: &#123;x_out.shape&#125;")  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment">### 测试结果: ###  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">xk.shape: torch.Size([10, 256, 4, 64])  </span></span><br><span class="line"><span class="string">keys.shape: torch.Size([10, 256, 8, 64])  </span></span><br><span class="line"><span class="string">x_out.shape: torch.Size([10, 256, 512])  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h2 id="前馈网络-使用swiglu激活函数">前馈网络 (使用SwiGLU激活函数)</h2><p>如图1所示,注意力输出首先经过RMSNorm,然后输入前馈网络。在前馈网络中,注意力输出嵌入会在其隐藏层中扩展到更高维度,学习标记的更复杂特征。</p><p><img src="/2024/10/24/llama3-from-scratch/8.webp"> 图8：带有SwiGLU函数的前馈网络</p><p>如图所示,SwiGLU函数在正轴上的行为与ReLU相似。然而,在负轴上,SwiGLU输出一些负值,这在学习较小值时可能有用,而不是像ReLU那样在负轴上为平坦的0。根据作者的研究,使用SwiGLU的性能优于ReLU,因此被选用。</p><p>前馈网络的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 步骤2e: 前馈网络 (SwiGLU激活)  </span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, dim: int, hidden_dim: int, multiple_of: int, ffn_dim_multiplier: Optional[float])</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment"># 模型嵌入维度  </span></span><br><span class="line">        self.dim = dim</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 我们必须使用Meta提供的隐藏维度计算方法,这是该模型的理想设置  </span></span><br><span class="line">        <span class="comment"># 隐藏维度的计算方式使其是256的倍数  </span></span><br><span class="line">        hidden_dim = int(<span class="number">2</span> * hidden_dim / <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">if</span> ffn_dim_multiplier <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            hidden_dim = int(ffn_dim_multiplier * hidden_dim)</span><br><span class="line">        hidden_dim = multiple_of * ((hidden_dim + multiple_of - <span class="number">1</span>) // multiple_of)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 定义隐藏层权重  </span></span><br><span class="line">        self.w1 = nn.Linear(self.dim, hidden_dim, bias=<span class="literal">False</span>, device=device)</span><br><span class="line">        self.w2 = nn.Linear(hidden_dim, self.dim, bias=<span class="literal">False</span>, device=device)</span><br><span class="line">        self.w3 = nn.Linear(self.dim, hidden_dim, bias=<span class="literal">False</span>, device=device)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 形状: [bsz,seq_len,dim]  </span></span><br><span class="line">        <span class="keyword">return</span> self.w2(F.silu(self.w1(x)) * self.w3(x))</span><br><span class="line"></span><br><span class="line">    <span class="comment">### 测试: 前馈模块 ###  </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注: x_out已在Attention测试中计算,这里用于测试  </span></span><br><span class="line"><span class="comment"># 取消下面的三重引号来执行测试  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">feed_forward = FeedForward(ModelArgs.dim, 4 * ModelArgs.dim, ModelArgs.multiple_of, ModelArgs.ffn_dim_multiplier)  </span></span><br><span class="line"><span class="string">x_out = rms_norm(x_out)  </span></span><br><span class="line"><span class="string">x_out = feed_forward(x_out)  </span></span><br><span class="line"><span class="string">print(f"前馈输出: x_out.shape: &#123;x_out.shape&#125;")  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 测试结果: ###  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">前馈输出: x_out.shape: torch.Size([10, 256, 512])  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h2 id="解码器块">解码器块</h2><p>如图1所示,解码器块由多个子组件组成,我们在前面的部分中已经实现了这些组件。以下是解码器块内进行的逐步操作:</p><ol type="1"><li><p>来自输入模块的嵌入首先经过注意力-RMSNorm,然后输入分组查询注意力模块。</p></li><li><p>同时,来自输入模块的原始嵌入与注意力输出相加。</p></li><li><p>然后,这个结果经过前馈-RMSNorm,输入前馈网络模块。</p></li><li><p>前馈网络的输出再次与步骤2的结果相加。</p></li><li><p>最终输出被称为解码器输出。这个解码器输出然后作为输入传递给下一个解码器块。这个过程在接下来的31个解码器块中重复。第32个解码器块的最终输出然后传递到输出模块。</p></li></ol><p>解码器块的代码实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 步骤2f: 解码器块。类名为TransformerBlock,以匹配Meta Llama 3代码库  </span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TransformerBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args: ModelArgs)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.args = args</span><br><span class="line">        <span class="comment"># 初始化注意力的RMSNorm  </span></span><br><span class="line">        self.attention_norm = RMSNorm(dim=args.dim, eps=args.norm_eps)</span><br><span class="line">        <span class="comment"># 初始化注意力类  </span></span><br><span class="line">        self.attention = Attention(args)</span><br><span class="line">        <span class="comment"># 初始化前馈网络的RMSNorm  </span></span><br><span class="line">        self.ff_norm = RMSNorm(dim=args.dim, eps=args.norm_eps)</span><br><span class="line">        <span class="comment"># 初始化前馈网络类  </span></span><br><span class="line">        self.feedforward = FeedForward(args.dim, <span class="number">4</span> * args.dim, args.multiple_of, args.ffn_dim_multiplier)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, start_pos, inference)</span>:</span></span><br><span class="line">        <span class="comment"># start_pos: 推理模式下的标记位置, inference: True表示推理模式,False表示训练模式  </span></span><br><span class="line">        <span class="comment"># 1) 将输入嵌入传递给attention_norm,然后传递给注意力模块  </span></span><br><span class="line">        <span class="comment"># 2) 注意力的输出与原始输入(归一化前)相加  </span></span><br><span class="line">        h = x + self.attention(self.attention_norm(x), start_pos, inference)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 1) 将注意力输出传递给ff_norm，然后传递给前馈网络  </span></span><br><span class="line">        <span class="comment"># 2) 前馈网络的输出与注意力输出(ff_norm前)相加  </span></span><br><span class="line">        out = h + self.feedforward(self.ff_norm(h))</span><br><span class="line">        <span class="comment"># 形状: [bsz,seq_len,dim]  </span></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="comment">### 测试: TransformerBlock ###  </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取消下面的三重引号来执行测试  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">x = torch.randn((ModelArgs.max_batch_size, ModelArgs.max_seq_len, ModelArgs.dim), device=device)  </span></span><br><span class="line"><span class="string">transformer_block = TransformerBlock(ModelArgs)  </span></span><br><span class="line"><span class="string">transformer_block_out = transformer_block(x,start_pos=0, inference=False)  </span></span><br><span class="line"><span class="string">print(f"transformer_block_out.shape: &#123;transformer_block_out.shape&#125;")  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### 测试结果: ###  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">transformer_block_out.shape: torch.Size([10, 64, 128])  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h1 id="输出模块">3、输出模块</h1><p>最后一个解码器块的输出将传入输出模块。它首先经过RMSNorm处理，然后传入线性层生成logits。接下来根据模式的不同，会执行以下两种操作之一：</p><p>如果是推理模式，计算top_p概率并生成下一个标记。如果达到最大生成长度或生成的下一个标记为句子结束标记，则停止生成。</p><p>如果是训练模式，使用目标标签计算损失，并重复训练直到达到最大epoch数。</p><p>下图展示了输出模块的流程：</p><p><img src="/2024/10/24/llama3-from-scratch/9.webp"> 图9：Llama 3在训练和推理模式下的输出流程图</p><p>最终的Llama 3模型实现</p><p>我们将组合三个模块（输入模块、解码器模块和输出模块）的所有组件。这就构成了我们的完整Llama 3模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 步骤3: 输出模块  </span></span><br><span class="line"><span class="comment"># 这是Llama 3模型。类名保持为Transformer以匹配Meta Llama 3模型  </span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Transformer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, params: ModelArgs)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="comment"># 设置params变量中的所有ModelArgs  </span></span><br><span class="line">        self.params = params</span><br><span class="line">        <span class="comment"># 从输入模块初始化嵌入类  </span></span><br><span class="line">        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化解码器块并将其存储在ModuleList中  </span></span><br><span class="line">        <span class="comment"># 这是因为我们的Llama 3模型中有4个解码器块 (官方Llama 3有32个块)  </span></span><br><span class="line">        self.layers = nn.ModuleList()</span><br><span class="line">        <span class="keyword">for</span> layer_id <span class="keyword">in</span> range(params.n_layers):</span><br><span class="line">            self.layers.append(TransformerBlock(args=params))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 为输出模块初始化RMSNorm  </span></span><br><span class="line">        self.norm = RMSNorm(params.dim, eps=params.norm_eps)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在输出模块初始化线性层  </span></span><br><span class="line">        self.output = nn.Linear(params.dim, params.vocab_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, start_pos=<span class="number">0</span>, targets=None)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># start_pos: 推理模式的标记位置, inference: True表示推理模式, False表示训练模式  </span></span><br><span class="line">        <span class="comment"># x是使用分词器从文本或提示生成的标记ID批次  </span></span><br><span class="line">        <span class="comment"># x[bsz, seq_len] -&gt; h[bsz, seq_len, dim]  </span></span><br><span class="line">        h = self.tok_embeddings(x)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果目标为None，则激活推理模式并设置为"True"，否则为训练模式"False"  </span></span><br><span class="line">        inference = targets <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 嵌入(h)然后将通过所有解码器块  </span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            h = layer(h, start_pos, inference)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 最后解码器块的输出将馈入RMSNorm  </span></span><br><span class="line">        h = self.norm(h)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 归一化后，嵌入h将馈入线性层  </span></span><br><span class="line">        <span class="comment"># 线性层的主要任务是生成将嵌入映射到词汇表大小的logits  </span></span><br><span class="line">        <span class="comment"># h[bsz, seq_len, dim] -&gt; logits[bsz, seq_len, vocab_size]  </span></span><br><span class="line">        logits = self.output(h).float()</span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果目标不可用，则为推理模式  </span></span><br><span class="line">        <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            loss = <span class="literal">None</span></span><br><span class="line">            <span class="comment"># 如果目标可用，则为训练模式。计算损失以进行进一步的模型训练  </span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss = F.cross_entropy(logits.view(<span class="number">-1</span>, self.params.vocab_size), targets.view(<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> logits, loss</span><br><span class="line"></span><br><span class="line">    <span class="comment">### 测试: Transformer (Llama模型) ###  </span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 取消下面的三重引号来执行测试  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">model = Transformer(ModelArgs).to(ModelArgs.device)  </span></span><br><span class="line"><span class="string">print(model)  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p><img src="/2024/10/24/llama3-from-scratch/10.webp"> 图10: Llama 3分层架构</p><p>我们刚刚构建的Llama 3模型结构看起来很完整。现在我们可以开始训练过程了。</p><h1 id="训练llama-3模型">4、训练Llama 3模型</h1><p>训练流程在输出模块流程图（图9）中已经展示。在开始训练之前，让我们先实现训练代码。以下代码块中包含了必要的解释。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 步骤4: 训练Llama 3模型:  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用我们在输入模块部分构建的分词器的encode函数，通过对整个tiny_shakespeare数据进行编码来创建数据集  </span></span><br><span class="line">dataset = torch.tensor(encode(data), dtype=torch.int).to(ModelArgs.device)</span><br><span class="line">print(<span class="string">f"dataset-shape: <span class="subst">&#123;dataset.shape&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数从给定数据集生成批次  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset_batch</span><span class="params">(data, split, args: ModelArgs)</span>:</span></span><br><span class="line">    seq_len = args.max_seq_len</span><br><span class="line">    batch_size = args.max_batch_size</span><br><span class="line">    device = args.device</span><br><span class="line"></span><br><span class="line">    train = data[:int(<span class="number">0.8</span> * len(data))]</span><br><span class="line">    val = data[int(<span class="number">0.8</span> * len(data)): int(<span class="number">0.9</span> * len(data))]</span><br><span class="line">    test = data[int(<span class="number">0.9</span> * len(data)):]</span><br><span class="line"></span><br><span class="line">    batch_data = train</span><br><span class="line">    <span class="keyword">if</span> split == <span class="string">"val"</span>:</span><br><span class="line">        batch_data = val</span><br><span class="line">    <span class="keyword">elif</span> split == <span class="string">"test"</span>:</span><br><span class="line">        batch_data = test</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 从数据集中选择随机起点，为训练、验证和测试提供随机样本  </span></span><br><span class="line">    ix = torch.randint(<span class="number">0</span>, len(batch_data) - seq_len - <span class="number">3</span>, (batch_size,)).to(device)</span><br><span class="line">    x = torch.stack([torch.cat([token_bos, batch_data[i:i + seq_len - <span class="number">1</span>]]) <span class="keyword">for</span> i <span class="keyword">in</span> ix]).long().to(device)</span><br><span class="line">    y = torch.stack([torch.cat([batch_data[i + <span class="number">1</span>:i + seq_len], token_eos]) <span class="keyword">for</span> i <span class="keyword">in</span> ix]).long().to(device)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 测试: get_dataset函数 ###  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">xs, ys = get_dataset_batch(dataset, split="train", args=ModelArgs)  </span></span><br><span class="line"><span class="string">print([(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))])  </span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义evaluate_loss函数来计算和存储训练和验证损失，用于日志记录和绘图  </span></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_loss</span><span class="params">(model, args: ModelArgs)</span>:</span></span><br><span class="line">    out = &#123;&#125;</span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> split <span class="keyword">in</span> [<span class="string">"train"</span>, <span class="string">"val"</span>]:</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            xb, yb = get_dataset_batch(dataset, split, args)</span><br><span class="line">            _, loss = model(x=xb, targets=yb)</span><br><span class="line">            losses.append(loss.item())</span><br><span class="line">        out[split] = np.mean(losses)</span><br><span class="line"></span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练函数来执行模型训练  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, optimizer, args: ModelArgs)</span>:</span></span><br><span class="line">    epochs = args.epochs</span><br><span class="line">    log_interval = args.log_interval</span><br><span class="line">    device = args.device</span><br><span class="line">    losses = []</span><br><span class="line">    start_time = time.time()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        xs, ys = get_dataset_batch(dataset, <span class="string">'train'</span>, args)</span><br><span class="line">        xs = xs.to(device)</span><br><span class="line">        ys = ys.to(device)</span><br><span class="line">        logits, loss = model(x=xs, targets=ys)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> epoch % log_interval == <span class="number">0</span>:</span><br><span class="line">            batch_time = time.time() - start_time</span><br><span class="line">            x = evaluate_loss(model, args)</span><br><span class="line">            losses.append(x)</span><br><span class="line">            print(<span class="string">f"Epoch <span class="subst">&#123;epoch&#125;</span> | val loss <span class="subst">&#123;x[<span class="string">'val'</span>]:<span class="number">.3</span>f&#125;</span> | Time <span class="subst">&#123;batch_time:<span class="number">.3</span>f&#125;</span>"</span>)</span><br><span class="line">            start_time = time.time()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 打印最终验证损失  </span></span><br><span class="line">    print(<span class="string">"验证损失: "</span>, losses[<span class="number">-1</span>][<span class="string">'val'</span>])</span><br><span class="line">    <span class="comment"># 在图表中显示间隔损失  </span></span><br><span class="line">    pd.DataFrame(losses).plot()</span><br><span class="line">    plt.savefig(<span class="string">"losses.png"</span>)</span><br><span class="line">    torch.save(model.state_dict(), <span class="string">f"model-<span class="subst">&#123;epoch&#125;</span>.pth"</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br></pre></td></tr></table></figure><p>定义完训练函数。就可以开始训练过程，并在训练完成后观察结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 开始训练我们的Llama 3模型  </span></span><br><span class="line">model = Transformer(ModelArgs).to(ModelArgs.device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br><span class="line"></span><br><span class="line">train(model, optimizer, ModelArgs)</span><br></pre></td></tr></table></figure><p><img src="/2024/10/24/llama3-from-scratch/11.webp"> 图11. 训练与验证损失图</p><p>上图显示了训练和验证损失的变化。训练进行了2500个epoch。使用Google Colab的默认GPU和RAM设置，整个训练过程大约花费了10分钟，这是相当快速的。最后一个epoch的验证损失为2.19，考虑到我们使用的训练数据量和epoch数量，这个结果是可以接受的。要显著降低损失，我们还需要增加训练数据的规模、提高epoch数量，并使用更强大的GPU或处理能力。</p><h1 id="llama-3模型推理">5、Llama 3模型推理</h1><p>推理流程在输出模块流程图（图9）中已经展示。让我们实现推理代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 步骤5: Llama 3模型推理  </span></span><br><span class="line"><span class="comment"># 这个函数使用我们构建和训练的Llama 3模型，基于提供的提示生成文本序列  </span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span><span class="params">(model, prompts: str, params: ModelArgs, max_gen_len: int = <span class="number">500</span>, temperature: float = <span class="number">0.6</span>, top_p: float = <span class="number">0.9</span>)</span>:</span></span><br><span class="line">    <span class="comment"># prompt_tokens: 用户输入文本或提示列表  </span></span><br><span class="line">    <span class="comment"># max_gen_len: 生成文本序列的最大长度  </span></span><br><span class="line">    <span class="comment"># temperature: 用于控制采样随机性的温度值。默认为0.6  </span></span><br><span class="line">    <span class="comment"># top_p: 从logits采样prob输出的top-p概率阈值。默认为0.9  </span></span><br><span class="line">    bsz = <span class="number">1</span>  <span class="comment"># 对于推理，通常用户只输入一个提示，我们将其作为1个批次  </span></span><br><span class="line">    prompt_tokens = token_bos.tolist() + encode(prompts)</span><br><span class="line">    <span class="keyword">assert</span> len(prompt_tokens) &lt;= params.max_seq_len, <span class="string">"提示标记长度应小于max_seq_len"</span></span><br><span class="line">    total_len = min(len(prompt_tokens) + max_gen_len, params.max_seq_len)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这个tokens矩阵用于存储输入提示和模型生成的所有输出  </span></span><br><span class="line">    <span class="comment"># 稍后我们将使用分词器的decode函数来解码这个token，以文本格式查看结果  </span></span><br><span class="line">    tokens = torch.full((bsz, total_len), fill_value=token_pad.item(), dtype=torch.long, device=params.device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将提示tokens填入token矩阵  </span></span><br><span class="line">    tokens[:, :len(prompt_tokens)] = torch.tensor(prompt_tokens, dtype=torch.long, device=params.device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建一个prompt_mask_token，用于稍后识别token是提示token还是填充token  </span></span><br><span class="line">    <span class="comment"># 如果是提示token则为True，如果是填充token则为False  </span></span><br><span class="line">    input_text_mask = tokens != token_pad.item()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 现在我们可以从第一个位置开始，一次使用一个token从prompt_tokens列表开始推理  </span></span><br><span class="line">    prev_pos = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> cur_pos <span class="keyword">in</span> range(<span class="number">1</span>, total_len):</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            logits, _ = model(x=tokens[:, prev_pos:cur_pos], start_pos=prev_pos)</span><br><span class="line">        <span class="keyword">if</span> temperature &gt; <span class="number">0</span>:</span><br><span class="line">            probs = torch.softmax(logits[:, <span class="number">-1</span>] / temperature, dim=<span class="number">-1</span>)</span><br><span class="line">            next_token = sample_top_p(probs, top_p)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            next_token = torch.argmax(logits[:, <span class="number">-1</span>], dim=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        next_token = next_token.reshape(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 只有在是填充token时才替换token  </span></span><br><span class="line">        next_token = torch.where(input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token)</span><br><span class="line">        tokens[:, cur_pos] = next_token</span><br><span class="line"></span><br><span class="line">        prev_pos = cur_pos</span><br><span class="line">        <span class="keyword">if</span> tokens[:, cur_pos] == token_pad.item() <span class="keyword">and</span> next_token == token_eos.item():</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    output_tokens, output_texts = [], []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, toks <span class="keyword">in</span> enumerate(tokens.tolist()):</span><br><span class="line">        <span class="keyword">if</span> token_eos.item() <span class="keyword">in</span> toks:</span><br><span class="line">            eos_idx = toks.index(token_eos.item())</span><br><span class="line">            toks = toks[:eos_idx]</span><br><span class="line"></span><br><span class="line">        output_tokens.append(toks)</span><br><span class="line">        output_texts.append(decode(toks))</span><br><span class="line">    <span class="keyword">return</span> output_tokens, output_texts</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对概率分布执行top-p (nucleus) 采样  </span></span><br><span class="line"><span class="comment"># probs (torch.Tensor): 由logits导出的概率分布张量  </span></span><br><span class="line"><span class="comment"># p: top-p采样的概率阈值  </span></span><br><span class="line"><span class="comment"># 根据相关研究，Top-p采样选择累积概率质量超过阈值p的最小标记集  </span></span><br><span class="line"><span class="comment"># 基于选定的标记重新归一化分布  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_top_p</span><span class="params">(probs, p)</span>:</span></span><br><span class="line">    probs_sort, prob_idx = torch.sort(probs, dim=<span class="number">-1</span>, descending=<span class="literal">True</span>)</span><br><span class="line">    probs_sum = torch.cumsum(probs_sort, dim=<span class="number">-1</span>)</span><br><span class="line">    mask = probs_sum - probs_sort &gt; p</span><br><span class="line">    probs_sort[mask] = <span class="number">0.0</span></span><br><span class="line">    probs_sort.div_(probs_sort.sum(dim=<span class="number">-1</span>, keepdim=<span class="literal">True</span>))</span><br><span class="line">    next_token = torch.multinomial(probs_sort, num_samples=<span class="number">1</span>)</span><br><span class="line">    next_token = torch.gather(prob_idx, <span class="number">-1</span>, next_token)</span><br><span class="line">    <span class="comment"># 返回从词汇表中采样的标记索引  </span></span><br><span class="line">    <span class="keyword">return</span> next_token</span><br></pre></td></tr></table></figure><p>对新的提示执行推理，并检查生成的输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 对用户输入的提示执行推理  </span></span><br><span class="line">prompts = <span class="string">"Consider you what services he has done"</span></span><br><span class="line">output_tokens, output_texts = generate(model, prompts, ModelArgs)</span><br><span class="line">output_texts = output_texts[<span class="number">0</span>].replace(<span class="string">"&lt;|begin_of_text|&gt;"</span>, <span class="string">""</span>)</span><br><span class="line">print(output_texts)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 输出 ##  </span></span><br><span class="line"><span class="string">"""  </span></span><br><span class="line"><span class="string">Consider you what services he has done o eretrane  </span></span><br><span class="line"><span class="string">adetranytnn i eey i ade hs rcuh i eey,ad hsatsTns rpae,T  </span></span><br><span class="line"><span class="string">eon o i hseflns o i eee ee hs ote i ocal ersl,Bnnlnface  </span></span><br><span class="line"><span class="string">o i hmr a il nwye ademto nt i a ere  </span></span><br><span class="line"><span class="string">h i ees.  </span></span><br><span class="line"><span class="string">Frm oe o etrane o oregae,alh,t orede i oeral  </span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><p>从结果可以看出，我们的Llama 3模型能够对新的提示执行推理并生成文本。虽然考虑到我们使用的训练数据量和训练轮数，输出质量并不是很高，但这证明了模型的基本功能是正常的。通过使用更大规模的训练数据和更多的训练轮数，我们将能够获得更高质量的输出。</p><h1 id="总结">总结</h1><p>我们已经成功地从零开始构建了自己的Llama 3模型。我们不仅实现了模型的架构，还成功地进行了训练，并能够执行推理以生成新的文本。值得注意的是，我们在相对有限的计算资源（Google Colab Notebook提供的免费GPU和RAM）下，在较短的时间内完成了这个过程。</p><p>本文中的代码和方法主要用于教育和研究目的。在实际应用中，可能需要进行更多的优化和调整，以达到生产级别的性能和效果。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>LLM-01-chatGLM的finetune</title>
      <link href="/2023/05/06/LLM-01-chatGLM%E7%9A%84finetune/"/>
      <url>/2023/05/06/LLM-01-chatGLM%E7%9A%84finetune/</url>
      
        <content type="html"><![CDATA[<p>在LLM大火的今天，无数技术研究者想要利用chatGPT提高自己的生产力。但由于其不开源，无法满足各种定制化的需求。在中文环境中，chatGLM是一个较为优秀的开源LLM模型。我们可以基于它进行微调，从而提升它在某些垂直领域的能力，满足定制化的需求。</p><blockquote><p>本文主要参考chatGLM项目中的finetune指引： https://github.com/THUDM/ChatGLM-6B/tree/main/ptuning</p></blockquote><h2 id="环境准备">01-环境准备</h2><p>建议内存48G以上，显存24G以上。</p><p>python环境按照chatGLM项目的需求进行配置即可。</p><p>配置完成后，检查显卡是否可用 <img src="/2023/05/06/LLM-01-chatGLM%E7%9A%84finetune/20230506113847.png"></p><h2 id="模型准备">02-模型准备</h2><p>首先clone项目 https://github.com/THUDM/ChatGLM-6B <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/THUDM/ChatGLM-6B.git</span><br></pre></td></tr></table></figure></p><blockquote><p>文章撰写时的commit_id为2873a6f452340565ff3cd130d5f7009a35c12154 hugging face上模型的commit_id为：658202d88ac4bb782b99e99ac3adff58b4d0b813</p></blockquote><p>然后按照readme中《从本地加载模型》章节的指引，将模型下载至本地。本文将其放置在<code>ChatGLM-6B/model/chatglm-6b</code>目录下。</p><p>修改<code>cli_demo.py</code>中<code>from_pretrained</code>方法的路径，将模型路径改为我们刚才设置的路径下</p><p>通过运行cli_demo.py来测试是否一切OK，如果出现问题，建议逐一check下载文件的sha256是否跟hugging face上一致。</p><p><img src="/2023/05/06/LLM-01-chatGLM%E7%9A%84finetune/20230506130900.png"></p><h2 id="训练数据">03-训练数据</h2><p>chatGLM官方的微调指引中，使用了<a href="https://github.com/THUDM/P-tuning-v2" target="_blank" rel="noopener">P-Tuning v2</a> 的微调技术。数据集使用的是 <a href="https://aclanthology.org/D19-1321.pdf" target="_blank" rel="noopener">ADGEN</a> (广告生成) 数据集。</p><p>ADGEN 数据集任务为根据输入（content）生成一段广告词（summary）。 <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"content"</span>: <span class="string">"类型#上衣*版型#宽松*版型#显瘦*图案#线条*衣样式#衬衫*衣袖型#泡泡袖*衣款式#抽绳"</span>,</span><br><span class="line">    <span class="attr">"summary"</span>: <span class="string">"这件衬衫的款式非常的宽松，利落的线条可以很好的隐藏身材上的小缺点，穿在身上有着很好的显瘦效果。领口装饰了一个可爱的抽绳，漂亮的绳结展现出了十足的个性，配合时尚的泡泡袖型，尽显女性甜美可爱的气息。"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>从 <a href="https://drive.google.com/file/d/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk/view?usp=sharing" target="_blank" rel="noopener">Google Drive</a> 或者 <a href="https://cloud.tsinghua.edu.cn/f/b3f119a008264b1cabd1/?dl=1" target="_blank" rel="noopener">Tsinghua Cloud</a> 下载处理好的 ADGEN 数据集，将解压后的 <code>AdvertiseGen</code> 目录放到项目的目录下。</p><p><img src="/2023/05/06/LLM-01-chatGLM%E7%9A%84finetune/20230506115651.png"></p><h2 id="微调模型">04-微调模型</h2><p>还需要安装依赖 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jieba datasets rouge_chinese</span><br></pre></td></tr></table></figure></p><p>进入<code>ptuning</code>文件夹，修改<code>train.sh</code>中数据集的路径和本地模型的路径，如果按照上图的结构放置训练数据集和模型，则需在路径前加<code>../</code> 其他配置可按需修改。运行p-tuning执行下面命令 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash train.sh</span><br></pre></td></tr></table></figure></p><p><code>quantization_bit</code>可以对模型进行量化，适合显存较小的情况。默认配置（quantization_bit=4、per_device_train_batch_size=1、gradient_accumulation_steps=16）的情况下训练仅需6个G的显存，训练3000个step需要4个多小时。 <img src="/2023/05/06/LLM-01-chatGLM%E7%9A%84finetune/20230506132259.png"></p><p>如果不使用量化，其他参数不变的情况下，我这边实测需要13.5G的显存，但训练速度反而更快了。 <img src="/2023/05/06/LLM-01-chatGLM%E7%9A%84finetune/20230506133044.png"></p><p>上述的默认配置中，除了量化之外的两个参数，表示一次训练迭代会以 1 的批处理大小进行 16 次累加的前后向传播，等效为 16 的总批处理大小。若想在同等批处理大小下提升训练效率，可在二者乘积不变的情况下，加大per_device_train_batch_size的值，但会增加显存消耗。我这里按照实际情况调整了参数，目前已经可以在1.5小时内完成训练了。</p><p><img src="/2023/05/06/LLM-01-chatGLM%E7%9A%84finetune/20230506150312.png"></p><p>如果需要进行全参数的 Finetune，需要安装 <a href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener">Deepspeed</a>，然后运行以下指令： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash ds_train_finetune.sh</span><br></pre></td></tr></table></figure> 由于资源问题（穷），这部分就先跳过了。之后有机会在详细介绍。</p><h3 id="使用自己的数据集微调">使用自己的数据集微调</h3><p>修改 <code>train.sh</code> 和 <code>evaluate.sh</code> 中的 <code>train_file</code>、<code>validation_file</code>和<code>test_file</code>为你自己的 JSON 格式数据集路径，并将 <code>prompt_column</code> 和 <code>response_column</code> 改为 JSON 文件中输入文本和输出文本对应的 KEY。可能还需要增大 <code>max_source_length</code> 和 <code>max_target_length</code> 来匹配你自己的数据集中的最大输入输出长度。</p><h3 id="对话数据集">对话数据集</h3><p>如需要使用多轮对话数据对模型进行微调，可以提供聊天历史，例如以下是一个三轮对话的训练数据： <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"prompt"</span>: <span class="string">"长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线"</span>, <span class="attr">"response"</span>: <span class="string">"用电脑能读数据流吗？水温多少"</span>, <span class="attr">"history"</span>: []&#125;</span><br><span class="line">&#123;<span class="attr">"prompt"</span>: <span class="string">"95"</span>, <span class="attr">"response"</span>: <span class="string">"上下水管温差怎么样啊？空气是不是都排干净了呢？"</span>, <span class="attr">"history"</span>: [[<span class="string">"长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线"</span>, <span class="string">"用电脑能读数据流吗？水温多少"</span>]]&#125;</span><br><span class="line">&#123;<span class="attr">"prompt"</span>: <span class="string">"是的。上下水管都好的"</span>, <span class="attr">"response"</span>: <span class="string">"那就要检查线路了，一般风扇继电器是由电脑控制吸合的，如果电路存在断路，或者电脑坏了的话会出现继电器不吸合的情况！"</span>, <span class="attr">"history"</span>: [[<span class="string">"长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线"</span>, <span class="string">"用电脑能读数据流吗？水温多少"</span>], [<span class="string">"95"</span>, <span class="string">"上下水管温差怎么样啊？空气是不是都排干净了呢？"</span>]]&#125;</span><br></pre></td></tr></table></figure></p><p>训练时需要指定 <code>--history_column</code> 为数据中聊天历史的 key（在此例子中是 <code>history</code>），将自动把聊天历史拼接。要注意超过输入长度 <code>max_source_length</code> 的内容会被截断。</p><p>可以参考以下指令： <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bash train_chat.sh</span><br></pre></td></tr></table></figure></p><h2 id="测试模型">05-测试模型</h2><p>文章撰写时的版本，保存的是新 Checkpoint（只包含 PrefixEncoder 参数），因此也需要load原有的模型参数。这里将上一级目录的<code>cli_demo.py</code>复制到ptuning目录下，并且将model.eval()之前的内容修改为 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> platform</span><br><span class="line"><span class="keyword">import</span> signal</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModel, AutoConfig</span><br><span class="line"></span><br><span class="line">CHECKPOINT_PATH = <span class="string">"../output/adgen-chatglm-6b-pt-128-2e-2/checkpoint-3000"</span></span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">"../model/chatglm-6b"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">config = AutoConfig.from_pretrained(<span class="string">"../model/chatglm-6b"</span>, trust_remote_code=<span class="literal">True</span>, pre_seq_len=<span class="number">128</span>)</span><br><span class="line">model = AutoModel.from_pretrained(<span class="string">"../model/chatglm-6b"</span>, config=config, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line">prefix_state_dict = torch.load(os.path.join(CHECKPOINT_PATH, <span class="string">"pytorch_model.bin"</span>))</span><br><span class="line">new_prefix_state_dict = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> prefix_state_dict.items():</span><br><span class="line">    <span class="keyword">if</span> k.startswith(<span class="string">"transformer.prefix_encoder."</span>):</span><br><span class="line">        new_prefix_state_dict[k[len(<span class="string">"transformer.prefix_encoder."</span>):]] = v</span><br><span class="line">model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)</span><br><span class="line"></span><br><span class="line">model = model.half().cuda()</span><br><span class="line">model.transformer.prefix_encoder.float()</span><br><span class="line">model = model.eval()</span><br></pre></td></tr></table></figure></p><p>运行cli_demo.py后，就可以进行测试了。 <img src="/2023/05/06/LLM-01-chatGLM%E7%9A%84finetune/20230506151424.png"></p><p>对比fine-tune前的模型 <img src="/2023/05/06/LLM-01-chatGLM%E7%9A%84finetune/20230506151727.png"></p><p>可以看到在训练数据集上的回答效果显著提升</p><h2 id="评估模型">06-评估模型</h2><p>评估模型是去评估微调模型的好坏，我们可以调用<code>evaluate.sh</code>来进行评估，其中部分路径也需要进行修改。评测指标为中文 Rouge score 和 BLEU-4，会将评估结果输出到文本文件中。</p><p>官方对比了全量微调，ptuning微调和lora微调的效果。其中LoRA实现采用的是 <a href="https://github.com/yuanzhoulvpi2017/zero_nlp/tree/main/simple_thu_chatglm6b" target="_blank" rel="noopener">simple_thu_chatglm6b</a></p><p>结果如下图所示，在官方的测试中，p-tuning &gt; Finetue &gt; Lora <img src="/2023/05/06/LLM-01-chatGLM%E7%9A%84finetune/20230506134755.png"></p><h2 id="总结">总结</h2><p>本文主要记录了一次对于fine-tune的尝试。下一步可能会去了解一下langchain的细节，做一些有意思的东西。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>LLM-00-chatGLM的使用</title>
      <link href="/2023/05/06/LLM-00-chatGLM%E7%9A%84%E4%BD%BF%E7%94%A8/"/>
      <url>/2023/05/06/LLM-00-chatGLM%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p>其实按照官方的readme一步步执行即可。</p><p>首先clone项目 https://github.com/THUDM/ChatGLM-6B <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/THUDM/ChatGLM-6B.git</span><br></pre></td></tr></table></figure></p><blockquote><p>文章撰写时的commit_id为2873a6f452340565ff3cd130d5f7009a35c12154</p></blockquote><p>安装依赖 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure></p><p>使用transformer包即可下载huggingface的模型并使用，具体的code在官方的readme中。如果需要指定模型的版本，在可以在 <code>from_pretrained</code> 的调用中增加 <code>revision="v0.1.0"</code> 参数。</p><p><img src="/2023/05/06/LLM-00-chatGLM%E7%9A%84%E4%BD%BF%E7%94%A8/20230506161935.jpg"></p><p>此外还有api调用和gradio界面等等方式，这里不再赘述。总之，想要简单的用起来还是很容易的，readme写的很好。</p><p>本文作为LLM系列的开始，内容比较简单，后面会有新的探索会继续更新。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Sam Altman论如何高效工作</title>
      <link href="/2023/03/07/Sam-Altman%E8%AE%BA%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E5%B7%A5%E4%BD%9C/"/>
      <url>/2023/03/07/Sam-Altman%E8%AE%BA%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E5%B7%A5%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<blockquote><p>今天翻译一篇Sam Altman的文章。他是硅谷著名投资人，年仅29岁时就被硅谷创业教父Paul Graham委以Y Combinator新任掌门人的重任，他还和Elon Musk一起创办了OpenAI 公司，可谓是天子骄子，年少有为。</p><p>以下是他对于效率的理解和如何提高效率的方法。</p></blockquote><p>我认为自己至少比普通人效率更高一点，人们有时也会问我提高效率的秘诀，所以我决定写一篇文章，谈谈我对效率的理解。</p><p>「复利」是金融领域的一个概念，但同样在职场上适用。每天提高一点点的效率，50年后的复利增长会让你大吃一惊。所以，现在花费一些时间了解如何提升效率是十分值得的。如果你每天能比别人多做10%的事情，精进1%，复利带来的积极效果将使你脱颖而出。</p><h1 id="你在忙些什么">1. 你在忙些什么</h1><p><strong>如果你在错误的道路上前行，无论你前进的多快，都是在做无用功。选择正确的事情去做，才是提升效率的根本，也是经常容易被人们忽略的部分。</strong> 所以，留出足够的时间想想你的方向吧！独立思考能力是极为稀缺的，但是通过不断地实践，你可以逐渐培养这项能力。</p><p>在我认识的人当中，能给我留下深刻印象的都是那些可以坚持自我的人，这在人群中并不常见。如果你发现自己总是在人云亦云，这可不是一个好的迹象。也许你在反对他人的时候会犯错，但是你需要培养自信心，坚持那些你认为是正确的原则，这样当你在面对重要的决策上，就不会随波逐流，而有勇气坚持己见。</p><p>我确保自己会留出一定的时间思考我应该忙些什么。看一本书，和有趣的人交谈，回归大自然，都可以帮助我的思考。</p><p>在过去的经验中不难发现，如果让我做一些我不喜欢或者不在乎的事情，我无法高效工作。所以，我不会轻易地把自己陷于不得不做某件事的境地（或者委托别人做，或者不做，或者采取其他方法），强迫自己做你不喜欢的事儿只会消磨士气和动力。</p><p>关于委托他人，我想多说一句：请记住，他人在做自己喜欢的事情时也会十分的高产，当你委托他人做你不想做的事情时，请确保这个人确实喜欢做，并且擅长做这件事。</p><p>如果你发现自己一直在做不喜欢的事情，请认真地考虑下转换工作跑道。短期的抱怨和劳累会发生，但如果你在休假后还是感到厌倦和疲累，也许你应该给自己找点喜欢做的事情了。</p><p>我十分幸运的找到了自己喜爱的工作，我甚至可以免费来工作，这是我可以保证高效工作的原因。</p><p>最重要的一点就是知道你可以学习任何你想学的东西，你也可以很快地进步。这听上去好像有点不可思议，但最终你会发现，你确实可以很快的转换跑道。</p><p>好的工作离不开好的同事。确保自己身边的同事是聪明的，高产的，快乐的，积极向上的人，他们不会贬低你的理想和抱负。我喜欢拥有可以鞭策我，鼓励我的同事。尽自己的可能避免那些有毒的同事，他们会严重影响你的工作效率和精力。</p><p>你必须选择正确的问题去解决，这条道路上没有捷径。如果你真的想做一些重要的事情，你必须努力而聪明地工作，尽最大的努力去角逐最终的收获，确实有例外的人（很多伟大的数学家一周并没有工作多长时间），但对大多数人来讲，勤能补拙才是良训。</p><h1 id="优先权">2. 优先权</h1><p>我的高效工作理念有三个支柱：<strong>「确保重要的事儿做完」、「别在蠢事上浪费时间」、「列大量的清单」</strong>。</p><p>这里十分推荐「列清单」这个方法。我会列出每年每月甚至每日要完成的任务列表，清单存在的意义在于让我可以排空大脑进而更加专注，如果我现在不想做某件事，我可以在清单上找到下一件事情来做。</p><p>我倾向于将待办事情写在纸上，但我不会将这些待办事项归类或者预估完成时间，我会在重要的事情后面列个星标，提醒我这些应该优先完成。</p><p>我尝试把待办事宜是否能激发我的动力来进行优先排列。我完成的事情越多，就会越感到开心，从而有动力来做更多的事情。所以我一般会在一天开始或者结束时做一些能让我尽快完成的事情。</p><p>我对于完成重要事情的决心近乎残酷，很多情况下当我真的想完成某事时，我会想法设法让其实现，而最终的结果也如我所愿。</p><p>我经常会无情地说「不」，尽可能快的去处理那些不重要的事情。我有可能在这一点上做的有些过头了，举个例子来说，我回复电子邮件时总是简单粗暴。</p><p>我尽量避免参加会议和活动，因为很多情况下这都是在浪费时间。但是，你应该确保在你的日程中留出足够的时间接触新面孔和新想法，拥有开放的人际关系网络十分重要。尽管我参与的会议中90%的都是在浪费时间，但是也有10%的会议和活动让我受益匪浅，可以弥补那90%的损失。</p><p>我发现最好的会议时间是15-20分钟或者2小时。一般默认的一小时会议通常会有大把的时间被浪费掉。</p><p>根据每天时间的不同，我会安排不同的工作。每天早上起床后的几个小时是我最高效的时候，这时候我会选择自我封闭起来工作，然后在下午见人。当我发现我的注意力偏离时，我会停下来休息，或者转换工作任务，以便保持高效率的工作状态。</p><p>我认为大多数人并没有把时间看的太重要，我实在想不明白有些人明明一小时赚100块钱，但却花费数小时做一些他们不喜欢的事情，只为省下20块钱。</p><p>同时，不要掉入高效率的陷阱中，为了高效而高效并没有什么意义。很多人花费大量的时间去优化自己的时间管理能力，却从没想过自己究竟是不是在正确的方向上做正确的事。如果你本身方向错了，那你挤出再多的时间来做错的事情，结果也不会令自己满意。</p><p>正确的做法是仔细规划好这一年，而不是纠结于规划一天的时间。</p><h1 id="体能状况">3. 体能状况</h1><p>很多情况下，对我有效的方法不一定对你有效。你需要找到自己身体的喜好，如果你可以早日研究透自己的生理规律，将对你生活的各个方面起到积极作用。</p><p>我在过去几年间每周都会花费一定的时间研究尝试适合自己的生活方式，我认为如果我能在如下方面做得足够好，我的效率将得到1.5倍的提高。</p><p>睡眠是影响我效率的最大因素，很多睡眠记录软件可以帮助我提升睡眠质量，我十分推荐Emfit QS+Active这家出品的睡眠记录仪，我只需要提前设置好，就不用再花费任何精力管理它。</p><p>我喜欢在凉爽，黑暗并且安静的房间入睡，我在购买床垫时毫不心疼花费多少钱。对于我来说，好的床垫可以提升我的睡眠质量，所以这项投资是十分值得的，我正在用的是这一家的床垫* 。避免睡前大量进食也对拥有好的睡眠质量起到帮助，避免喝酒也会有效，但我很难遵守这条规则。</p><p>我在睡觉时习惯使用冰袋 ，因为我很难在闷热的环境中入睡。在旅行时，我使用眼罩和耳塞。</p><p>现在我要介绍的方法可能会产生争议，我通常会使用低剂量的安眠药助眠（吞服指定剂量的1/3），或者在很难入睡时吸服大麻。我的睡眠质量总体来说并不理想，尤其是在出差时，我很难入睡，所以吃点药对我有很大的帮助，如果你本身没有睡眠问题，那我不推荐你尝试我的方法。</p><p>当我在清晨花费1--15分钟处理邮件时，我使用全光谱LED灯照明，如果你读完我的文章只想采取一条建议，我推荐你购买这盏灯*，尤其是在旅行中，你也可以随身携带。</p><p>运动是仅次于睡眠的一项影响我效率的因素，我经常花费数月的时间尝试某种运动项目，经年累月下来，我发现每周举重三次，每次一小时对于我来说是最有效的运动方式，我偶尔也会增加HIIT训练，不仅对提升我的效率有用，对我的身体素质提高也大有裨益。</p><p>第三点就是饮食习惯，我一般不吃早餐，所以一天当中我有15个小时处于不进食状态（除了每天醒来后喝一杯浓缩咖啡），我知道这项建议并不是对每一个都适用，但是对我来说却十分适用。</p><p>吃很多糖对我的状态会造成不好的影响，所以我一直尽力避免食用过多糖分，我同时避免食用对我消化系统不利的或者刺激类的食品（比如说辛辣食品）。我对于甜品一直没有抵抗力，所以通常情况下，我会避免购买任何甜品放在家中。</p><p>每天起床后我会引用一杯浓缩咖啡，午饭后再来一杯，我每天的咖啡因摄入量是200毫克，对于我来说足够提神醒脑。但是，有时候如果我感到特别的疲累，或者什么项目临近截止日期，我也会喝很多的咖啡来帮助我抵抗困倦。</p><p>我从小就是素食主义者，我同时服用维生素B-12, Omega-3脂肪酸, 铁, 维生素D-3等营养素。我每个季度会做一次血常规，也会喝大量的蛋白粉，如果我不是素食者，我想我一定不会喝蛋白粉，实在不是我的菜。</p><h1 id="其他建议">4. 其他建议</h1><p>我喜欢的工作环境是：自然光，安静，未经允许不会被打扰，不间断的工作时长，舒适休闲（我拥有一台很漂亮的办公桌，上面有很多台4K显示器，但是我大多数时间还是在坐沙发上和我的笔记本电脑相伴）。</p><p>我编写了一些软件帮助我处理冗杂琐碎的事情，我还学会了如何快速打字，如何使用快捷键来帮我提升工作效率。</p><p><strong>和大多数人一样，我也会有几周毫无斗志（我觉得这可能和饮食有关），这种状态实在令人抓狂，而且经常会在不合时宜的时候发生。我还没有发现彻底地解决之道，但是我通常会随遇而安，知道这种状态早晚会过去，事情确实如此。总体来说，我会避开那些会使我心情变糟的人和事儿，无论你是否想提高效率，都应该考虑下这条建议。</strong></p><p>在给自己安排任务时，应该略微过量些。我发现我总能完成交给我的任务，有时候任务稍微过量可以使我更加专注，更加高效。但是，请一定不要让自己超负荷工作。</p><p>不要为了高产而忽略你的家人和朋友，拿工作和亲情友情去交换实在不划算，不要忽视做自己喜欢的事情。</p><p>最后，我想再重申一下，<strong>在错误的方向上提高效率是毫无意义的，当务之急是先选择好自己的方向。</strong></p><hr><p><strong>延伸阅读：</strong></p><ol type="1"><li>《纽约客》介绍Sam Altman的文章https://zhuanlan.zhihu.com/p/23672766</li><li>原文链接：https://blog.samaltman.com/productivity </li><li>Sam推荐的床垫：https://www.tempurpedic.com/shop-mattresses/tempur-contour-elite/v/715/ </li><li>Sam推荐的灯：https://www.amazon.com/gp/product/B075H39NDL/ </li><li>我的时间管理经验：<a href="http://mp.weixin.qq.com/s?__biz=MzAwODgwMjU2Ng==&amp;mid=2651981957&amp;idx=1&amp;sn=f4d130ffd264836fe79af2ef459091a3" target="_blank" rel="noopener">请问你是真的很忙吗？</a></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 其他 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>渲染 Hexo 博客里的 latex 公式</title>
      <link href="/2021/09/13/LATEX-IN-HEXO/"/>
      <url>/2021/09/13/LATEX-IN-HEXO/</url>
      
        <content type="html"><![CDATA[<h2 id="markdown渲染器的不足">markdown渲染器的不足</h2><p>hexo的默认渲染器hexo-renderer-marked不支持latex语法，一些latex中的符号会被误认为是markdown语法，比如<code>\</code>、<code>_</code>和<code>*</code>，这让mathjax不能正确地渲染（katex需要渲染器支持，根本不能工作）。</p><h2 id="替换pandoc">替换pandoc</h2><p>办法就是替换Hexo的渲染器，比如在博客目录下执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">brew install pandoc</span><br><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-pandoc --save</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Blog </category>
          
          <category> Hexo </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 其他 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CapsuleNetwork调研</title>
      <link href="/2021/09/10/CapsuleNetwork/"/>
      <url>/2021/09/10/CapsuleNetwork/</url>
      
        <content type="html"><![CDATA[<h2 id="为什么提出capsule">为什么提出capsule</h2><p>在讨论胶囊网络前，我们先来看一下目前最通用的深度学习模型之一，卷积神经网络（CNN）。 CNN目前已经完成了很多不可思议的任务，对于整个机器学习领域都产生了很大的影响。然而，CNN善于检测特征，却在探索特征（视角，大小，方位）之间的空间关系方面效果较差。举一个简单的例子，对于一张人脸而言，它的组成部分包括面部轮廓，两个眼睛，一个鼻子和一张嘴巴。对于CNN而言，这些部分就足以识别一张人脸；然而，这些组成部分的相对位置以及朝向就没有那么重要。 <img src="/2021/09/10/CapsuleNetwork/16086894305226.jpg"></p><a id="more"></a><p>一个简单的CNN模型可以正确提取鼻子、眼睛和嘴巴的特征，但会错误地激活神经元进行人脸检测。如果不了解空间方向，大小不匹配，那么对于人脸检测的激活将会太高，比如下图95%。</p><p><img src="/2021/09/10/CapsuleNetwork/16086894931180.jpg"></p><p>现在，假设每个神经元都包含特征的可能性和属性。例如，神经元输出的是一个包含 [可能性，方向，大小] 的向量。利用这种空间信息，就可以检测鼻子、眼睛和耳朵特征之间的方向和大小的一致性，因此对于人脸检测的激活输出就会低很多。</p><p><img src="/2021/09/10/CapsuleNetwork/16086895133550.jpg"></p><p>而如果我们将神经元从标量升级为向量，则相同的胶囊就可以检测不同方向的同一个物体类别</p><p><img src="/2021/09/10/CapsuleNetwork/16086896627419.jpg"></p><h2 id="capsule的结构">Capsule的结构</h2><p>基本一张图搞定，很好理解。这张图下游只有一个向量，实际上可以有多个向量，然后通过动态路由来决定每一个c的值。</p><p><img src="/2021/09/10/CapsuleNetwork/16086902514866.jpg"></p><p><img src="/2021/09/10/CapsuleNetwork/16086901663237.jpg"></p><p>唯一需要想明白的是，为什么要这样做squash</p><h2 id="动态路由">动态路由</h2><p>动态路由确定的是上图的标量c，动态路由的motivation：低层的胶囊将会输入到和它“一致”的胶囊中。 &gt; Lower level capsule will send its input to the higher level capsule that “agrees” with its input. This is the essence of the dynamic routing algorithm.</p><p>动态路由算法如下所示： <img src="/2021/09/10/CapsuleNetwork/16086904139573.jpg"></p><p>也十分的好理解，刚开始将分配权重b设置为0，将l层的向量平均分发给l+1层，计算得出l+1层的结果后，反向计算l层向量到l+1层向量的距离，以此修改分配权重b。把l+1看做是聚类中心就更好理解了。</p><h2 id="mind">MIND</h2><p>MIND在capsule的基础之上，对其进行了一些改进。</p><blockquote><p>胶囊是一种新的神经元，它由传统的神经网络用一个向量来表示，而不是用一个标量来表示。基于向量的胶囊预计能够代表一个实体的不同性质，其中胶囊的方向代表一个属性，胶囊的长度用来表示该属性存在的概率。相应地，多兴趣提取器层的目标是学习表示用户兴趣属性的表示以及是否存在相应的兴趣。胶囊与兴趣表征之间的语义联系促使我们将行为/兴趣表征视为行为/兴趣胶囊，并采用动态路径从行为胶囊中学习兴趣胶囊。然而，原有的图像数据路由算法并不能直接应用于用户行为数据的处理。因此，我们提出了行为兴趣（B2I）动态路由算法，将用户的行为自适应地聚合到兴趣表示向量中，与原有的路由算法在三个方面有所不同：Shared bilinear mapping matrix、Randomly initialized routing logits、Dynamic interest number.</p></blockquote><h3 id="shared-bilinear-mapping-matrix">Shared bilinear mapping matrix</h3><p>其实也是fixed bilinear mapping matrix，也就是上面结构图中的<span class="math inline">\(W\)</span>，对于不同的capsule，只用同一个映射变量<span class="math inline">\(W\)</span>。作者的理由： 1. 用户序列是变长的，因此不好设置某个数量的<span class="math inline">\(W\)</span> 2. 希望item embedding映射到同一个空间</p><p>感觉有道理。</p><h3 id="randomly-initialized-routing-logits">Randomly initialized routing logits</h3><p>顾名思义，高斯分布随机初始化的c，代替原本全0的c</p><h3 id="dynamic-interest-number">Dynamic interest number</h3><p>对于每个用户，兴趣的个数是动态确定的： <span class="math display">\[K_{u}^{\prime}=\max \left(1, \min \left(K, \log _{2}\left(\left|\mathcal{I}_{u}\right|\right)\right)\right)\]</span> 这条感觉是凑数的</p><h2 id="代码">代码</h2><p>可以看 https://github.com/bojone/Capsule 或者这里也有另一个实现： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.initializers <span class="keyword">import</span> RandomNormal</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.keras.layers <span class="keyword">import</span> Layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CapsuleLayer</span><span class="params">(Layer)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_units, out_units, max_len, k_max, iteration_times=<span class="number">3</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 init_std=<span class="number">1.0</span>, **kwargs)</span>:</span></span><br><span class="line">        self.input_units = input_units</span><br><span class="line">        self.out_units = out_units</span><br><span class="line">        self.max_len = max_len</span><br><span class="line">        self.k_max = k_max</span><br><span class="line">        self.iteration_times = iteration_times</span><br><span class="line">        self.init_std = init_std</span><br><span class="line">        super(CapsuleLayer, self).__init__(**kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="comment"># self.routing_logits = self.add_weight(shape=[1, self.k_max, self.max_len],</span></span><br><span class="line">        <span class="comment">#                                       initializer=RandomNormal(stddev=self.init_std),</span></span><br><span class="line">        <span class="comment">#                                       trainable=False, name="B", dtype=tf.float32)</span></span><br><span class="line">        self.bilinear_mapping_matrix = self.add_weight(shape=[self.input_units, self.out_units],</span><br><span class="line">                                                       initializer=RandomNormal(stddev=self.init_std),</span><br><span class="line">                                                       name=<span class="string">"S"</span>, dtype=tf.float32)</span><br><span class="line">        super(CapsuleLayer, self).build(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, **kwargs)</span>:</span></span><br><span class="line">        behavior_embddings, seq_len = inputs</span><br><span class="line">        batch_size = tf.shape(behavior_embddings)[<span class="number">0</span>]</span><br><span class="line">        <span class="comment">#seq_len = tf.squeeze(seq_len)</span></span><br><span class="line">        seq_len_tile = tf.tile(seq_len, [<span class="number">1</span>, self.k_max])</span><br><span class="line"></span><br><span class="line">        routing_logits = tf.stop_gradient(tf.truncated_normal(shape=[<span class="number">1</span>, self.k_max, self.max_len], stddev=self.init_std, name=<span class="string">'B'</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.iteration_times):</span><br><span class="line">            mask = tf.sequence_mask(seq_len_tile, self.max_len)</span><br><span class="line">            pad = tf.ones_like(mask, dtype=tf.float32) * (<span class="number">-2</span> ** <span class="number">32</span> + <span class="number">1</span>)</span><br><span class="line">            routing_logits_with_padding = tf.where(mask, tf.tile(routing_logits, [batch_size, <span class="number">1</span>, <span class="number">1</span>]), pad)</span><br><span class="line">            weight = tf.nn.softmax(routing_logits_with_padding)</span><br><span class="line">            behavior_embdding_mapping = tf.tensordot(behavior_embddings, self.bilinear_mapping_matrix, axes=<span class="number">1</span>)</span><br><span class="line">            Z = tf.matmul(weight, behavior_embdding_mapping)</span><br><span class="line">            interest_capsules = squash(Z)</span><br><span class="line">            delta_routing_logits = tf.reduce_sum(</span><br><span class="line">                tf.matmul(interest_capsules, tf.transpose(behavior_embdding_mapping, perm=[<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])),</span><br><span class="line">                axis=<span class="number">0</span>, keep_dims=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">            routing_logits += delta_routing_logits</span><br><span class="line">        interest_capsules = tf.reshape(interest_capsules, [<span class="number">-1</span>, self.k_max, self.out_units])</span><br><span class="line">        <span class="keyword">return</span> interest_capsules</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (<span class="literal">None</span>, self.k_max, self.out_units)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_config</span><span class="params">(self, )</span>:</span></span><br><span class="line">        config = &#123;<span class="string">'input_units'</span>: self.input_units, <span class="string">'out_units'</span>: self.out_units, <span class="string">'max_len'</span>: self.max_len,</span><br><span class="line">                  <span class="string">'k_max'</span>: self.k_max, <span class="string">'iteration_times'</span>: self.iteration_times, <span class="string">"init_std"</span>: self.init_std&#125;</span><br><span class="line">        base_config = super(CapsuleLayer, self).get_config()</span><br><span class="line">        <span class="keyword">return</span> dict(list(base_config.items()) + list(config.items()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squash</span><span class="params">(inputs)</span>:</span></span><br><span class="line">    vec_squared_norm = tf.reduce_sum(tf.square(inputs), axis=<span class="number">-1</span>, keep_dims=<span class="literal">True</span>)</span><br><span class="line">    scalar_factor = vec_squared_norm / (<span class="number">1</span> + vec_squared_norm) / tf.sqrt(vec_squared_norm + <span class="number">1e-8</span>)</span><br><span class="line">    vec_squashed = scalar_factor * inputs</span><br><span class="line">    <span class="keyword">return</span> vec_squashed</span><br></pre></td></tr></table></figure> 调用的时候 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">high_capsule = CapsuleLayer(input_units=<span class="number">8</span>,</span><br><span class="line">                            out_units=<span class="number">8</span>, max_len=<span class="number">50</span>,</span><br><span class="line">                            k_max=<span class="number">3</span>)((hist_seq, seq_len))</span><br><span class="line"><span class="comment">## hist_seq [None, 50, 8]</span></span><br><span class="line"><span class="comment">## seq_len [None, 50]</span></span><br></pre></td></tr></table></figure></p><h2 id="存在的问题">存在的问题</h2><p>有人通过实验指出，Capsule的Routing算法并不合理（包括一些改进的routing算法）:<a href="http://proceedings.mlr.press/v101/paik19a/paik19a.pdf" target="_blank" rel="noopener">Capsule Networks Need an Improved Routing Algorithm</a>。 文章中提到了一些Routing的方法，这里总结一下： 1. CapNet 《Dynamic routing between capsules》 <img src="/2021/09/10/CapsuleNetwork/16087948402135.jpg"> 1. EMCaps 《Matrix capsules with em routing》 <img src="/2021/09/10/CapsuleNetwork/16087949130702.jpg"> 1. OptimCaps 《An optimization view on dynamic routing between capsules》 <img src="/2021/09/10/CapsuleNetwork/16087949392584.jpg"> 1. GroupCaps 《Group equivariant capsule networks》 <img src="/2021/09/10/CapsuleNetwork/16087949594454.jpg"> 1. AttnCaps 《Dynamic capsule attention for visual question answering》 <img src="/2021/09/10/CapsuleNetwork/16087949936883.jpg"></p><h2 id="参考文献">参考文献</h2><ol type="1"><li><p>https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-iii-dynamic-routing-between-capsules-349f6d30418</p></li><li><p>https://zhuanlan.zhihu.com/p/67910276</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FiBiNet：特征重要性+Bilinear交叉</title>
      <link href="/2020/07/07/FiBiNet/"/>
      <url>/2020/07/07/FiBiNet/</url>
      
        <content type="html"><![CDATA[<p>FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction</p><h2 id="简介">简介</h2><p>文章指出当前的许多通过特征组合进行CTR预估的工作主要使用特征向量的内积或哈达玛积来计算交叉特征，这种方法忽略了特征本身的重要程度。提出通过使用Squeeze-Excitation network (SENET) 结构动态学习特征的重要性以及使用一个双线性函数(Bilinear function)来更好的建模交叉特征。</p><a id="more"></a><h2 id="方法">方法</h2><p>本文的网络结构图如下： <img src="/2020/07/07/FiBiNet/15941047971358.jpg"></p><p>可以看到有2处与普通的CTR模型不同：1.SENET Layer 2.Bilinear-Interaction Layer。我们着重看这两部分。假设ID特征有<span class="math inline">\(f\)</span>个，第<span class="math inline">\(i\)</span>个特征的embedding为<span class="math inline">\(e_i \in R^k\)</span>，<span class="math inline">\(k\)</span>为embed_size，下面先看一次前向的数据流： 1.<span class="math inline">\(E=[e_1,e_2,...,e_f]\)</span>，<span class="math inline">\(A\)</span>=SENET(<span class="math inline">\(E\)</span>)，A为每个ID特征的权重，<span class="math inline">\(A\in R^f\)</span> 2.用权重<span class="math inline">\(A\)</span>给<span class="math inline">\(E\)</span>的对应特征加权，得到了加权后的Embedding <span class="math inline">\(V=[v_1,...,v_f],v_i\in R^k\)</span> 3.<span class="math inline">\(p\)</span>=Bilinear-Interaction(<span class="math inline">\(E\)</span>)，<span class="math inline">\(p=[p_1,...,p_n]\)</span>，其中<span class="math inline">\(n=\frac{f(f-2)}{2}\)</span>，即交叉组合数。同理<span class="math inline">\(q\)</span>=Bilinear-Interaction(<span class="math inline">\(V\)</span>) 4.将<span class="math inline">\(p\)</span>和<span class="math inline">\(q\)</span>concat起来之后过DNN输出结果。</p><h3 id="senet">SENET</h3><h4 id="squeeze">1.Squeeze</h4><p>将每一个ID特征压缩到一个标量，压缩可以是mean-pooling或者max-pooling，下面是mean-pooling的数学表示 <span class="math display">\[z_i = F_{sq}(e_i) = \frac{1}{k}\sum_{t=1}^ke_i^{(t)}\]</span> #### 2.Excitation 得到<span class="math inline">\(z\in R^{f\times 1}\)</span>后，类似AutoEncoder，过两层全连接得到每个特征的权重<span class="math inline">\(A=[a_1,...,a_f]\)</span> <span class="math display">\[A=F_{e x}(Z)=\sigma_{2}\left(W_{2} \sigma_{1}\left(W_{1} Z\right)\right)\]</span> 其中<span class="math inline">\(\sigma\)</span>为激活函数，<span class="math inline">\(W_1\in R^{f\times f/r}\)</span>，<span class="math inline">\(W_2\in R^{f/r \times f}\)</span>，<span class="math inline">\(r\)</span>为缩减比例，据说设置为8比较好。 这一步我理解是把不重要的特征的weight进一步压低。 #### 3.Re-weight 最后一步即是用<span class="math inline">\(A\)</span>缩放<span class="math inline">\(E\)</span>得到<span class="math inline">\(V\)</span> <span class="math display">\[V=F_{R e W e i g h t}(A, E)=\left[a_{1} \cdot e_{1}, \cdots, a_{f} \cdot e_{f}\right]=\left[v_{1}, \cdots, v_{f}\right]\]</span></p><h3 id="bilinear-interaction">Bilinear-Interaction</h3><p>一般的交叉方法用内积（<span class="math inline">\(\cdot\)</span>）或者哈达玛积（<span class="math inline">\(\odot\)</span>） <span class="math display">\[\begin{aligned}\left[a_{1}, a_{2}, \ldots, a_{n}\right] \cdot\left[b_{1}, b_{2}, \ldots, b_{n}\right] &amp;=\sum_{i=1}^{n} a_{i} b_{i} \\\left[a_{1}, a_{2}, \ldots, a_{n}\right] \odot\left[b_{1}, b_{2}, \ldots, b_{n}\right] &amp;=\left[a_{1} b_{1}, a_{2} b_{2}, \ldots, a_{n} b_{n}\right]\end{aligned}\]</span> 作者认为这些方法没有办法很好表示稀疏特征之间的组合，他选择在特征交叉时加入一个(<span class="math inline">\(k\times k\)</span>)的参数<span class="math inline">\(W\)</span> <span class="math display">\[p_{i,j} = v_i \cdot W \odot v_j\]</span> 这样要学习的<span class="math inline">\(W\)</span>会暴增，作者提出了三种方案： 1. Field-ALL：所有的交叉公用一个<span class="math inline">\(W\)</span> 2. Field-Each：每个在左边的<span class="math inline">\(v_i\)</span>都有一个<span class="math inline">\(W_i\)</span>，则一共需要<span class="math inline">\(f\)</span>个<span class="math inline">\(W\)</span> 3. Field-Interaction：每一对<span class="math inline">\((v_i,v_j)\)</span>单独一个<span class="math inline">\(W\)</span>，一共需要<span class="math inline">\(\frac{f(f-1)}{2}\)</span>个<span class="math inline">\(W\)</span></p><p>通过Bilinear-Interaction Layer，<span class="math inline">\(E \to p, V \to q\)</span>，最终<span class="math inline">\([p,q]\)</span>进DNN输出最后的logits。</p>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
          <category> 论文阅读 </category>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AutoInt：构造高阶特征交叉</title>
      <link href="/2020/07/07/AutoInt/"/>
      <url>/2020/07/07/AutoInt/</url>
      
        <content type="html"><![CDATA[<p>AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks (CIKM 2019)</p><a id="more"></a><h2 id="简介">简介</h2><p>本文通过另外的角度来做特征交叉和构造高阶特征，结构如下图 <img src="/2020/07/07/AutoInt/15941030467753.jpg"></p><p>一句话总结：<strong>把Dense特征也转化为embedding，然后所有特征一起过Multi-Head Attention。</strong></p><p>对dense特征做embedding：每个dense特征对应一个嵌入向量，乘以具体的dense特征值 作为其最终的emeddding。</p><h2 id="优缺点">优缺点</h2><p>优点：简单易实现；可以让Dense和ID特征交叉。 缺点：提升似乎不是特别显著，如果模型里有了PNN，看起来再加AutoInt提升不大。</p>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
          <category> 论文阅读 </category>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度召回算法</title>
      <link href="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/"/>
      <url>/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="一简介">一、简介</h2><p>推荐系统的基本架构一般由索引、召回、粗排、精排、重排等几个部分构成，而其中的召回阶段（或者称之为Matching阶段）则主要负责根据用户和内容特征，从众多的内容库中找到用户可能感兴趣的内容。传统的召回算法，如ItemCF等，大多基于统计的方法来计算item之间的相似度，根据用户近期买过的商品进行召回。传统召回算法实现成本低，模型简单响应速度快，但是基于数据统计得到的相似关系缺乏个性化能力，召回商品的发现性，多样性较低。 近年来，随着深度学习的兴起，一些深度召回算法被提出。它们利用深度模型表达能力高，特征提取能力强的特点，提高了个性化召回算法的效果。最初的深度召回算法主要通过深度神经网络挖掘用户和商品在低维表示空间上的相似性，为用户召回低维表示相似的商品，这类方法我们往往称之为向量化召回算法。相比于基于数据统计的相似性计算，向量化召回的个性化程度更高，召回商品的发现性和多样性也较高。还有一些深度召回算法尝试建立商品之间的关系图，通过基于图的深度网络学习，从拓扑关系的角度捕捉商品之间的相关性，这类方法我们称之为基于图的深度召回算法。最后，还有一些方法致力于攻克深度模型带来的性能开销，使得深度模型能够在大规模的召回场景中可用，比如通过树结构和哈希的方法对召回的过程进行提速等等，这类方法我们称之为大规模深度召回算法。本文就从这三类方法展开，介绍一些现有的深度召回算法。</p><a id="more"></a><h2 id="二分类">二、分类</h2><ul><li><p>DeepMatch youtube在2016年提出了用深度模型进行多分类的监督训练，得到item和user的embedding最后在线上通过内积进行召回检索，算是深度模型时代向量化召回的开端。本文中将通过分类任务监督训练得到embedding最后通过内积进行检索的方法都归到这一类</p></li><li><p>Graph-based 这类方法大多同样是产出item和user的embedding，最后通过向量内积进行召回检索。但是这类方法引入图的结构来描述item和user之间的关系，他们声称相比于直接进行分类训练，引入了额外的拓扑信息。</p></li><li><p>Large-scale 这类方法可能属于上面两类方法，但是他们着重处理Large-scale的问题，致力于缩短召回检索的开销。</p></li></ul><h2 id="三deepmatch">三、DeepMatch</h2><h3 id="deep-neural-networks-for-youtube-recommendations.-recsys-2016">1. Deep neural networks for youtube recommendations. (RecSys 2016)</h3><p><strong>结构和特征</strong> 本文提出了一个较为基础的个性化向量召回算法，结构如下 <img src="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/15855389430461.jpg"></p><p>主要特征： (a) 历史搜索query：把历史搜索的query分词后的token的embedding向量进行加权平均 (b) 人口统计学信息：性别、年龄、地域等 (c) 其他上下文信息 (d) Age信息：视频上传时间，保证时效性。</p><p><strong>训练和生效</strong> 离线训练时把问题建模成一个多分类问题，对用户U和上下文C，预测视频V是否点击，数学表示如下： <img src="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/15855396893807.jpg"> 其中u和v为用户和商品的embedding，内积后进行softmax多分类。最后线上生效直接使用内积选取Top—N的视频。</p><h3 id="sequential-deep-matching-model-for-online-large-scale-recommender-system.-cikm-2019">2. Sequential Deep Matching Model for Online Large-scale Recommender System. (CIKM 2019)</h3><p>本文相比于上面Youtube的工作，引入了用户行为序列的信息进行序列化建模。示意图如下 <img src="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/15855399525004.jpg"> 对于user，这边用user prediction network（上图虚线框）来提取用户向量，user prediction network中，使用Attention来提取用户长周期的特征(防止遗忘)，使用LSTM提取短周期的特征(注重实效)，最后通过一个门结构将长短周期和user embedding进行混合得到最终的用户向量表示。</p><h2 id="四graph-based">四、Graph-based</h2><h3 id="billion-scale-commodity-embedding-for-e-commerce-recommendation-in-alibaba.-kdd-2018">1. Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba. (KDD 2018)</h3><p>本文提出了一种基于图网络的低维表示提取方法，并且提出了两种改进方案。 提取的低维表示用于做个性化召回检索</p><h4 id="bge">BGE</h4><p>首先最基础的图网络提取低维表示的方法，BGE（Base Graph Embedding）如下： (1) 构建图：商品为点，用户行为为边，构建带权图，表示商品之间的关联。 (2) 采样：在图上进行Random walk的采样，得到一些商品序列，相关的商品在序列中同时出现的概率较高 (3) 训练：将采样得到的商品序列当做句子，其中每一个商品当做一个词、用NLP中的Skip-gram方法进行训练，得到每一个词的词向量，也就是每一个商品的向量。 <img src="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/15855442964204.jpg"></p><h4 id="ges-eges">GES &amp; EGES</h4><p>上述的BGE方法难以处理冷启动的问题，新商品没有任何点击，无法产生有效的低维表示。 为了解决这个问题，本文提出了改进方案GES（Graph Embedding with Side information） 相比于只对商品学习低维表示，本方法还一并学习商品的其他属性信息。如下图SI0为item_id信息，SI1可能是store_id信息，etc. <img src="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/15855447612156.jpg"> 最终一个商品的低维表示由所有的Side Information向量求均值得到： <img src="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/15855448211571.jpg"></p><p>而EGES（Enhanced Graph Embedding with Side information）是GES的改进，认为不同的Information有着不同的重要性，最终商品的低维表示是各个Side Information的向量加权求和得到，权值也在训练中学习。 <img src="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/15855448929951.jpg"></p><h3 id="graph-convolutional-neural-networks-for-web-scal-recommender-systems.-kdd-2018">2. Graph Convolutional Neural Networks for Web-Scal Recommender Systems. (KDD 2018)</h3><p>本文提出用图卷积网络来提取商品的低维表示。 图的构建方式还是同上，商品为点，用户行为作为边，构建一个图。但是本文用图卷积来生成一个商品的向量，即一个商品的向量通过他的近邻混合得到。下图展示了一个2层的图卷积，对于商品A，他的近邻是BCD，而BCD的近邻分别又是AC，ABEF，A。则商品A的最终向量由两层级的卷积操作得到。 <img src="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/15855475119931.jpg"> 一次卷积操作的过程如下，实际上就是对所有近邻的向量过一层全连接，然后通过pooling得到<span class="math inline">\(n_u\)</span>，然后将目标向量<span class="math inline">\(z_u\)</span>和<span class="math inline">\(n_u\)</span> concat之后再过一个全连接，得到新的目标向量<span class="math inline">\(z_n^{new}\)</span> <img src="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/15855478737833.jpg"></p><p>训练使用了max-margin ranking loss，数据组织为<span class="math inline">\(&lt;i,j,l&gt;\)</span>，其中<span class="math inline">\(&lt;i,j&gt;\)</span>为一个商品对，<span class="math inline">\(l\)</span>为label，表示这一对商品是否相关。</p><h2 id="五largescale">五、Large—Scale</h2><h3 id="candidate-generation-with-binary-codes-for-large-scale-top-n-recommendation.-cikm-2019">1. Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation. (CIKM 2019)</h3><p>本文侧重关注召回检索的效率，结合哈希检索和向量化召回。通过对输出向量施加逐渐逼近符号函数的约束，使得生成的向量是二值的，以便使用哈希检索结构Multi-Index Hashing进行检索。这种检索方法的耗时与总商品数据量是一种次线性的关系。</p><p>本文其他部分与大多数DeepMatch类的算法相似，但是最后内积计算分类loss之前，对商品和用户向量施加一个符号函数<span class="math inline">\(sgn(x)\)</span>来让输出的向量成为二值的。但是，符号函数不可导，这里就用带参数的tanh函数来进行逼近，随着训练epoch逐渐增加参数<span class="math inline">\(\beta\)</span>大小，使其逼近符号函数<span class="math inline">\(sgn\)</span>。损失函数和训练过程如下图。 <img src="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/15855485459116.jpg"> <img src="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/15855485133191.jpg"></p><h3 id="learning-tree-based-deep-model-for-recommender-systems.-kdd-2018">2. Learning tree-based deep model for recommender systems. (KDD 2018)</h3><p>向量化召回由于使用向量内积作为最后的结果，表达能力有限，因此难以取得较好的个性化效果。如果使用深度模型，则超大的候选集合使得线上的延迟难以接受。本文结合了最大堆树和深度模型，提出了TDM算法。将庞大数量的商品分配到树的各个叶子节点，每个中间节点相当于其子节点商品的一个抽象表示，通过BeamSearch的方法对树结构进行高效检索。选取Top-K的商品，只需要深度模型做<span class="math inline">\(Klog(N)\)</span>次预测。 <img src="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/15855493488403.jpg" alt="w400"></p><p>在训练过程中，每一个中间节点也被当做一个普通的item进行处理。 具体的训练方式是 树的结构 和 深度模型 交替进行更新。 当树的结构固定时，对于一条样本，如果用户点击了一个item，则这个item的搜索父节点也被视作点击。如此便可构造数据集对深度模型进行训练。 而树的结构，则是通过对当前的embedding进行聚类得到。 <img src="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/15855518986729.jpg"></p><h3 id="joint-optimization-of-tree-based-index-and-deep-model-for-recommender-systems.-nips-2019">3. Joint Optimization of Tree-based Index and Deep Model for Recommender Systems. (NIPS 2019)</h3><p>本文是对上面TDM算法的改进。上面TDM算法中，树的构建和深度模型的训练是分隔开的，目标并不相同，可能两者的优化相互牵制导致总体效果次优。 本文主要有两个改进： 1.树结构、深度模型联合优化 通过最大似然构建联合优化的loss function。其中树结构的优化难以求解，转化为带权二部图的最大匹配问题，通过贪心算法求解。 2.用户序列特征分层建模 深度模型中用到了用户的行为序列，TDM中训练数的中间层时在序列特征方面也是用的item_id粒度的序列特征。在本文中改进为使用当前层的embedding作为序列的特征。这样可以减少每层训练的噪声，并且可以从粗到细的精准建模。</p><h2 id="六总结">六、总结</h2><h4 id="deep-neural-networks-for-youtube-recommendations">1. Deep neural networks for youtube recommendations</h4><p>方法名：无 动机：使用深度模型进行个性化的召回，同时保证效率 方法：通过深度模型多分类任务训练，得到用户和商品的向量，用向量内积做召回检索。</p><h4 id="sequential-deep-matching-model-for-online-large-scale-recommender-system">2. Sequential Deep Matching Model for Online Large-scale Recommender System</h4><p>方法名：SDM 动机：建模用户行为序列 方法：引入用户长短期行为序列，分别用Attention和LSTM结构进行建模，最终进行向量召回。</p><h4 id="billion-scale-commodity-embedding-for-e-commerce-recommendation-in-alibaba.">3. Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba.</h4><p>方法名：BGE，GES，EGES 动机：使用图网络表示商品之间的关系，再学习商品的低维表示 方法：使用用户行为建图，通过采样得到多条商品轨迹。将轨迹当成句子，商品当做词，通过NLP中求解词向量的方法求解商品向量。并且使用商品其他属性的向量来缓解冷启动问题。</p><h4 id="graph-convolutional-neural-networks-for-web-scal-recommender-systems.">4. Graph Convolutional Neural Networks for Web-Scal Recommender Systems.</h4><p>方法名：PinSage 动机：使用图表示商品之间的关系，再使用图卷积网络学习商品的低维表示 方法：每一个商品的embedding由其近邻的embedding通过图卷积操作得到。通过max-margine ranking loss进行训练。</p><h4 id="candidate-generation-with-binary-codes-for-large-scale-top-n-recommendation.">5. Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation.</h4><p>方法名：CICAR 动机：通过高效哈希检索结构来提升召回的效率 方法：使用次线性复杂度的哈希检索方法Multi-index hashing，需要embedding是二值的。因此在现有双塔内积网络的基础上，使用一个近似符号函数的映射函数<span class="math inline">\(tanh_\beta\)</span>将输出的向量映射到二值。</p><h4 id="learning-tree-based-deep-model-for-recommender-systems.">6. Learning tree-based deep model for recommender systems.</h4><p>方法名：TDM 动机：通过内积进行召回表达能力有限，使用深度模型打分则商品太多开销过大。 方法：在全部N个商品集合上构建树的结构，在树上通过BeamSearch进行检索TopK商品，深度模型只需要预测<span class="math inline">\(Klog(N)\)</span>次。树的中间节点是商品的抽象，当做商品一次训练，其label来自于子节点。深度模型和树结构交替训练。</p><h4 id="joint-optimization-of-tree-based-index-and-deep-model-for-recommender-systems.">7. Joint Optimization of Tree-based Index and Deep Model for Recommender Systems.</h4><p>方法名：JTM 动机：TDM中深度模型和树结构训练是完全分隔开的，目标不同可能导致相互牵制无法训练到最优。 方法：构建统一loss function，深度模型和树结构联合优化。树结构的优化是个较难的组合优化问题，转化为带权二部图匹配问题后用贪心算法求解。</p><h2 id="七结语">七、结语</h2><p>本文介绍了近几年内的一些深度召回算法，主要把它们划分成“Deep Matching类算法”、“基于图的算法”、“面向大规模场景算法”3类算法，并对这3类算法进行归纳和总结，分析他们的动机、做法和优缺点。深度学习在推荐系统中的应用是近年来持续的热门研究课题，每年都会有大量新技术和新模型出现。我们希望这篇文章能帮助读者对这个领域有一个大致的了解，并为未来的研究提供一些思路和参考。</p>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pandas处理MovieLen25M数据集</title>
      <link href="/2020/03/21/Pandas%E5%A4%84%E7%90%86MovieLen25M%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
      <url>/2020/03/21/Pandas%E5%A4%84%E7%90%86MovieLen25M%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p>最近做了一些推荐侧召回的实验尝试，除了在业务的数据上进行测试，我还想在公开数据集上进行验证。参考Related works，最终选择使用MovieLen和Amazon的数据集。由于这两个数据集给的是裸的数据，因此需要我们根据自己的需要做一些处理。这里我用pandas来做数据的分析和处理。</p><a id="more"></a><p>Pandas是数据科学常用的工具，但是说来惭愧，我之前倒是很少用Pandas。因为之前做强化学习较多，不太需要大量的这种数据分析。因此这次也就趁机在熟悉一下。我个人觉得相比于算法原理，工具类的东西大致记录一下就好。工作中用到多了自然就记住了，记不住的说明平时也不怎么用，到时候再查就好，相关的文档网上有很多很多，倒是不必死记。</p><p>废话不多说，直接上Notebook吧，该说的都在注释里。（下面的链接是个html，但是如果我以HTML的格式放到资源文件夹里，编译后blog首页排版会有错） <a href="/2020/03/21/Pandas%E5%A4%84%E7%90%86MovieLen25M%E6%95%B0%E6%8D%AE%E9%9B%86/ml_data_process" title="[pandas数据处理notebook]">[pandas数据处理notebook]</a></p><p>经过这样处理，我得到了几个处理后的表，表的含义见notebook <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">viewed_sequence</span><br><span class="line">genome_feature</span><br><span class="line">genres_tag_one_hot_feature</span><br><span class="line">movie_rating_feature</span><br><span class="line">tag_feature_movie</span><br><span class="line">tag_feature_user</span><br><span class="line">user_rating_feature</span><br></pre></td></tr></table></figure></p><p>最后通过python+MPI并行的聚合生成最后的数据，写入到本地文件 <a href="/2020/03/21/Pandas%E5%A4%84%E7%90%86MovieLen25M%E6%95%B0%E6%8D%AE%E9%9B%86/run.py" title="[python+MPI产出聚合数据集]">[python+MPI产出聚合数据集]</a></p><p>不过最终处理出来的数据集有十几个G，受限于线上docker容器的内存限制，无法放到一个文件里序列化存储和恢复，还是得通过file的方式来读，算是没有完美的达到预期，但是问题不大。</p>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2020年03月15日</title>
      <link href="/2020/03/16/2020-03-15/"/>
      <url>/2020/03/16/2020-03-15/</url>
      
        <content type="html"><![CDATA[<p>今天感觉最近的生活状态不是很对，花了很多时间在没有特别多意义的事情上。如何能够专注在一个事情上，做更深入的思考，这时一个值得探索的课题。不仅是做完，更是打磨完善，直到超出某个阈值，这样才能把事情做到让旁人觉得值得点赞的程度。</p><a id="more"></a><p>此外，“以为自己想明白了”和“自己确实想明白了”之前是有个gap的。能把自己想明白的东西整理输出出来，这才是真正的想明白了。很多时候以为自己想明白了，但是一跟别人说，或者一做ppt就讲不明白，说明理解的还不够通透。因此，多思考，并且把自己的思考输出、记录下来，这应该是有利于理解和积累的。这也是我为什么又重新开始写自己的blog。 看着师弟，同级还有师兄中，都有人在知乎上撰写技术文章，我现在也感觉到这里面确实是有必要的，不管是对内的自我沉淀，和对外的自我宣传。</p><p>此外，还有很多事情，也是工作之后看到身边优秀的人们才渐渐想明白。包括对自我行为的认知，包括怎么做事情会高效，怎么样才能快速拿结果避免无用功。这些事情现在已经渐渐有了一些自己的理解，以后有机会在慢慢详述。当然还有一些事情我隐约觉得很重要，但是目前还没有一个清晰的认知和思考，在之后的生活中应该会渐渐参悟吧。</p><p>最后，做什么事情都需要坚持，希望写blog这个事情我也能坚持下来，深入思考，不断打磨，不断积累。相信长期下来应该有不错的收货。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/01/20/hello-world/"/>
      <url>/2020/01/20/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p><hr><p>test math <span class="math inline">\(e=mc^2\)</span> <span class="math inline">\(x = argmax(\sum_{i=0}^N \mu(a_i|x))\)</span></p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>加Tag和categories的例子</title>
      <link href="/2020/01/20/post-title-with-whitespace/"/>
      <url>/2020/01/20/post-title-with-whitespace/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 临时 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>

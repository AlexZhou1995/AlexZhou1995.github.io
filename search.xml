<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>FiBiNet：特征重要性+Bilinear交叉</title>
      <link href="/2020/07/07/FiBiNet/"/>
      <url>/2020/07/07/FiBiNet/</url>
      
        <content type="html"><![CDATA[<p>FiBiNET: Combining Feature Importance and Bilinear feature Interaction for Click-Through Rate Prediction</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>文章指出当前的许多通过特征组合进行CTR预估的工作主要使用特征向量的内积或哈达玛积来计算交叉特征，这种方法忽略了特征本身的重要程度。提出通过使用Squeeze-Excitation network (SENET) 结构动态学习特征的重要性以及使用一个双线性函数(Bilinear function)来更好的建模交叉特征。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>本文的网络结构图如下：<br><img src="15941047971358.jpg" alt=""></p><p>可以看到有2处与普通的CTR模型不同：1.SENET Layer 2.Bilinear-Interaction Layer。我们着重看这两部分。假设ID特征有$f$个，第$i$个特征的embedding为$e_i \in R^k$，$k$为embed_size，下面先看一次前向的数据流：<br>1.$E=[e_1,e_2,…,e_f]$，$A$=SENET($E$)，A为每个ID特征的权重，$A\in R^f$<br>2.用权重$A$给$E$的对应特征加权，得到了加权后的Embedding $V=[v_1,…,v_f],v_i\in R^k$<br>3.$p$=Bilinear-Interaction($E$)，$p=[p_1,…,p_n]$，其中$n=\frac{f(f-2)}{2}$，即交叉组合数。同理$q$=Bilinear-Interaction($V$)<br>4.将$p$和$q$concat起来之后过DNN输出结果。</p><h3 id="SENET"><a href="#SENET" class="headerlink" title="SENET"></a>SENET</h3><h4 id="1-Squeeze"><a href="#1-Squeeze" class="headerlink" title="1.Squeeze"></a>1.Squeeze</h4><p>将每一个ID特征压缩到一个标量，压缩可以是mean-pooling或者max-pooling，下面是mean-pooling的数学表示<br>$$<br>z_i = F_{sq}(e_i) = \frac{1}{k}\sum_{t=1}^ke_i^{(t)}<br>$$</p><h4 id="2-Excitation"><a href="#2-Excitation" class="headerlink" title="2.Excitation"></a>2.Excitation</h4><p>得到$z\in R^{f\times 1}$后，类似AutoEncoder，过两层全连接得到每个特征的权重$A=[a_1,…,a_f]$<br>$$<br>A=F_{e x}(Z)=\sigma_{2}\left(W_{2} \sigma_{1}\left(W_{1} Z\right)\right)<br>$$<br>其中$\sigma$为激活函数，$W_1\in R^{f\times f/r}$，$W_2\in R^{f/r \times f}$，$r$为缩减比例，据说设置为8比较好。<br>这一步我理解是把不重要的特征的weight进一步压低。</p><h4 id="3-Re-weight"><a href="#3-Re-weight" class="headerlink" title="3.Re-weight"></a>3.Re-weight</h4><p>最后一步即是用$A$缩放$E$得到$V$<br>$$<br>V=F_{R e W e i g h t}(A, E)=\left[a_{1} \cdot e_{1}, \cdots, a_{f} \cdot e_{f}\right]=\left[v_{1}, \cdots, v_{f}\right]<br>$$</p><h3 id="Bilinear-Interaction"><a href="#Bilinear-Interaction" class="headerlink" title="Bilinear-Interaction"></a>Bilinear-Interaction</h3><p>一般的交叉方法用内积（$\cdot$）或者哈达玛积（$\odot$）<br>$$<br>\begin{aligned}<br>\left[a_{1}, a_{2}, \ldots, a_{n}\right] \cdot\left[b_{1}, b_{2}, \ldots, b_{n}\right] &amp;=\sum_{i=1}^{n} a_{i} b_{i} \<br>\left[a_{1}, a_{2}, \ldots, a_{n}\right] \odot\left[b_{1}, b_{2}, \ldots, b_{n}\right] &amp;=\left[a_{1} b_{1}, a_{2} b_{2}, \ldots, a_{n} b_{n}\right]<br>\end{aligned}<br>$$<br>作者认为这些方法没有办法很好表示稀疏特征之间的组合，他选择在特征交叉时加入一个($k\times k$)的参数$W$<br>$$<br>p_{i,j} = v_i \cdot W \odot v_j<br>$$<br>这样要学习的$W$会暴增，作者提出了三种方案：</p><ol><li>Field-ALL：所有的交叉公用一个$W$</li><li>Field-Each：每个在左边的$v_i$都有一个$W_i$，则一共需要$f$个$W$</li><li>Field-Interaction：每一对$(v_i,v_j)$单独一个$W$，一共需要$\frac{f(f-1)}{2}$个$W$</li></ol><p>通过Bilinear-Interaction Layer，$E \to p, V \to q$，最终$[p,q]$进DNN输出最后的logits。</p>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
          <category> 论文阅读 </category>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AutoInt：构造高阶特征交叉</title>
      <link href="/2020/07/07/AutoInt/"/>
      <url>/2020/07/07/AutoInt/</url>
      
        <content type="html"><![CDATA[<p>AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks (CIKM 2019)</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本文通过另外的角度来做特征交叉和构造高阶特征，结构如下图<br><img src="15941030467753.jpg" alt=""></p><p>一句话总结：<strong>把Dense特征也转化为embedding，然后所有特征一起过Multi-Head Attention。</strong></p><p>对dense特征做embedding：每个dense特征对应一个嵌入向量，乘以具体的dense特征值 作为其最终的emeddding。</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>优点：简单易实现；可以让Dense和ID特征交叉。<br>缺点：提升似乎不是特别显著，如果模型里有了PNN，看起来再加AutoInt提升不大。</p>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
          <category> 论文阅读 </category>
          
          <category> 推荐系统 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度召回算法</title>
      <link href="/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/"/>
      <url>/2020/04/04/%E6%B7%B1%E5%BA%A6%E5%8F%AC%E5%9B%9E%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h2 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h2><p>推荐系统的基本架构一般由索引、召回、粗排、精排、重排等几个部分构成，而其中的召回阶段（或者称之为Matching阶段）则主要负责根据用户和内容特征，从众多的内容库中找到用户可能感兴趣的内容。传统的召回算法，如ItemCF等，大多基于统计的方法来计算item之间的相似度，根据用户近期买过的商品进行召回。传统召回算法实现成本低，模型简单响应速度快，但是基于数据统计得到的相似关系缺乏个性化能力，召回商品的发现性，多样性较低。<br>近年来，随着深度学习的兴起，一些深度召回算法被提出。它们利用深度模型表达能力高，特征提取能力强的特点，提高了个性化召回算法的效果。最初的深度召回算法主要通过深度神经网络挖掘用户和商品在低维表示空间上的相似性，为用户召回低维表示相似的商品，这类方法我们往往称之为向量化召回算法。相比于基于数据统计的相似性计算，向量化召回的个性化程度更高，召回商品的发现性和多样性也较高。还有一些深度召回算法尝试建立商品之间的关系图，通过基于图的深度网络学习，从拓扑关系的角度捕捉商品之间的相关性，这类方法我们称之为基于图的深度召回算法。最后，还有一些方法致力于攻克深度模型带来的性能开销，使得深度模型能够在大规模的召回场景中可用，比如通过树结构和哈希的方法对召回的过程进行提速等等，这类方法我们称之为大规模深度召回算法。本文就从这三类方法展开，介绍一些现有的深度召回算法。</p><h2 id="二、分类"><a href="#二、分类" class="headerlink" title="二、分类"></a>二、分类</h2><ul><li><p>DeepMatch<br>youtube在2016年提出了用深度模型进行多分类的监督训练，得到item和user的embedding最后在线上通过内积进行召回检索，算是深度模型时代向量化召回的开端。本文中将通过分类任务监督训练得到embedding最后通过内积进行检索的方法都归到这一类</p></li><li><p>Graph-based<br>这类方法大多同样是产出item和user的embedding，最后通过向量内积进行召回检索。但是这类方法引入图的结构来描述item和user之间的关系，他们声称相比于直接进行分类训练，引入了额外的拓扑信息。</p></li><li><p>Large-scale<br>这类方法可能属于上面两类方法，但是他们着重处理Large-scale的问题，致力于缩短召回检索的开销。</p></li></ul><h2 id="三、DeepMatch"><a href="#三、DeepMatch" class="headerlink" title="三、DeepMatch"></a>三、DeepMatch</h2><h3 id="1-Deep-neural-networks-for-youtube-recommendations-RecSys-2016"><a href="#1-Deep-neural-networks-for-youtube-recommendations-RecSys-2016" class="headerlink" title="1. Deep neural networks for youtube recommendations. (RecSys 2016)"></a>1. Deep neural networks for youtube recommendations. (RecSys 2016)</h3><p><strong>结构和特征</strong><br>本文提出了一个较为基础的个性化向量召回算法，结构如下<br><img src="15855389430461.jpg" alt=""></p><p>主要特征：<br>(a) 历史搜索query：把历史搜索的query分词后的token的embedding向量进行加权平均<br>(b) 人口统计学信息：性别、年龄、地域等<br>(c) 其他上下文信息<br>(d) Age信息：视频上传时间，保证时效性。</p><p><strong>训练和生效</strong><br>离线训练时把问题建模成一个多分类问题，对用户U和上下文C，预测视频V是否点击，数学表示如下：<br><img src="15855396893807.jpg" alt=""><br>其中u和v为用户和商品的embedding，内积后进行softmax多分类。最后线上生效直接使用内积选取Top—N的视频。</p><h3 id="2-Sequential-Deep-Matching-Model-for-Online-Large-scale-Recommender-System-CIKM-2019"><a href="#2-Sequential-Deep-Matching-Model-for-Online-Large-scale-Recommender-System-CIKM-2019" class="headerlink" title="2. Sequential Deep Matching Model for Online Large-scale Recommender System. (CIKM 2019)"></a>2. Sequential Deep Matching Model for Online Large-scale Recommender System. (CIKM 2019)</h3><p>本文相比于上面Youtube的工作，引入了用户行为序列的信息进行序列化建模。示意图如下<br><img src="15855399525004.jpg" alt=""><br>对于user，这边用user prediction network（上图虚线框）来提取用户向量，user prediction network中，使用Attention来提取用户长周期的特征(防止遗忘)，使用LSTM提取短周期的特征(注重实效)，最后通过一个门结构将长短周期和user embedding进行混合得到最终的用户向量表示。</p><h2 id="四、Graph-based"><a href="#四、Graph-based" class="headerlink" title="四、Graph-based"></a>四、Graph-based</h2><h3 id="1-Billion-scale-Commodity-Embedding-for-E-commerce-Recommendation-in-Alibaba-KDD-2018"><a href="#1-Billion-scale-Commodity-Embedding-for-E-commerce-Recommendation-in-Alibaba-KDD-2018" class="headerlink" title="1. Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba. (KDD 2018)"></a>1. Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba. (KDD 2018)</h3><p>本文提出了一种基于图网络的低维表示提取方法，并且提出了两种改进方案。<br>提取的低维表示用于做个性化召回检索</p><h4 id="BGE"><a href="#BGE" class="headerlink" title="BGE"></a>BGE</h4><p>首先最基础的图网络提取低维表示的方法，BGE（Base Graph Embedding）如下：<br>(1) 构建图：商品为点，用户行为为边，构建带权图，表示商品之间的关联。<br>(2) 采样：在图上进行Random walk的采样，得到一些商品序列，相关的商品在序列中同时出现的概率较高<br>(3) 训练：将采样得到的商品序列当做句子，其中每一个商品当做一个词、用NLP中的Skip-gram方法进行训练，得到每一个词的词向量，也就是每一个商品的向量。<br><img src="15855442964204.jpg" alt=""></p><h4 id="GES-amp-EGES"><a href="#GES-amp-EGES" class="headerlink" title="GES &amp; EGES"></a>GES &amp; EGES</h4><p>上述的BGE方法难以处理冷启动的问题，新商品没有任何点击，无法产生有效的低维表示。<br>为了解决这个问题，本文提出了改进方案GES（Graph Embedding with Side information）<br>相比于只对商品学习低维表示，本方法还一并学习商品的其他属性信息。如下图SI0为item_id信息，SI1可能是store_id信息，etc.<br><img src="15855447612156.jpg" alt=""><br>最终一个商品的低维表示由所有的Side Information向量求均值得到：<br><img src="15855448211571.jpg" alt=""></p><p>而EGES（Enhanced Graph Embedding with Side information）是GES的改进，认为不同的Information有着不同的重要性，最终商品的低维表示是各个Side Information的向量加权求和得到，权值也在训练中学习。<br><img src="15855448929951.jpg" alt=""></p><h3 id="2-Graph-Convolutional-Neural-Networks-for-Web-Scal-Recommender-Systems-KDD-2018"><a href="#2-Graph-Convolutional-Neural-Networks-for-Web-Scal-Recommender-Systems-KDD-2018" class="headerlink" title="2. Graph Convolutional Neural Networks for Web-Scal Recommender Systems. (KDD 2018)"></a>2. Graph Convolutional Neural Networks for Web-Scal Recommender Systems. (KDD 2018)</h3><p>本文提出用图卷积网络来提取商品的低维表示。<br>图的构建方式还是同上，商品为点，用户行为作为边，构建一个图。但是本文用图卷积来生成一个商品的向量，即一个商品的向量通过他的近邻混合得到。下图展示了一个2层的图卷积，对于商品A，他的近邻是BCD，而BCD的近邻分别又是AC，ABEF，A。则商品A的最终向量由两层级的卷积操作得到。<br><img src="15855475119931.jpg" alt=""><br>一次卷积操作的过程如下，实际上就是对所有近邻的向量过一层全连接，然后通过pooling得到$n_u$，然后将目标向量$z_u$和$n_u$ concat之后再过一个全连接，得到新的目标向量$z_n^{new}$<br><img src="15855478737833.jpg" alt=""></p><p>训练使用了max-margin ranking loss，数据组织为$&lt;i,j,l&gt;$，其中$&lt;i,j&gt;$为一个商品对，$l$为label，表示这一对商品是否相关。</p><h2 id="五、Large—Scale"><a href="#五、Large—Scale" class="headerlink" title="五、Large—Scale"></a>五、Large—Scale</h2><h3 id="1-Candidate-Generation-with-Binary-Codes-for-Large-Scale-Top-N-Recommendation-CIKM-2019"><a href="#1-Candidate-Generation-with-Binary-Codes-for-Large-Scale-Top-N-Recommendation-CIKM-2019" class="headerlink" title="1. Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation. (CIKM 2019)"></a>1. Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation. (CIKM 2019)</h3><p>本文侧重关注召回检索的效率，结合哈希检索和向量化召回。通过对输出向量施加逐渐逼近符号函数的约束，使得生成的向量是二值的，以便使用哈希检索结构Multi-Index Hashing进行检索。这种检索方法的耗时与总商品数据量是一种次线性的关系。</p><p>本文其他部分与大多数DeepMatch类的算法相似，但是最后内积计算分类loss之前，对商品和用户向量施加一个符号函数$sgn(x)$来让输出的向量成为二值的。但是，符号函数不可导，这里就用带参数的tanh函数来进行逼近，随着训练epoch逐渐增加参数$\beta$大小，使其逼近符号函数$sgn$。损失函数和训练过程如下图。<br><img src="15855485459116.jpg" alt=""><br><img src="15855485133191.jpg" alt=""></p><h3 id="2-Learning-tree-based-deep-model-for-recommender-systems-KDD-2018"><a href="#2-Learning-tree-based-deep-model-for-recommender-systems-KDD-2018" class="headerlink" title="2. Learning tree-based deep model for recommender systems. (KDD 2018)"></a>2. Learning tree-based deep model for recommender systems. (KDD 2018)</h3><p>向量化召回由于使用向量内积作为最后的结果，表达能力有限，因此难以取得较好的个性化效果。如果使用深度模型，则超大的候选集合使得线上的延迟难以接受。本文结合了最大堆树和深度模型，提出了TDM算法。将庞大数量的商品分配到树的各个叶子节点，每个中间节点相当于其子节点商品的一个抽象表示，通过BeamSearch的方法对树结构进行高效检索。选取Top-K的商品，只需要深度模型做$Klog(N)$次预测。<br><img src="15855493488403.jpg" alt="w400"></p><p>在训练过程中，每一个中间节点也被当做一个普通的item进行处理。<br>具体的训练方式是 树的结构 和 深度模型 交替进行更新。<br>当树的结构固定时，对于一条样本，如果用户点击了一个item，则这个item的搜索父节点也被视作点击。如此便可构造数据集对深度模型进行训练。<br>而树的结构，则是通过对当前的embedding进行聚类得到。<br><img src="15855518986729.jpg" alt=""></p><h3 id="3-Joint-Optimization-of-Tree-based-Index-and-Deep-Model-for-Recommender-Systems-NIPS-2019"><a href="#3-Joint-Optimization-of-Tree-based-Index-and-Deep-Model-for-Recommender-Systems-NIPS-2019" class="headerlink" title="3. Joint Optimization of Tree-based Index and Deep Model for Recommender Systems. (NIPS 2019)"></a>3. Joint Optimization of Tree-based Index and Deep Model for Recommender Systems. (NIPS 2019)</h3><p>本文是对上面TDM算法的改进。上面TDM算法中，树的构建和深度模型的训练是分隔开的，目标并不相同，可能两者的优化相互牵制导致总体效果次优。<br>本文主要有两个改进：<br>1.树结构、深度模型联合优化<br>通过最大似然构建联合优化的loss function。其中树结构的优化难以求解，转化为带权二部图的最大匹配问题，通过贪心算法求解。<br>2.用户序列特征分层建模<br>深度模型中用到了用户的行为序列，TDM中训练数的中间层时在序列特征方面也是用的item_id粒度的序列特征。在本文中改进为使用当前层的embedding作为序列的特征。这样可以减少每层训练的噪声，并且可以从粗到细的精准建模。</p><h2 id="六、总结"><a href="#六、总结" class="headerlink" title="六、总结"></a>六、总结</h2><h4 id="1-Deep-neural-networks-for-youtube-recommendations"><a href="#1-Deep-neural-networks-for-youtube-recommendations" class="headerlink" title="1. Deep neural networks for youtube recommendations"></a>1. Deep neural networks for youtube recommendations</h4><p>方法名：无<br>动机：使用深度模型进行个性化的召回，同时保证效率<br>方法：通过深度模型多分类任务训练，得到用户和商品的向量，用向量内积做召回检索。</p><h4 id="2-Sequential-Deep-Matching-Model-for-Online-Large-scale-Recommender-System"><a href="#2-Sequential-Deep-Matching-Model-for-Online-Large-scale-Recommender-System" class="headerlink" title="2. Sequential Deep Matching Model for Online Large-scale Recommender System"></a>2. Sequential Deep Matching Model for Online Large-scale Recommender System</h4><p>方法名：SDM<br>动机：建模用户行为序列<br>方法：引入用户长短期行为序列，分别用Attention和LSTM结构进行建模，最终进行向量召回。</p><h4 id="3-Billion-scale-Commodity-Embedding-for-E-commerce-Recommendation-in-Alibaba"><a href="#3-Billion-scale-Commodity-Embedding-for-E-commerce-Recommendation-in-Alibaba" class="headerlink" title="3. Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba."></a>3. Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba.</h4><p>方法名：BGE，GES，EGES<br>动机：使用图网络表示商品之间的关系，再学习商品的低维表示<br>方法：使用用户行为建图，通过采样得到多条商品轨迹。将轨迹当成句子，商品当做词，通过NLP中求解词向量的方法求解商品向量。并且使用商品其他属性的向量来缓解冷启动问题。</p><h4 id="4-Graph-Convolutional-Neural-Networks-for-Web-Scal-Recommender-Systems"><a href="#4-Graph-Convolutional-Neural-Networks-for-Web-Scal-Recommender-Systems" class="headerlink" title="4. Graph Convolutional Neural Networks for Web-Scal Recommender Systems."></a>4. Graph Convolutional Neural Networks for Web-Scal Recommender Systems.</h4><p>方法名：PinSage<br>动机：使用图表示商品之间的关系，再使用图卷积网络学习商品的低维表示<br>方法：每一个商品的embedding由其近邻的embedding通过图卷积操作得到。通过max-margine ranking loss进行训练。</p><h4 id="5-Candidate-Generation-with-Binary-Codes-for-Large-Scale-Top-N-Recommendation"><a href="#5-Candidate-Generation-with-Binary-Codes-for-Large-Scale-Top-N-Recommendation" class="headerlink" title="5. Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation."></a>5. Candidate Generation with Binary Codes for Large-Scale Top-N Recommendation.</h4><p>方法名：CICAR<br>动机：通过高效哈希检索结构来提升召回的效率<br>方法：使用次线性复杂度的哈希检索方法Multi-index hashing，需要embedding是二值的。因此在现有双塔内积网络的基础上，使用一个近似符号函数的映射函数$tanh_\beta$将输出的向量映射到二值。</p><h4 id="6-Learning-tree-based-deep-model-for-recommender-systems"><a href="#6-Learning-tree-based-deep-model-for-recommender-systems" class="headerlink" title="6. Learning tree-based deep model for recommender systems."></a>6. Learning tree-based deep model for recommender systems.</h4><p>方法名：TDM<br>动机：通过内积进行召回表达能力有限，使用深度模型打分则商品太多开销过大。<br>方法：在全部N个商品集合上构建树的结构，在树上通过BeamSearch进行检索TopK商品，深度模型只需要预测$Klog(N)$次。树的中间节点是商品的抽象，当做商品一次训练，其label来自于子节点。深度模型和树结构交替训练。</p><h4 id="7-Joint-Optimization-of-Tree-based-Index-and-Deep-Model-for-Recommender-Systems"><a href="#7-Joint-Optimization-of-Tree-based-Index-and-Deep-Model-for-Recommender-Systems" class="headerlink" title="7. Joint Optimization of Tree-based Index and Deep Model for Recommender Systems."></a>7. Joint Optimization of Tree-based Index and Deep Model for Recommender Systems.</h4><p>方法名：JTM<br>动机：TDM中深度模型和树结构训练是完全分隔开的，目标不同可能导致相互牵制无法训练到最优。<br>方法：构建统一loss function，深度模型和树结构联合优化。树结构的优化是个较难的组合优化问题，转化为带权二部图匹配问题后用贪心算法求解。</p><h2 id="七、结语"><a href="#七、结语" class="headerlink" title="七、结语"></a>七、结语</h2><p>本文介绍了近几年内的一些深度召回算法，主要把它们划分成“Deep Matching类算法”、“基于图的算法”、“面向大规模场景算法”3类算法，并对这3类算法进行归纳和总结，分析他们的动机、做法和优缺点。深度学习在推荐系统中的应用是近年来持续的热门研究课题，每年都会有大量新技术和新模型出现。我们希望这篇文章能帮助读者对这个领域有一个大致的了解，并为未来的研究提供一些思路和参考。</p>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
          <category> 论文阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pandas处理MovieLen25M数据集</title>
      <link href="/2020/03/21/Pandas%E5%A4%84%E7%90%86MovieLen25M%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
      <url>/2020/03/21/Pandas%E5%A4%84%E7%90%86MovieLen25M%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<p>最近做了一些推荐侧召回的实验尝试，除了在业务的数据上进行测试，我还想在公开数据集上进行验证。参考Related works，最终选择使用MovieLen和Amazon的数据集。由于这两个数据集给的是裸的数据，因此需要我们根据自己的需要做一些处理。这里我用pandas来做数据的分析和处理。<br>Pandas是数据科学常用的工具，但是说来惭愧，我之前倒是很少用Pandas。因为之前做强化学习较多，不太需要大量的这种数据分析。因此这次也就趁机在熟悉一下。我个人觉得相比于算法原理，工具类的东西大致记录一下就好。工作中用到多了自然就记住了，记不住的说明平时也不怎么用，到时候再查就好，相关的文档网上有很多很多，倒是不必死记。</p><p>废话不多说，直接上Notebook吧，该说的都在注释里。（下面的链接是个html，但是如果我以HTML的格式放到资源文件夹里，编译后blog首页排版会有错）</p><a href="/2020/03/21/Pandas%E5%A4%84%E7%90%86MovieLen25M%E6%95%B0%E6%8D%AE%E9%9B%86/ml_data_process" title="[pandas数据处理notebook]">[pandas数据处理notebook]</a><p>经过这样处理，我得到了几个处理后的表，表的含义见notebook</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">viewed_sequence</span><br><span class="line">genome_feature</span><br><span class="line">genres_tag_one_hot_feature</span><br><span class="line">movie_rating_feature</span><br><span class="line">tag_feature_movie</span><br><span class="line">tag_feature_user</span><br><span class="line">user_rating_feature</span><br></pre></td></tr></table></figure><p>最后通过python+MPI并行的聚合生成最后的数据，写入到本地文件</p><a href="/2020/03/21/Pandas%E5%A4%84%E7%90%86MovieLen25M%E6%95%B0%E6%8D%AE%E9%9B%86/run.py" title="[python+MPI产出聚合数据集]">[python+MPI产出聚合数据集]</a><p>不过最终处理出来的数据集有十几个G，受限于线上docker容器的内存限制，无法放到一个文件里序列化存储和恢复，还是得通过file的方式来读，算是没有完美的达到预期，但是问题不大。</p>]]></content>
      
      
      <categories>
          
          <category> ML </category>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2020年03月15日</title>
      <link href="/2020/03/16/2020-03-15/"/>
      <url>/2020/03/16/2020-03-15/</url>
      
        <content type="html"><![CDATA[<p>今天感觉最近的生活状态不是很对，花了很多时间在没有特别多意义的事情上。如何能够专注在一个事情上，做更深入的思考，这时一个值得探索的课题。不仅是做完，更是打磨完善，直到超出某个阈值，这样才能把事情做到让旁人觉得值得点赞的程度。</p><p>此外，“以为自己想明白了”和“自己确实想明白了”之前是有个gap的。能把自己想明白的东西整理输出出来，这才是真正的想明白了。很多时候以为自己想明白了，但是一跟别人说，或者一做ppt就讲不明白，说明理解的还不够通透。因此，多思考，并且把自己的思考输出、记录下来，这应该是有利于理解和积累的。这也是我为什么又重新开始写自己的blog。<br>看着师弟，同级还有师兄中，都有人在知乎上撰写技术文章，我现在也感觉到这里面确实是有必要的，不管是对内的自我沉淀，和对外的自我宣传。</p><p>此外，还有很多事情，也是工作之后看到身边优秀的人们才渐渐想明白。包括对自我行为的认知，包括怎么做事情会高效，怎么样才能快速拿结果避免无用功。这些事情现在已经渐渐有了一些自己的理解，以后有机会在慢慢详述。当然还有一些事情我隐约觉得很重要，但是目前还没有一个清晰的认知和思考，在之后的生活中应该会渐渐参悟吧。</p><p>最后，做什么事情都需要坚持，希望写blog这个事情我也能坚持下来，深入思考，不断打磨，不断积累。相信长期下来应该有不错的收货。</p>]]></content>
      
      
      <categories>
          
          <category> 随笔 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 随笔 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/01/20/hello-world/"/>
      <url>/2020/01/20/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p><hr><p>test math<br>$e=mc^2$<br>$x = argmax(\sum_{i=0}^N \mu(a_i|x))$</p>]]></content>
      
      
      <categories>
          
          <category> 笔记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>加Tag和categories的例子</title>
      <link href="/2020/01/20/post-title-with-whitespace/"/>
      <url>/2020/01/20/post-title-with-whitespace/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 临时 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
